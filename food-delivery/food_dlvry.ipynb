{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "# IMPORTING STUFF\n",
    "\n",
    "import os \n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import glob \n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import ortools                       \n",
    "import logging\n",
    "import datetime\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser  \n",
    "import plotly.express as px  \n",
    "from shapely import geometry\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.geocoders import Nominatim  \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=26) \n",
    "matplotlib.rc('ytick', labelsize=26) \n",
    "\n",
    "plt.rcParams['font.size'] = '26'\n",
    "plt.rcParams['figure.figsize'] = (10,7.5)\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'ahm'\n",
    "\n",
    "flag = 0\n",
    "w1, w2 = 0.5, 0.5\n",
    "'''\n",
    "\"flag\" decides which distance metric/measure to consider:\n",
    "0: euclidean distance (or physical distance)\n",
    "1: rating\n",
    "2: combination of euclidean distance and rating\n",
    "   where 'w1' is weight given to euclidean distance and 'w2' is weight given to rating\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASETS\n",
    "import glob \n",
    "\n",
    "driver_files_ahm = sorted(glob.glob(\"data/driver_locs/ahm/driver_data_ahm_day*.csv\"))\n",
    "num_days = len(driver_files_ahm) \n",
    "driver_dfs_ahm = [pd.read_csv(driver_files_ahm[idx]) for idx in range(num_days)]\n",
    "driver_dfs_dict = {'ahm': driver_dfs_ahm}\n",
    "zone_df_ahm = pd.read_csv(\"data/zone_data/zone_data_ahm.csv\")\n",
    "zone_dfs_dict = {'ahm': zone_df_ahm}\n",
    "income_df_ahm = pd.read_csv(\"data/income_data/incomes_ahm.csv\")\n",
    "income_dfs_dict = {'ahm': income_df_ahm} \n",
    "base_zone_ahm = pd.read_csv(\"data/base_zones/ahm_base_zones.csv\")\n",
    "base_zones_dict = {'ahm': base_zone_ahm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "#'city' takes values in {'ahm', 'blr', 'del'}\n",
    "\n",
    "def driver_union(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding union of all the drivers over the days \n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city] \n",
    "    num_days = len(driver_dfs)\n",
    "    \n",
    "    driver_udf = pd.concat([driver_dfs[idx] for idx in range(num_days)])\n",
    "    driver_udf = driver_udf.drop('Unnamed: 0', axis=1)\n",
    "    driver_udf = driver_udf.drop_duplicates('de_id').reset_index().drop('index', axis=1)\n",
    "    \n",
    "    return driver_udf\n",
    "\n",
    "\n",
    "def driver_intersection(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding intersection of all the drivers over the days\n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city]\n",
    "    driver_idf = reduce(lambda left,right: pd.merge(left,right,on='de_id'), driver_dfs)\n",
    "    driver_idf = driver_idf[['de_id', 'lat_x', 'lng_x']]\n",
    "    driver_idf = driver_idf.loc[:, ~driver_idf.columns.duplicated()] \n",
    "    driver_idf = driver_idf.rename(columns={'lat_x':'lat', 'lng_x':'lng'})\n",
    "    \n",
    "    return driver_idf\n",
    "\n",
    "\n",
    "\n",
    "def drivers_zones(drivers_dict, zones_dict):\n",
    "    \"\"\"\n",
    "    To get the data to be input to fair_clustering: \"driver_locs\" and \"zone_locs\"\n",
    "    \"\"\"\n",
    "    driver_idf = driver_intersection(drivers_dict) \n",
    "    \n",
    "    # finding \"driver_locs\":\n",
    "    driver_locs = driver_idf[['lat', 'lng']] \n",
    "    driver_locs = driver_locs.values \n",
    "    \n",
    "    # finding \"zone_locs\":\n",
    "    zone_df = zones_dict[city]\n",
    "    zone_locs = np.array(zone_df[['lat', 'lng']])\n",
    "    \n",
    "    return driver_locs, zone_locs\n",
    "\n",
    "\n",
    "def get_capacities(zones_dict):\n",
    "    \"\"\"\n",
    "    returns \"lower_caps\" and \"upper_caps\"\n",
    "    lower_caps: [1 x num_centres] array with lower capacity of each zone\n",
    "    upper_caps: [1 x num_centres] array with upper capacity of each zone\n",
    "    \"\"\"\n",
    "    zone_df = zones_dict[city] \n",
    "\n",
    "    lower_caps = 0.3*zone_df['avg_cap'].values\n",
    "    upper_caps = 1.0*zone_df['avg_cap'].values\n",
    "    \n",
    "    return lower_caps, upper_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGER\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=city+\".log\", format='%(asctime)s  %(message)s', filemode='w')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the inputs to fair_clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_udf = driver_union(driver_dfs_dict)\n",
    "driver_idf = driver_intersection(driver_dfs_dict) \n",
    "\n",
    "driver_locs, zone_locs = drivers_zones(driver_dfs_dict, zone_dfs_dict)\n",
    "num_drivers, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "lower_caps, upper_caps = get_capacities(zone_dfs_dict)\n",
    "assert driver_locs.shape[0]>lower_caps.sum() and driver_locs.shape[0]<upper_caps.sum(), \\\n",
    "\"This set of num_drivers, lower_caps and upper_caps will lead to an infeasible solution !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(d_loc, z_loc):\n",
    "    lat1, lng1 = d_loc[0], d_loc[1]\n",
    "    lat2, lng2 = z_loc[0], z_loc[1]\n",
    "\n",
    "    dist = np.sqrt(np.power(lat1-lat2, 2) + np.power(lng1-lng2, 2))\n",
    "    return dist\n",
    "\n",
    "def L2Distance(data):\n",
    "  # \"data\": latitude-longitude level locations \n",
    "  transposed = np.expand_dims(data, axis = 1)\n",
    "  distance = np.power(data - transposed, 2)\n",
    "  distance = np.power(np.abs(distance).sum(axis = 2), 0.5) \n",
    "  return distance \n",
    "\n",
    "driver_dists = L2Distance(driver_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings to sellers:\n",
    "from scipy.stats import truncnorm\n",
    "from numpy.random import SeedSequence \n",
    "from numpy.random import default_rng\n",
    "\n",
    "def get_truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm( (low-mean)/sd, (upp-mean)/sd, loc=mean, scale=sd) \n",
    "\n",
    "def generate_ratings(num_drivers):\n",
    "    mean = 3.5\n",
    "    sd = 1\n",
    "    min_rating = 0.0\n",
    "    max_rating = 5.0\n",
    "    seedVal = 36778738061272522495168595294022739449\n",
    "    rng = default_rng(seedVal)\n",
    "    dist = get_truncated_normal(mean, sd, min_rating, max_rating)\n",
    "    ratings = dist.rvs(num_drivers, random_state=rng)\n",
    "    ratings = [round(x, 1) for x in ratings]\n",
    "    return ratings\n",
    "\n",
    "def abs_difference(ratings):\n",
    "    transposed = np.expand_dims(ratings, axis=1)\n",
    "    diff = abs(ratings-transposed) \n",
    "    return diff   \n",
    "\n",
    "def minmax(distance, fair_distance):\n",
    "    num_samples = len(distance)\n",
    "    mx, mn = distance.max(), distance.min()\n",
    "    dists = distance.flatten()\n",
    "    dists = np.asarray( [((x-mn)/(mx-mn)) for x in dists] )\n",
    "    distance = dists.reshape((num_samples, num_samples))\n",
    "    fair_distance = (fair_distance-mn)/(mx-mn)\n",
    "    return distance, fair_distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will go with the default parameters of cplex:\n",
    "from cplex import Cplex\n",
    "model = Cplex()\n",
    "model.parameters.simplex.tolerances.feasibility.get(),\\\n",
    "model.parameters.simplex.tolerances.optimality.get(),\\\n",
    "model.parameters.simplex.tolerances.markowitz.get()      \n",
    "\n",
    "model.parameters.workmem.set(10240) # 10GB  \n",
    "model.parameters.emphasis.memory.set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Clustering - LPP contstraints and Cplex\n",
    "from cplex import Cplex\n",
    "# from lp_tools import *\n",
    "from lp_tools_kn import * \n",
    "\n",
    "alpha_fair = 2\n",
    "\n",
    "def fair_clustering(dataset, centres, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "  # Step 1: \t Create an instance of Cplex \n",
    "  problem = Cplex()\n",
    "  problem.parameters.simplex.tolerances.feasibility.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.optimality.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.markowitz.set(float(0.9999)) \n",
    "  problem.parameters.emphasis.memory.set(1)\n",
    "  problem.parameters.workmem.set(10240)\n",
    "\n",
    "  # Step 2: \t Declare that this is a minimization problem\n",
    "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
    "    \n",
    "  \"\"\"\n",
    "   Step 3.   Declare and  add variables to the model. \n",
    "        The function prepare_to_add_variables (dataset, centres) prepares all the required information for this stage.\n",
    "  \n",
    "    objective: a list of coefficients (float) in the linear objective function\n",
    "    lower bound: a list of floats containing the lower bounds for each variable\n",
    "    upper bound: a list of floats containing the upper bounds for each variable\n",
    "    variable_names: a list of strings that contains the name of the variables\n",
    "  \"\"\"\n",
    "  ## if working with \"lp_tools\":\n",
    "  print(\"Adding Variables...\")\n",
    "  \n",
    "  # objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
    "  ## if working with \"lp_tools_kn\": \n",
    "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres, prohibited_assignments)\n",
    "  problem.variables.add(\n",
    "      obj = objective,\n",
    "      lb = lower_bound,\n",
    "      ub = upper_bound,\n",
    "      names = variable_names\n",
    "     \n",
    "    )\n",
    "  \n",
    "  print(\"Variables Added !\")\n",
    "    \n",
    "    \n",
    "  \"\"\"\n",
    "  Step 4.   Declare and add constraints to the model.\n",
    "            There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
    "            Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
    "            The function prepare_to_add_constraints(dataset, centres) prepares the required data for this step.\n",
    "  \n",
    "   coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
    "   senses: a list of strings that identifies whether the corresponding constraint is\n",
    "           an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
    "   rhs: a list of floats corresponding to the rhs of the constraints.\n",
    "   constraint_names: a list of string corresponding to the name of the constraint\n",
    "  \"\"\"\n",
    "  print(\"Adding Constraints...\")\n",
    "    \n",
    "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair, fair_distance, ratings, flag)\n",
    "  print(\"num_constraints:\", len(senses)) \n",
    "  logger.info(f\"\\t\\t\\tnum_constraints = {len(senses)}\")\n",
    "  problem.linear_constraints.add(\n",
    "      rhs = rhs,\n",
    "      senses = senses,\n",
    "      names = row_names\n",
    "    )\n",
    "  problem.linear_constraints.set_coefficients(coefficients)\n",
    "\n",
    "  print(\"Constraints Added !\")\n",
    "    \n",
    "  # Step 5.\tSolve the problem\n",
    "  problem.solve()\n",
    "\n",
    "  result = {\n",
    "    \"status\": problem.solution.get_status(),\n",
    "    \"success\": problem.solution.get_status_string(),\n",
    "    \"objective\": problem.solution.get_objective_value(),\n",
    "    \"assignment\": problem.solution.get_values(),\n",
    "  }\n",
    "    \n",
    "  qm = problem.solution.quality_metric  \n",
    "  print(\"Solution Quality:\", problem.solution.get_float_quality([qm.max_x, qm.max_primal_infeasibility]))\n",
    "  \n",
    "  # print(\"Status:\", result['status']) # outputs a number: \"1\" for optimal solution, \"2\" for unbounded ray and \"3\" for infeasible solution\n",
    "  solution_status = result['status']\n",
    "  assert solution_status==1, \"Solution isn't optimal !\"\n",
    "\n",
    "  print(\"Status:\", problem.solution.get_status_string()) # optimal, unbounded ray, infeasible\n",
    "\n",
    "  return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Assignment of drivers to the FFCs / warehouses\n",
    "import copy\n",
    "import dependent_routing as dp\n",
    "\n",
    "# configParser.read(configFilePath)\n",
    "\n",
    "num_samples, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "def fair_assignment(prob_dis,driver_loc):\n",
    "  '''Assigning the driver using the probaility distribution using dependent rounding'''  \n",
    "  \n",
    "  # \"prob_dis\" is the result of the Fair-LP program \"fair_clustering\"  \n",
    "  prob_dist = copy.deepcopy(prob_dis)\n",
    "  # print(\"prob_dist shape [num_drivers x num_ffc]:\", prob_dist.shape)\n",
    "\n",
    "  rounding = dp.DependentRounding(prob_dist)\n",
    "  rounding._buildGraph(prob_dist)\n",
    "  final_assignment = rounding.round()\n",
    "  final_assignment = np.around(final_assignment,2)\n",
    "\n",
    "  driver_df = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df['ffc_index'] = -1 # unassigned\n",
    "\n",
    "  for i in range(num_samples):\n",
    "    for j in range(num_centres):\n",
    "      # choose values which are close to 1\n",
    "      if abs(final_assignment[i][j]-1) < 0.01: \n",
    "        driver_df.at[i,'ffc_index'] = j\n",
    "        \n",
    "  return driver_df, final_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(probs):\n",
    "    \"\"\"\n",
    "    To cope with bound violations which can occur upto the feasibility parameter range \n",
    "    So the lower bound of 0.0 on the probabilities can get violated and the values can go down to (0-feasibility_parameter_value)\n",
    "    \"\"\"\n",
    "    for i in range(len(probs)):\n",
    "        last_pos_index = -1\n",
    "        neg_value = 0\n",
    "        \n",
    "        for j in range(len(probs[0])):\n",
    "            assert probs[i][j] >= -1e-6 \n",
    "            \n",
    "            if probs[i][j] < 0:\n",
    "                neg_value += probs[i][j]\n",
    "                probs[i][j] = 0\n",
    "            elif probs[i][j] > 0:\n",
    "                last_pos_index = j\n",
    "\n",
    "        max_pos_index = np.argmax(probs[i])\n",
    "        probs[i][max_pos_index] += neg_value\n",
    "        \n",
    "        assert probs[i][max_pos_index] > 0\n",
    "        \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main :\n",
    "def FairAssign_solver(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "    # Fair-LP:\n",
    "    try:\n",
    "        lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    except:\n",
    "        logger.error(\"Solution Non-optimal (Unbounded Ray or Infeasible) !\")\n",
    "        return None, None\n",
    "        \n",
    "    prob_dis = np.reshape(lp_output['assignment'][:num_samples*num_centres], (-1, num_centres))\n",
    "    \n",
    "    try:\n",
    "        prob_dist = sanityCheck(copy.deepcopy(prob_dis)) # this might raise an assertion error\n",
    "    except:\n",
    "        logger.error(\"Sanity Check Assertion !\")\n",
    "        return None, None\n",
    "    \n",
    "    # Randomized Dependent Rounding:\n",
    "    try:\n",
    "        df = fair_assignment(prob_dist, driver_locs)[0] # this might raise an assertion error\n",
    "        final_assignment = df['ffc_index'].values\n",
    "    except:\n",
    "        logger.error(\"Dependent Rounding Assertion !\")\n",
    "        return prob_dist, None\n",
    "    \n",
    "    return prob_dist, final_assignment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [(x-1) for x in [7, 5, 3]] # x-1 NOT x bcz 'k' is a 0-based index\n",
    "fd_list = [(driver_dists.mean()/alpha) for alpha in [28, 14, 8]] \n",
    "num_runs = 5\n",
    "\n",
    "k_fd_dict = {k:\\\n",
    "                {fd_idx:\\\n",
    "                    {num_run:\\\n",
    "                        {'p_dist':None, 'assignment':None}\n",
    "                        for num_run in range(num_runs)\n",
    "                    } \n",
    "                    for fd_idx in range(len(fd_list))\n",
    "                } \n",
    "            for k in k_list\n",
    "            }\n",
    "\n",
    "for k in k_list:\n",
    "    print(k)\n",
    "    logger.info(f\"Considering k = {k+1} nearest zones\")\n",
    "    prhbtd_assigns = get_pa(dz_dist, k)\n",
    "    \n",
    "    for f_idx, fair_distance in enumerate(fd_list):\n",
    "        logger.info(f\"\\tfair_distance = {fair_distance}\")\n",
    "        \n",
    "        for num_run in range(num_runs):\n",
    "            logger.info(f\"\\t\\tnum_run = {num_run}\")\n",
    "            \n",
    "            prob_dist, final_assignment = FairAssign_solver(driver_locs, zone_locs, lower_caps, upper_caps, fair_distance, prhbtd_assigns)\n",
    "            \n",
    "            k_fd_dict[k][f_idx][num_run]['p_dist'] = prob_dist\n",
    "            k_fd_dict[k][f_idx][num_run]['assignment'] = final_assignment\n",
    "    \n",
    "    # Store intermediate results as well as fail-safe:\n",
    "    # saving current state of \"k_fd_dict\":\n",
    "    pickling_on = open(f\"dict_k={k+1}.pickle\", \"wb\")\n",
    "    pickle.dump(k_fd_dict, pickling_on)\n",
    "    pickling_on.close() \n",
    "\n",
    "pickling_on = open(\"Assignments_ratings_\"+city+\".pickle\", \"wb\")\n",
    "pickle.dump(k_fd_dict, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS:\n",
    "\n",
    "def gini_index(incomes):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum() \n",
    "    inc_sum = 0.0\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            inc_sum += abs(incomes[i]-incomes[j])\n",
    "    gini = inc_sum / (2*num*total)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def avg_distance(zone_labels, driver_locs, zone_locs):\n",
    "    \"\"\"\n",
    "    returns the 'cost' of the assignment\n",
    "    zone_labels: indices of the assigned zones\n",
    "    a zone_label 'z' has location zone_locs[z]\n",
    "    \"\"\"\n",
    "    driver_dists = L2Distance(driver_locs) \n",
    "    num = len(zone_labels)\n",
    "    dist = 0.0 \n",
    "    for i in range(num):\n",
    "        assigned_zone = zone_labels[i]\n",
    "        driver_loc, zone_loc = driver_locs[i], zone_locs[int(assigned_zone)]\n",
    "        driver_zone_dist = euclidean_distance(driver_loc, zone_loc)\n",
    "        dist += np.sqrt(driver_zone_dist)\n",
    "    avg_dist = dist/num\n",
    "    return avg_dist                       \n",
    "    \n",
    "\n",
    "def spatial_inequality_index(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "    if flag==0:\n",
    "        driver_dists = L2Distance(driver_locs)\n",
    "    if flag==1:\n",
    "        driver_dists = abs_difference(ratings)\n",
    "    if flag==2:\n",
    "        driver_dists = combined\n",
    "\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "\n",
    "    term_i = 0.0    \n",
    "    for i in range(num):\n",
    "        sum_j = 0.0\n",
    "        num_j = 1e-9    \n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j] <= fair_distance and driver_dists[i][j]>0:\n",
    "                num_j += 1\n",
    "                sum_j += abs(incomes[i]-incomes[j])   \n",
    "        term_i += (sum_j / num_j) \n",
    "    \n",
    "    spin_idx = term_i / total \n",
    "    # spin_idx = round(spin_idx, 2)\n",
    "    return spin_idx \n",
    "\n",
    "\n",
    "def income_gap(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "    \"\"\" \n",
    "    difference between incomes between any two drivers per unit distance (within fair_distance) \n",
    "    \"\"\"\n",
    "    alpha = 100\n",
    "\n",
    "    if flag==0:\n",
    "        driver_dists = L2Distance(driver_locs)\n",
    "    if flag==1:\n",
    "        driver_dists = abs_difference(ratings)\n",
    "    if flag==2:\n",
    "        driver_dists = combined\n",
    "\n",
    "    driver_dists = driver_dists * alpha\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "\n",
    "    terms = 0.0\n",
    "    num_pair_drivers = 1e-7 # NOT 0 => to avoid division by 0\n",
    "    for i in range(num-1):\n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j]>0:\n",
    "                num_pair_drivers += 1\n",
    "                terms += (abs(incomes[i]-incomes[j])/driver_dists[i][j])\n",
    "    \n",
    "    inc_gap = terms/num_pair_drivers\n",
    "    # inc_gap = round(inc_gap, 2)\n",
    "    return inc_gap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation of locations within a given zone\n",
    "\n",
    "# generating random locations withing a zone (given the zone boundary):\n",
    "def random_loc_generator(zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  min_lat, max_lat = min(lats), max(lats)\n",
    "  min_lng, max_lng = min(longs), max(longs)\n",
    "  new_lat, new_lng = random.uniform(min_lat, max_lat), random.uniform(min_lng, max_lng) \n",
    "  return [new_lat, new_lng]\n",
    "\n",
    "# Checking if a given location lies inside a given zone:\n",
    "def path_related_preprocessing(path_bdry):\n",
    "  # exemplar path_bdry: '12.954619258010608,77.6149292592163 12.954680993923494,77.61640664016727 ....'\n",
    "  path_bdry = str(path_bdry)\n",
    "  df = pd.DataFrame({'lts':[], 'lngs':[]})\n",
    "  bdry_locs = path_bdry.split()\n",
    "  lats, longs = [], []\n",
    "  for loc in bdry_locs:\n",
    "    lat, lng = loc.split(',')\n",
    "    lats.append(float(lat))\n",
    "    longs.append(float(lng))\n",
    "  return lats, longs\n",
    "\n",
    "def loc_in_zone(loc, zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  polygon = geometry.MultiPoint(coords).convex_hull\n",
    "  Point_X, Point_Y = loc[0], loc[1]\n",
    "  point = geometry.Point(Point_X, Point_Y)\n",
    "  return point.within(polygon)\n",
    "\n",
    "# code to generate 'm' locations that lie within a given zone:\n",
    "def generate_locs(m, zone_bdry):\n",
    "    new_locs = []\n",
    "    num_generated = 0\n",
    "    while num_generated < m:\n",
    "        new_loc = random_loc_generator(zone_bdry)\n",
    "        sanity_check = loc_in_zone(new_loc, zone_bdry)\n",
    "        if sanity_check:\n",
    "            num_generated += 1\n",
    "            new_locs.append(new_loc)\n",
    "    return new_locs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing required for FoodMatch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
