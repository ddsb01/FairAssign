{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import warnings # need to be imported before other imports\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os \n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import glob \n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import ortools\n",
    "import logging\n",
    "import datetime\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser   \n",
    "from shapely import geometry\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "# from geopy.geocoders import Nominatim  \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=26) \n",
    "matplotlib.rc('ytick', labelsize=26) \n",
    "\n",
    "plt.rcParams['font.size'] = '26'\n",
    "plt.rcParams['figure.figsize'] = (10,7.5)\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigns_path = \"./assign_results\"\n",
    "if os.path.exists(assigns_path):\n",
    "    shutil.rmtree(assigns_path)\n",
    "\n",
    "logs_path = \"./logs\"\n",
    "if os.path.exists(logs_path):\n",
    "    shutil.rmtree(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "city = 'A'            # takes values in {'A', 'B'}\n",
    "flag = 0              # takes values in {0, 1, 2}\n",
    "day_num = 1           # takes values in {1, 2, 3, 4, 5, 6}\n",
    "w1, w2 = 0.5, 0.5     # w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\n",
    "NUM_DRIVERS = 977     # Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\n",
    "NUM_SIM = 5           # Number of similar drivers for each driver to be considered in the fairness constraint\n",
    "K_VAL = 10            # Number of nearest zones to which a driver can be assigned\n",
    "algo = 'fair-assign'  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flag**  \n",
    "\"flag\" decides which distance metric/measure to consider:    \n",
    "0: euclidean distance (or physical distance)   \n",
    "1: rating   \n",
    "2: combination of euclidean distance and rating   \n",
    "   where 'w1' is weight given to euclidean distance and 'w2' is weight given to rating \n",
    "\n",
    "\n",
    "**algo**  \n",
    "\"algo\" decides the assignment algorithm;    \n",
    "It can take values in {'FairAssign', 'RoundRobin', 'LIPA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASETS\n",
    "import glob \n",
    "\n",
    "driver_files_A = sorted(glob.glob(\"data/driver_locs/A/driver_data_A_day*.csv\"))\n",
    "driver_files_B = sorted(glob.glob(\"data/driver_locs/B/driver_data_B_day*.csv\"))\n",
    "driver_files_C = sorted(glob.glob(\"data/driver_locs/C/driver_data_C_day*.csv\"))\n",
    "\n",
    "num_days = len(driver_files_A) \n",
    "assert len(driver_files_A)==len(driver_files_B), \"error in reading data or incomplete data\"\n",
    "assert len(driver_files_A)==len(driver_files_C), \"error in reading data or incomplete data\"\n",
    "\n",
    "driver_dfs_A = [pd.read_csv(driver_files_A[idx]) for idx in range(num_days)]\n",
    "driver_dfs_B = [pd.read_csv(driver_files_B[idx]) for idx in range(num_days)]\n",
    "driver_dfs_C = [pd.read_csv(driver_files_C[idx]) for idx in range(num_days)]\n",
    "driver_dfs_dict = {'A': driver_dfs_A, 'B': driver_dfs_B, 'C': driver_dfs_C}\n",
    "\n",
    "zone_df_A = pd.read_csv(\"data/zone_data/zone_data_A.csv\")\n",
    "zone_df_B = pd.read_csv(\"data/zone_data/zone_data_B.csv\")\n",
    "zone_df_C = pd.read_csv(\"data/zone_data/zone_data_C.csv\")\n",
    "zone_dfs_dict = {'A': zone_df_A, 'B': zone_df_B, 'C': zone_df_C}\n",
    "\n",
    "income_df_A = pd.read_csv(\"data/income_data/incomes_A.csv\")\n",
    "income_df_B = pd.read_csv(\"data/income_data/incomes_B.csv\")\n",
    "income_df_C = pd.read_csv(\"data/income_data/incomes_C.csv\")\n",
    "income_dfs_dict = {'A': income_df_A, 'B': income_df_B, 'C': income_df_C}\n",
    "\n",
    "base_zone_A = pd.read_csv(\"data/base_zones/A_base_zones.csv\")\n",
    "base_zone_B = pd.read_csv(\"data/base_zones/B_base_zones.csv\")\n",
    "base_zone_C = pd.read_csv(\"data/base_zones/C_base_zones.csv\")\n",
    "base_zones_dict = {'A': base_zone_A, 'B': base_zone_B, 'C': base_zone_C}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "#'city' takes values in {'A'}\n",
    "\n",
    "def driver_union(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding union of all the drivers over the days \n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city] \n",
    "    num_days = len(driver_dfs)\n",
    "    \n",
    "    driver_udf = pd.concat([driver_dfs[idx] for idx in range(num_days)])\n",
    "    driver_udf = driver_udf.drop('Unnamed: 0', axis=1)\n",
    "    driver_udf = driver_udf.drop_duplicates('de_id').reset_index().drop('index', axis=1)\n",
    "    \n",
    "    return driver_udf\n",
    "\n",
    "\n",
    "def driver_intersection(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding intersection of all the drivers over the days\n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city]\n",
    "    driver_idf = reduce(lambda left,right: pd.merge(left,right,on='de_id'), driver_dfs)\n",
    "    driver_idf = driver_idf[['de_id', 'lat_x', 'lng_x']]\n",
    "    driver_idf = driver_idf.loc[:, ~driver_idf.columns.duplicated()] \n",
    "    driver_idf = driver_idf.rename(columns={'lat_x':'lat', 'lng_x':'lng'})\n",
    "    \n",
    "    return driver_idf\n",
    "\n",
    "\n",
    "def drivers_zones(drivers_dict, zones_dict):\n",
    "    \"\"\"\n",
    "    To get the data to be input to fair_clustering: \"driver_locs\" and \"zone_locs\"\n",
    "    \"\"\"\n",
    "    driver_idf = driver_intersection(drivers_dict) \n",
    "    \n",
    "    # finding \"driver_locs\":\n",
    "    driver_locs = driver_idf[['lat', 'lng']] \n",
    "    driver_locs = driver_locs.values \n",
    "    \n",
    "    # finding \"zone_locs\":\n",
    "    zone_df = zones_dict[city]\n",
    "    zone_locs = np.array(zone_df[['lat', 'lng']])\n",
    "    \n",
    "    return driver_locs, zone_locs\n",
    "\n",
    "\n",
    "def get_capacities(zones_dict):\n",
    "    \"\"\"\n",
    "    returns \"lower_caps\" and \"upper_caps\"\n",
    "    lower_caps: [1 x num_centres] array with lower capacity of each zone\n",
    "    upper_caps: [1 x num_centres] array with upper capacity of each zone\n",
    "    \"\"\"\n",
    "    zone_df = zones_dict[city] \n",
    "\n",
    "    lower_caps = 0.3*zone_df['avg_cap'].values\n",
    "    upper_caps = 1.0*zone_df['avg_cap'].values\n",
    "    \n",
    "    return lower_caps, upper_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGER\n",
    "\n",
    "import logging\n",
    "\n",
    "logs_path = \"./logs\"\n",
    "if not os.path.exists(logs_path):\n",
    "    os.mkdir(logs_path)\n",
    "\n",
    "logging.basicConfig(filename=f\"logs/{city}_{NUM_DRIVERS}_{NUM_SIM}.log\", format='%(asctime)s  %(message)s', filemode='w')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the inputs to fair_clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_udf = driver_union(driver_dfs_dict)\n",
    "driver_idf = driver_intersection(driver_dfs_dict) \n",
    "\n",
    "driver_locs, zone_locs = drivers_zones(driver_dfs_dict, zone_dfs_dict)\n",
    "num_drivers, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "lower_caps, upper_caps = get_capacities(zone_dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(d_loc, z_loc):\n",
    "    lat1, lng1 = d_loc[0], d_loc[1]\n",
    "    lat2, lng2 = z_loc[0], z_loc[1]\n",
    "    dist = np.sqrt(np.power(lat1-lat2, 2) + np.power(lng1-lng2, 2))\n",
    "    return dist\n",
    "\n",
    "def L2Distance(data):\n",
    "  # \"data\": latitude-longitude level locations \n",
    "  transposed = np.expand_dims(data, axis = 1)\n",
    "  distance = np.power(data - transposed, 2)\n",
    "  distance = np.power(np.abs(distance).sum(axis = 2), 0.5) \n",
    "  return distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_dist = driver_dists.mean()/dnr\n",
    "\n",
    "# dnr to NUM_SIM maps: (tells the number of similar drivers per driver for a given dnr (hence fair_dist))\n",
    "# manually curated for quick experimentation\n",
    "sim2dnr_A = {80:4, 60:5, 30:8, 20:11, 15:14, 10:21, 7:28, 5:53} \n",
    "sim2dnr_B = {70:8, 60:9, 50:10, 40:12, 30:14, 20:19, 15:23, 10:32, 7:45, 5:65}\n",
    "sim2dnr_C = {90:10, 80:11, 70:12, 60:13, 50: 15, 40:17, 30:22, 20:32, 15:42, 10:65, 8:80}\n",
    "# these values are true only when all the drivers in driver_idf are considered !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_DRIVERS!=-1 and NUM_DRIVERS <= driver_locs.shape[0]:\n",
    "    driver_locs = driver_locs[:NUM_DRIVERS]\n",
    "\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "\n",
    "# Adjust lower_caps for given NUM_DRIVERS\n",
    "# Do not adjust upper_caps\n",
    "num_drivers = driver_locs.shape[0]\n",
    "print(f\"Sum of lower capacities: {lower_caps.sum()}\")\n",
    "while(lower_caps.sum()>num_drivers):\n",
    "    # print(lower_caps)\n",
    "    for idx in range(lower_caps.shape[0]):\n",
    "        lower_caps[idx] = max(lower_caps[idx]-50, 0)\n",
    "\n",
    "print(f\"Sum of (adjusted) lower capacities: {lower_caps.sum()}\")\n",
    "print(f\"Sum of upper capacities: {upper_caps.sum()}\")\n",
    "      \n",
    "assert num_drivers>lower_caps.sum() and num_drivers<upper_caps.sum(), \\\n",
    "\"This set of num_drivers, lower_caps and upper_caps will lead to an infeasible solution !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_drivers, num_zones = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "dz_dist = np.zeros(shape=(num_drivers, num_zones))\n",
    "for d_idx, driver in enumerate(driver_locs):\n",
    "    d_dist = np.zeros(num_zones)\n",
    "    for z_idx, zone in enumerate(zone_locs):\n",
    "        dist = euclidean_distance(driver, zone)\n",
    "        d_dist[z_idx] = dist \n",
    "    dz_dist[d_idx] = d_dist\n",
    "\n",
    "# print(dz_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prohibited_assigns(d_dist, k):\n",
    "    prohibited_assignments = np.zeros(shape=(num_drivers, num_zones))\n",
    "    \n",
    "    for d_idx, d_dist in enumerate(d_dist):\n",
    "        idx = np.argpartition(d_dist, k) \n",
    "        prohibited_assignments[d_idx][idx[k:]] = 1 # set the indices NOT corresponding to k-smallest elements \n",
    "    \n",
    "    return prohibited_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings to sellers:\n",
    "from scipy.stats import truncnorm\n",
    "from numpy.random import SeedSequence \n",
    "from numpy.random import default_rng\n",
    "\n",
    "def get_truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm( (low-mean)/sd, (upp-mean)/sd, loc=mean, scale=sd) \n",
    "\n",
    "def generate_ratings(num_drivers):\n",
    "    mean = 3.5\n",
    "    sd = 1\n",
    "    min_rating = 0.0\n",
    "    max_rating = 5.0\n",
    "    seedVal = 36778738061272522495168595294022739449 # arbitrary\n",
    "    rng = default_rng(seedVal)\n",
    "    dist = get_truncated_normal(mean, sd, min_rating, max_rating)\n",
    "    ratings = dist.rvs(num_drivers, random_state=rng)\n",
    "    ratings = [round(x, 1) for x in ratings]\n",
    "    return ratings\n",
    "\n",
    "def abs_difference(ratings):\n",
    "    transposed = np.expand_dims(ratings, axis=1)\n",
    "    diff = abs(ratings-transposed) \n",
    "    return diff   \n",
    "\n",
    "def minmax(distance, fair_distance):\n",
    "    num_samples = len(distance)\n",
    "    mx, mn = distance.max(), distance.min()\n",
    "    dists = distance.flatten()\n",
    "    dists = np.asarray( [((x-mn)/(mx-mn)) for x in dists] )\n",
    "    distance = dists.reshape((num_samples, num_samples))\n",
    "    fair_distance = (fair_distance-mn)/(mx-mn)\n",
    "    return distance, fair_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_ids = zone_dfs_dict[city]['zone_id']\n",
    "zone_id2idx = {zone_id: idx for idx, zone_id in enumerate(zone_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FairAssign**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will go with the default parameters of cplex:\n",
    "from cplex import Cplex\n",
    "model = Cplex()\n",
    "model.parameters.simplex.tolerances.feasibility.get(),\\\n",
    "model.parameters.simplex.tolerances.optimality.get(),\\\n",
    "model.parameters.simplex.tolerances.markowitz.get()      \n",
    "\n",
    "model.parameters.workmem.set(10240) # 10GB  \n",
    "model.parameters.emphasis.memory.set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Clustering - LPP contstraints and Cplex\n",
    "from cplex import Cplex\n",
    "# from lp_tools import *\n",
    "from lp_tools_kn import * \n",
    "\n",
    "alpha_fair = 2\n",
    "\n",
    "def fair_clustering(dataset, centres, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "  # Step 1: \t Create an instance of Cplex \n",
    "  problem = Cplex()\n",
    "  problem.parameters.simplex.tolerances.feasibility.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.optimality.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.markowitz.set(float(0.9999)) \n",
    "  problem.parameters.emphasis.memory.set(1)\n",
    "  problem.parameters.workmem.set(10240)\n",
    "\n",
    "  # Step 2: \t Declare that this is a minimization problem\n",
    "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
    "    \n",
    "  \"\"\"\n",
    "   Step 3.   Declare and  add variables to the model. \n",
    "        The function prepare_to_add_variables (dataset, centres) prepares all the required information for this stage.\n",
    "  \n",
    "    objective: a list of coefficients (float) in the linear objective function\n",
    "    lower bound: a list of floats containing the lower bounds for each variable\n",
    "    upper bound: a list of floats containing the upper bounds for each variable\n",
    "    variable_names: a list of strings that contains the name of the variables\n",
    "  \"\"\"\n",
    "  ## if working with \"lp_tools\":\n",
    "  print(\"Adding Variables...\")\n",
    "  \n",
    "  # objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
    "  ## if working with \"lp_tools_kn\": \n",
    "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres, prohibited_assignments)\n",
    "  problem.variables.add(\n",
    "      obj = objective,\n",
    "      lb = lower_bound,\n",
    "      ub = upper_bound,\n",
    "      names = variable_names\n",
    "     \n",
    "    )\n",
    "  \n",
    "  print(\"Variables Added !\")\n",
    "    \n",
    "    \n",
    "  \"\"\"\n",
    "  Step 4.   Declare and add constraints to the model.\n",
    "            There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
    "            Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
    "            The function prepare_to_add_constraints(dataset, centres) prepares the required data for this step.\n",
    "  \n",
    "   coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
    "   senses: a list of strings that identifies whether the corresponding constraint is\n",
    "           an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
    "   rhs: a list of floats corresponding to the rhs of the constraints.\n",
    "   constraint_names: a list of string corresponding to the name of the constraint\n",
    "  \"\"\"\n",
    "  print(\"Adding Constraints...\")\n",
    "    \n",
    "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair, fair_distance, ratings, flag)\n",
    "  print(\"num_constraints:\", len(senses)) \n",
    "  logger.info(f\"\\t\\t\\tnum_constraints = {len(senses)}\")\n",
    "  problem.linear_constraints.add(\n",
    "      rhs = rhs,\n",
    "      senses = senses,\n",
    "      names = row_names\n",
    "    )\n",
    "  problem.linear_constraints.set_coefficients(coefficients)\n",
    "\n",
    "  print(\"Constraints Added !\")\n",
    "    \n",
    "  # Step 5.\tSolve the problem\n",
    "  problem.solve()\n",
    "\n",
    "  result = {\n",
    "    \"status\": problem.solution.get_status(),\n",
    "    \"success\": problem.solution.get_status_string(),\n",
    "    \"objective\": problem.solution.get_objective_value(),\n",
    "    \"assignment\": problem.solution.get_values(),\n",
    "  }\n",
    "    \n",
    "  qm = problem.solution.quality_metric  \n",
    "  print(\"Solution Quality:\", problem.solution.get_float_quality([qm.max_x, qm.max_primal_infeasibility]))\n",
    "  \n",
    "  # print(\"Status:\", result['status']) # outputs a number: \"1\" for optimal solution, \"2\" for unbounded ray and \"3\" for infeasible solution\n",
    "  solution_status = result['status']\n",
    "  assert solution_status==1, \"Solution isn't optimal !\"\n",
    "\n",
    "  print(\"Status:\", problem.solution.get_status_string()) # optimal, unbounded ray, infeasible\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Assignment of drivers to the FFCs / warehouses\n",
    "import copy\n",
    "import dependent_routing as dp\n",
    "\n",
    "# configParser.read(configFilePath)\n",
    "\n",
    "num_samples, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "def fair_assignment(prob_dis, driver_loc):\n",
    "  '''Assigning the driver using the probaility distribution using dependent rounding'''  \n",
    "  \n",
    "  # \"prob_dis\" is the result of the Fair-LP program \"fair_clustering\"  \n",
    "  prob_dist = copy.deepcopy(prob_dis)\n",
    "  # print(\"prob_dist shape [num_drivers x num_ffc]:\", prob_dist.shape)\n",
    "\n",
    "  rounding = dp.DependentRounding(prob_dist)\n",
    "  rounding._buildGraph(prob_dist)\n",
    "  final_assignment = rounding.round()\n",
    "  final_assignment = np.around(final_assignment,2)\n",
    "\n",
    "  driver_df = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df['ffc_index'] = -1 # unassigned\n",
    "\n",
    "  for i in range(num_samples):\n",
    "    for j in range(num_centres):\n",
    "      if abs(final_assignment[i][j]-1) < 0.01: \n",
    "        driver_df.at[i,'ffc_index'] = j\n",
    "        \n",
    "  return driver_df, final_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(probs):\n",
    "    \"\"\"\n",
    "    To cope with bound violations which can occur upto the feasibility parameter range \n",
    "    So the lower bound of 0.0 on the probabilities can get violated and the values can go down to (0-feasibility_parameter_value)\n",
    "    \"\"\"\n",
    "    for i in range(len(probs)):\n",
    "        last_pos_index = -1\n",
    "        neg_value = 0\n",
    "        \n",
    "        for j in range(len(probs[0])):\n",
    "            assert probs[i][j] >= -1e-6 \n",
    "            \n",
    "            if probs[i][j] < 0:\n",
    "                neg_value += probs[i][j]\n",
    "                probs[i][j] = 0\n",
    "            elif probs[i][j] > 0:\n",
    "                last_pos_index = j\n",
    "\n",
    "        max_pos_index = np.argmax(probs[i])\n",
    "        probs[i][max_pos_index] += neg_value\n",
    "        \n",
    "        assert probs[i][max_pos_index] > 0\n",
    "        \n",
    "    return probs\n",
    "\n",
    "\n",
    "def picklify(ds, filepath):\n",
    "    pickling_on = open(filepath, \"wb\")\n",
    "    pickle.dump(ds, pickling_on)\n",
    "    pickling_on.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main :\n",
    "def FairAssign_solver(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "    # Fair-LP:\n",
    "    # lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    try:\n",
    "        lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    except:\n",
    "        logger.error(\"Solution Non-optimal (Unbounded Ray or Infeasible) !\")\n",
    "        return None, None\n",
    "    prob_dis = np.reshape(lp_output['assignment'][:num_samples*num_centres], (-1, num_centres))\n",
    "    \n",
    "    try:\n",
    "        prob_dist = sanityCheck(copy.deepcopy(prob_dis)) # this might raise an assertion error\n",
    "    except:\n",
    "        logger.error(\"Sanity Check Assertion !\")\n",
    "        return None, None\n",
    "    \n",
    "    # Randomized Dependent Rounding:\n",
    "    try:\n",
    "        df = fair_assignment(prob_dist, driver_locs)[0] # this might raise an assertion error\n",
    "        final_assignment = df['ffc_index'].values\n",
    "    except:\n",
    "        logger.error(\"Dependent Rounding Assertion !\")\n",
    "        return prob_dist, None\n",
    "    \n",
    "    return prob_dist, final_assignment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"# drivers : {num_drivers}\")\n",
    "print(f\"# zones : {num_zones}\")\n",
    "\n",
    "if city=='A':\n",
    "    nk_list = [num_zones//3]\n",
    "elif city=='B':\n",
    "    nk_list = [num_zones//3]\n",
    "print(\"How many nearest zones? :\", nk_list)\n",
    "\n",
    "k_list = [(x-1) for x in nk_list] # [7, 5, 3] assign only to k-nearest zones \n",
    "\n",
    "if city=='A':\n",
    "    fd_dnr = sim2dnr_A[NUM_SIM]\n",
    "elif city=='B':\n",
    "    fd_dnr = sim2dnr_B[NUM_SIM]\n",
    "\n",
    "fd_list = [(driver_dists.mean()/alpha) for alpha in [fd_dnr]] # fair_distances\n",
    "ratings = generate_ratings(num_drivers)\n",
    "\n",
    "num_runs = 1\n",
    "# k_list and fd_list contain hyperparameters\n",
    "\n",
    "k_fd_dict = {k:\\\n",
    "                {fd_idx:\\\n",
    "                    {num_run:\\\n",
    "                        {'p_dist':None, 'assignment':None}\n",
    "                        for num_run in range(num_runs)\n",
    "                    } \n",
    "                    for fd_idx in range(len(fd_list))\n",
    "                } \n",
    "            for k in k_list\n",
    "            }\n",
    "\n",
    "assign_results_path = f\"assign_results/results_{city}/\"\n",
    "if not os.path.exists(assign_results_path):\n",
    "    os.makedirs(assign_results_path) # directory to store the results of FairAssign_solver\n",
    "\n",
    "# shutil.rmtree(assign_results_path)\n",
    "# os.mkdir(assign_results_path) # doesn't work for nested directories\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for k in k_list:\n",
    "    logger.info(f\"Considering k = {k+1} nearest zones\")\n",
    "    prhbtd_assigns = get_prohibited_assigns(dz_dist, k)\n",
    "    \n",
    "    for f_idx, fair_distance in enumerate(fd_list):\n",
    "        logger.info(f\"\\tfair_distance = {fair_distance}\")\n",
    "        for num_run in range(num_runs):\n",
    "            logger.info(f\"\\t\\tnum_run = {num_run}\")\n",
    "            prob_dist, final_assignment = FairAssign_solver(driver_locs, zone_locs, lower_caps, upper_caps, fair_distance, prhbtd_assigns)\n",
    "            k_fd_dict[k][f_idx][num_run]['p_dist'] = prob_dist\n",
    "            k_fd_dict[k][f_idx][num_run]['assignment'] = final_assignment \n",
    "            \n",
    "    # Store intermediate results as well as fail-safe:\n",
    "    # saving current state of \"k_fd_dict\":\n",
    "    filepath = os.path.join(assign_results_path, f\"dict_k={k+1}_{NUM_DRIVERS}_{NUM_SIM}.pickle\")\n",
    "    # print(filepath)\n",
    "    picklify(k_fd_dict, filepath)\n",
    "end = time.time()\n",
    "print(f\"Execution time: {(end-start)/3600}hrs\")\n",
    "logger.info(f\"Execution time: {(end-start)/3600}hrs\")\n",
    "\n",
    "final_file_path = os.path.join(assign_results_path, f\"Assignments_{city}_{NUM_DRIVERS}_{NUM_SIM}.pickle\")\n",
    "picklify(k_fd_dict, final_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment(result_path, driver_locs, k):\n",
    "    '''\n",
    "    returns ffc_index (or zone index) for each driver based on the \"FairAssign\" assignment \n",
    "    '''\n",
    "    pickle_off = open(result_path, \"rb\")\n",
    "    assignments = pickle.load(pickle_off)\n",
    "    prob_dist = assignments[k-1][0][0]['p_dist']\n",
    "    ## get assignment by applying dependent rounding: \n",
    "    df = fair_assignment(prob_dist, driver_locs)[0] \n",
    "    final_assignment = df['ffc_index'].values\n",
    "\n",
    "    return final_assignment\n",
    "\n",
    "# RoundRobin assignment while maintaining upper capacity bounds of zones only:\n",
    "def round_robin_dist(day_num, driver_locs):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rr_df = pd.DataFrame(driver_locs, columns=['lat', 'lng'])\n",
    "    rr_df['bz_idx_rr'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = (day_num+1) % num_zones\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = (zone+1) % num_zones \n",
    "        rr_df.at[i, 'bz_idx_rr'] = int(zone)\n",
    "        temp_upper_cap[zone] -= 1 \n",
    "    return rr_df\n",
    "\n",
    "# Random assignment while maintaining only upper capacity bounds of the zones:\n",
    "def random_dist(day_num, driver_locs, upper_caps):\n",
    "    random.seed(1234+day_num)\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rand_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    rand_df['bz_idx_rand'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = random.randint(1, num_zones)-1\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = random.randint(1, num_zones)-1\n",
    "        rand_df.at[i, 'bz_idx_rand'] = zone \n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return rand_df \n",
    "\n",
    "# LIPA while maintaining upper capacity bounds of zones only:\n",
    "def lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "\n",
    "    lipa_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    lipa_df['bz_idx_lipa'] = -1\n",
    "\n",
    "    temp_upper_cap = copy.deepcopy(upper_caps)\n",
    "\n",
    "    driver_idx_inc = np.argsort(np.array(driver_prev_incomes))\n",
    "    zone_idx_inc = np.argsort(np.array(zone_prev_incomes))\n",
    "  \n",
    "    j = num_zones-1\n",
    "    for i in driver_idx_inc:\n",
    "        zone = zone_idx_inc[j]\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            j = j-1\n",
    "            zone = zone_idx_inc[j]\n",
    "        lipa_df.at[i, 'bz_idx_lipa'] = zone\n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return lipa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim of this cell is to get \"assign_df\"\n",
    "assign_df = None\n",
    "\n",
    "if algo=='FairAssign':\n",
    "    result_path = f\"assign_results/results_{city}/Assignments_{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}.pickle\"\n",
    "    assignment = get_assignment(result_path, driver_locs, K_VAL) \n",
    "    assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "    # original base zones\n",
    "    assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "    assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "    # base zones assigned by Fair Assign\n",
    "    assign_df['fa_bz_idx'] = assignment \n",
    "    \n",
    "elif algo=='RoundRobin':\n",
    "    rr_df = round_robin_dist(day_num, driver_locs)\n",
    "    assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "    # original base zones\n",
    "    assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "    assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "    # base zones assigned by \"RoundRobin\"\n",
    "    assign_df['fa_bz_idx'] = rr_df['bz_idx_rr']\n",
    "\n",
    "elif algo=='Random':\n",
    "    rand_df = random_dist(driver_locs, upper_caps)\n",
    "    assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "    # original base zones\n",
    "    assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "    assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "    # base zones assigned at \"Random\"\n",
    "    assign_df['fa_bz_idx'] = rand_df['bz_idx_rand']\n",
    "\n",
    "elif algo=='LIPA':\n",
    "    # use the first day of FoodMatch as the first day of LIPA \n",
    "\n",
    "    # Requires simulation results of previous day\n",
    "    local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "    local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "    day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "    sim_path = f'results/sim_results/sim_results_{city}/{NUM_DRIVERS}_{algo}/sim_results_lipa{day_num-1}'\n",
    "    print(sim_path)\n",
    "\n",
    "    data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "    data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "    data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "    vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "        \n",
    "    data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "    for d_id in driver_idf['de_id']:\n",
    "        try:\n",
    "            day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "        except:\n",
    "            # handles the cases for which d_id is not present in data_deliver \n",
    "            continue\n",
    "                    \n",
    "    local_incomes_df[f'day{day_num}'] = local_incomes_df['de_id'].map(day_incomes)\n",
    "\n",
    "    # previous day incomes are used to determine the next day's assignment:\n",
    "    fm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "    fm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "    fm_inc_df = copy.deepcopy(local_incomes_df)\n",
    "    fm_inc_df = pd.merge(fm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "    # find day 1 incomes of drivers (FairAssign):\n",
    "    prev_incomes_df = copy.deepcopy(fm_inc_df[['de_id', 'lat', 'lng', 'day1']])\n",
    "    driver_prev_incomes = prev_incomes_df['day1'].values\n",
    "\n",
    "    # find day 1 number of orders in each zone (FairAssign):\n",
    "    orders_data = pd.read_csv(f\"data/orders_data/{city}/orders_0{day_num}05.csv\") \n",
    "    sim_path = 'sim_results/sim_results_'+city+'/sim.resultsAlipa'+str(day_num)\n",
    "    \n",
    "    data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "    data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "    data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        \n",
    "    vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "    df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "\n",
    "    cust_zones = df['customer_zone'].unique()\n",
    "    cust_zones_gb = df.groupby('customer_zone')\n",
    "\n",
    "    orders_per_zone = {key:0 for key in cust_zones if key in zone_ids.values}\n",
    "    for key in cust_zones:\n",
    "        if key in zone_ids.values:\n",
    "            orders_per_zone[key] = cust_zones_gb.get_group(key).shape[0] \n",
    "\n",
    "    # orders_per_zone = {k: v for k, v in sorted(orders_per_zone.items(), key=lambda item: item[1])}\n",
    "    orders_per_zone = {k:v for k, v in sorted(orders_per_zone.items())}\n",
    "    zone_prev_incomes = [v for k, v in orders_per_zone.items()]\n",
    "    # -----------------------------------------------------------------------\n",
    "    lipa_df = lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes)\n",
    "    assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "    # original base zones\n",
    "    assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "    assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "    # base zones assigned by LIPA\n",
    "    assign_df['fa_bz_idx'] = assign_df['bz_idx_lipa']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the \"LIPA\" baseline, we need to have the FoodMatch simulation corresponding to day 1 beforehand. Also, unlike other baselines, LIPA assignment for day 'd' depends on the simulation done using LIPA assignment of day 'd-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assign_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have the probability distributions corresponding to each driver. The next step is to use a last-mile delivery algorithm on top of this assignment. \n",
    "\n",
    "- However, before running simulations using a last-mile delivery algorithm such as FoodMatch or FairFoody, we need to generate relevant \"de_intervals\" files that are required by those algorithms based on our assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File generation starts:\n"
     ]
    }
   ],
   "source": [
    "print(\"File generation starts:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation of locations within a given zone\n",
    "\n",
    "# generating random locations withing a zone (given the zone boundary):\n",
    "def random_loc_generator(zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  min_lat, max_lat = min(lats), max(lats)\n",
    "  min_lng, max_lng = min(longs), max(longs)\n",
    "  new_lat, new_lng = random.uniform(min_lat, max_lat), random.uniform(min_lng, max_lng) \n",
    "  return [new_lat, new_lng]\n",
    "\n",
    "# Checking if a given location lies inside a given zone:\n",
    "def path_related_preprocessing(path_bdry):\n",
    "  # exemplar path_bdry: '12.954619258010608,77.6149292592163 12.954680993923494,77.61640664016727 ....'\n",
    "  path_bdry = str(path_bdry)\n",
    "  df = pd.DataFrame({'lts':[], 'lngs':[]})\n",
    "  bdry_locs = path_bdry.split()\n",
    "  lats, longs = [], []\n",
    "  for loc in bdry_locs:\n",
    "    lat, lng = loc.split(',')\n",
    "    lats.append(float(lat))\n",
    "    longs.append(float(lng))\n",
    "  return lats, longs\n",
    "\n",
    "def loc_in_zone(loc, zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  polygon = geometry.MultiPoint(coords).convex_hull\n",
    "  Point_X, Point_Y = loc[0], loc[1]\n",
    "  point = geometry.Point(Point_X, Point_Y)\n",
    "  return point.within(polygon)\n",
    "\n",
    "# code to generate 'm' locations that lie within a given zone:\n",
    "def generate_locs(m, zone_bdry):\n",
    "    new_locs = []\n",
    "    num_generated = 0\n",
    "    while num_generated < m:\n",
    "        new_loc = random_loc_generator(zone_bdry)\n",
    "        sanity_check = loc_in_zone(new_loc, zone_bdry)\n",
    "        if sanity_check:\n",
    "            num_generated += 1\n",
    "            new_locs.append(new_loc)\n",
    "    return new_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_csv(f\"data/location_data/location_{city}.csv\", header=None)\n",
    "location_df = location_df.rename(columns={0:'node_id', 1:'lat', 2:'lng'})\n",
    "# location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files corresponding to driver_idf:\n",
    "de_idf_ids = driver_idf['de_id'][:NUM_DRIVERS]\n",
    "\n",
    "de_idf_files = []\n",
    "for d_id in de_idf_ids:\n",
    "    file_name = 'data/de_data/'+city +'_de_data/'+str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    de_idf_files.append(file_name)\n",
    "# de_idf_files\n",
    "\n",
    "# get the node_ids:\n",
    "orig_node_ids = []\n",
    "na = 0 # number of files in de_idf_files which are not present in de_intervals/\n",
    "for file in de_idf_files[:NUM_DRIVERS]:\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        na += 1\n",
    "        d_id = int(file.split('/')[-1][:-4])\n",
    "        to_drop_idx = assign_df[assign_df['de_id']==d_id].index\n",
    "        assign_df = assign_df.drop(to_drop_idx)\n",
    "        # print(to_drop_idx)\n",
    "        continue \n",
    "    # get starting node ids corresponding to all shifts, it will be useful for random generation for unswappable drivers\n",
    "    num_shifts = int(file_df.shape[0]/2)\n",
    "    node_id = [int(file_df.iloc[x*2].values[0].split()[1]) for x in range(num_shifts)]\n",
    "    orig_node_ids.append(node_id)\n",
    "\n",
    "assign_df['node_id'] = orig_node_ids \n",
    "assign_df['fa_node_id'] = assign_df['node_id']\n",
    "\n",
    "print(f\"{na}/{len(de_idf_files[:NUM_DRIVERS])} intersection drivers not found in ../{day_num}/de_intervals/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final file generation:\n",
    "intersection_files = []\n",
    "for d_id in driver_idf['de_id'][:NUM_DRIVERS]:\n",
    "    file_name = 'data/de_data/'+city+'_de_data/'+ str(day_num)+'/de_intervals/'+str(int(d_id))+'.csv'\n",
    "    intersection_files.append(file_name)\n",
    "\n",
    "old_dir_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}/de_int_old'\n",
    "new_dir_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}/de_intervals'\n",
    "\n",
    "if os.path.exists(old_dir_path):\n",
    "    shutil.rmtree(old_dir_path)\n",
    "    \n",
    "if os.path.exists(new_dir_path):\n",
    "    shutil.rmtree(new_dir_path)\n",
    "\n",
    "os.makedirs(old_dir_path)\n",
    "os.makedirs(new_dir_path)\n",
    "\n",
    "for file in intersection_files:\n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        print(\"file_df not found\")\n",
    "        continue \n",
    "    old_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}/de_int_old/{d_id}.csv'\n",
    "    file_df.to_csv(old_path, index=False)\n",
    "    new_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}/de_intervals/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWAPPING LOGIC:\n",
    "# The swapping logic is applicable only to the drivers for which the fa_bz_idx is different from bz_idx\n",
    "rel_df = assign_df[assign_df['bz_idx'] != assign_df['fa_bz_idx']].reset_index()\n",
    "print(rel_df.shape[0])\n",
    "rel_df['paired_de'+str(day_num)] = None\n",
    "\n",
    "# first assign swappable nodes:\n",
    "bz_nodes_dict = {zone_idx:{'freq':0, 'nodes':[], 'paired_de_id':[]} for zone_idx, _ in enumerate(zone_ids)} # it'll contain the frequency of each base zone in rel_df['bz_idx'] as well as the corresponding node_ids in a list\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    z_id = int(rel_df.iloc[idx]['bz_idx'])\n",
    "    n_id = int(rel_df.iloc[idx]['node_id'][0])\n",
    "    paired_did = int(rel_df.iloc[idx]['de_id']) # remove later?\n",
    "    bz_nodes_dict[z_id]['freq'] += 1\n",
    "    bz_nodes_dict[z_id]['nodes'].append(n_id)\n",
    "    bz_nodes_dict[z_id]['paired_de_id'].append(paired_did) # remove later?\n",
    "bz_nodes_dict_store = copy.deepcopy(bz_nodes_dict)\n",
    "# IF a required zone_id in fa_bz_idx is present in bz_nodes_dict then use that data point\n",
    "# ELSE generate a random location in the zone corresponding to the zone_id\n",
    "num_random = 0 # number of drivers for whom random generation of location was done to get the corresponding start node\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    print(idx, end=' ')\n",
    "    z_id = int(rel_df.iloc[idx]['fa_bz_idx'])\n",
    "    if(bz_nodes_dict[z_id]['freq'] > 0):\n",
    "        rel_df.loc[idx, 'fa_node_id'] = bz_nodes_dict[z_id]['nodes'][0]\n",
    "        rel_df.loc[idx, 'paired_de'] = bz_nodes_dict[z_id]['paired_de_id'][0]\n",
    "        bz_nodes_dict[z_id]['freq'] -= 1\n",
    "        bz_nodes_dict[z_id]['nodes'].pop(0)\n",
    "        bz_nodes_dict[z_id]['paired_de_id'].pop(0) # remove later?\n",
    "    else:\n",
    "        # randomly generate a location in the zone assigned by FairAssign\n",
    "        zone_df = zone_dfs_dict[city]\n",
    "        # zone_bdry = zone_df[zone_df['zone_id']==z_id]['path'].values[0] # Wrong ! bcz z_id is the index of zone_id\n",
    "        zone_bdry = zone_df.iloc[z_id]['path']\n",
    "        new_loc = generate_locs(1, zone_bdry)[0] \n",
    "\n",
    "        # based on new_loc, get the closest node_id from location_df\n",
    "        min_dist = 1e9\n",
    "        n_id = -1\n",
    "        for i in range(location_df.shape[0]):\n",
    "            node_loc = [ location_df.iloc[i]['lat'], location_df.iloc[i]['lng'] ]\n",
    "            curr_dist = euclidean_distance(new_loc, node_loc)\n",
    "            if(curr_dist <= min_dist):\n",
    "                min_dist = curr_dist \n",
    "                n_id = location_df.iloc[i]['node_id']     \n",
    "        num_random += 1\n",
    "        num_shifts = int(len(rel_df.iloc[idx]['node_id']))\n",
    "        n_idz = [int(n_id)]*num_shifts\n",
    "        rel_df.at[idx, 'fa_node_id'] = n_idz\n",
    "        \n",
    "print()\n",
    "print(f\"{rel_df.shape[0]-num_random} data points out of {rel_df.shape[0]} could be swapped !\")\n",
    "print(f\"{num_random} data points were randomly generated !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the de_interval profiles of swappable or paired drivers are to be swapped \n",
    "# and that of drivers for whom random locations are generated, the profile is to be modified\n",
    "rel_drivers = rel_df['de_id']\n",
    "\n",
    "rel_files = []\n",
    "for d_id in rel_drivers:\n",
    "    file_name = 'data/de_data/'+city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    rel_files.append(file_name)\n",
    "\n",
    "for file in rel_files:\n",
    "    # d_id = file[40:-4] \n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    file_df = pd.read_csv(file)\n",
    "    \n",
    "    swap_node = rel_df[rel_df['de_id']==int(d_id)].paired_de.values[0]\n",
    "    # for those drivers who could be swapped:\n",
    "    if not np.isnan(swap_node):\n",
    "        swap_with_file = 'data/de_data/' + city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(swap_node)) + '.csv'\n",
    "        file_df = pd.read_csv(swap_with_file)\n",
    "    # for those drivers whose starting nodes for each shift were randomly generated\n",
    "    else:\n",
    "        num_shifts = int(file_df.shape[0]/2)\n",
    "        new_start_nodes = rel_df[rel_df['de_id']==int(d_id)].fa_node_id.values[0]\n",
    "        for i in range(num_shifts):\n",
    "            new_node = new_start_nodes[i]\n",
    "            file_df.iloc[i*2] = str(file_df.iloc[0].values[0].split()[0])+ ' ' + str(new_node) \n",
    "\n",
    "    new_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}/de_intervals/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"File generation ends!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulations**       \n",
    "---\n",
    "Follow the given steps to apply the last-mile delivery algorithm on top of the new assignments and get the simulation dumps:     \n",
    "- For _FoodMatch_, refer to https://github.com/idea-iitd/FoodMatch     \n",
    "\n",
    "- Before running the simulation, however, first copy the 'de_intervals' directories from \"data/de_data/{city}\\_de_data/{day_num}/{city}\\_{NUM_DRIVERS}\\_{NUM_SIM}\\_{K_VAL}/\" to \"FoodMatch/Swiggy/data/data\\_{city}_anonymized/food_data/{day_num}\" for day_num in [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "- save the simulation output corresponding to each day 'day_num' using the following nomenclature:   \n",
    "\"sim_results_{algo}{day_num}\" for each day_num; day_num \\in {1, 2, 3, 4, 5, 6} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the results on 'FoodMatch' or 'FairFoody' w/o considering the new assignments done by the FairAssign, don't copy the directories as directed in the previous step rather just simulate on the data that FoodMatch or FairFoody provides directly.   \n",
    "\n",
    "For evaluation, remember to save the simulations as \"sim_results_{algo}{day_num}\". Take algo='foodmatch' for FoodMatch (only) simulations and algo='fairfoody' for FairFoody (only) simulations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS:\n",
    "\n",
    "def gini_index(incomes):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum() \n",
    "    inc_sum = 0.0\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            inc_sum += abs(incomes[i]-incomes[j])\n",
    "    gini = inc_sum / (2*num*total)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def avg_distance(zone_labels, driver_locs, zone_locs):\n",
    "    \"\"\"\n",
    "    returns the 'cost' of the assignment\n",
    "    zone_labels: indices of the assigned zones\n",
    "    a zone_label 'z' has location zone_locs[z]\n",
    "    \"\"\"\n",
    "    driver_dists = L2Distance(driver_locs) \n",
    "    num = len(zone_labels)\n",
    "    dist = 0.0 \n",
    "    for i in range(num):\n",
    "        assigned_zone = zone_labels[i]\n",
    "        driver_loc, zone_loc = driver_locs[i], zone_locs[int(assigned_zone)]\n",
    "        driver_zone_dist = euclidean_distance(driver_loc, zone_loc)\n",
    "        dist += np.sqrt(driver_zone_dist)\n",
    "    avg_dist = dist/num\n",
    "    return avg_dist                       \n",
    "    \n",
    "\n",
    "# def spatial_inequality_index(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "def spatial_inequality_index(incomes, driver_dists, fair_distance):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    term_i = 0.0    \n",
    "    for i in range(num):\n",
    "        sum_j = 0.0\n",
    "        num_j = 1e-9    \n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j] <= fair_distance and driver_dists[i][j]>0:\n",
    "                num_j += 1\n",
    "                sum_j += abs(incomes[i]-incomes[j])   \n",
    "        term_i += (sum_j / num_j) \n",
    "    \n",
    "    spin_idx = term_i / total \n",
    "    # spin_idx = round(spin_idx, 2)\n",
    "    return spin_idx \n",
    "\n",
    "\n",
    "# def income_gap(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "def income_gap(incomes, driver_dists, fair_distance): \n",
    "    \"\"\" \n",
    "    difference between incomes between any two drivers per unit distance (within fair_distance) \n",
    "    \"\"\"\n",
    "    alpha = 100\n",
    "    driver_dists = driver_dists * alpha\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    terms = 0.0\n",
    "    num_pair_drivers = 1e-7 # NOT 0 => to avoid division by 0\n",
    "    for i in range(num-1):\n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j]>0:\n",
    "                num_pair_drivers += 1\n",
    "                terms += (abs(incomes[i]-incomes[j])/driver_dists[i][j])\n",
    "    inc_gap = terms/num_pair_drivers\n",
    "    # inc_gap = round(inc_gap, 2)\n",
    "    return inc_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the FoodMatch simulation results for all 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incomes_df(algo):\n",
    "    # get the incomes on all 6 days for all drivers \n",
    "    '''\n",
    "    input string: algo\n",
    "    'fm' : FoodMatch\n",
    "    'fafm' : FairAssign + FoodMatch\n",
    "    '''\n",
    "    local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "    local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "    num_days = 6\n",
    "    day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "    for day in range(1, num_days+1):\n",
    "        sim_path = f'results/sim_results/sim_results_{city}/{NUM_DRIVERS}_{algo}/sim_results_{algo}{day}'\n",
    "        # print(sim_path)\n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        # print(data_deliver)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "        data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "        for d_id in driver_idf['de_id']:\n",
    "            try:\n",
    "                day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "            except:\n",
    "                # handles the cases for which d_id is not present in data_deliver \n",
    "                continue  \n",
    "        local_incomes_df[f'day{day}'] = local_incomes_df['de_id'].map(day_incomes)\n",
    "    \n",
    "    cols = ['day1', 'day2', 'day3', 'day4', 'day5', 'day6'] \n",
    "    local_incomes_df['num_orders'] = local_incomes_df[cols].sum(axis=1) \n",
    "\n",
    "    return copy.deepcopy(local_incomes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_idf = driver_idf[:NUM_DRIVERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo can take values in {'foodmatch', 'fairfoody', 'fairassign', 'roundrobin', 'lipa'}\n",
    "incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "incomes_df['de_id'] = driver_idf['de_id']\n",
    "incomes_df = get_incomes_df(algo)\n",
    "incomes_df = pd.merge(incomes_df, driver_idf, on='de_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating driver_dists\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "\n",
    "# generating ratings\n",
    "num_drivers = driver_dists.shape[0]\n",
    "ratings = generate_ratings(num_drivers)\n",
    "\n",
    "# generating combined \n",
    "ratings_matrix = abs_difference(ratings)\n",
    "normalized_dists = minmax(driver_dists, 0)[0]\n",
    "normalized_ratings = minmax(ratings_matrix, 0)[0]\n",
    "_combined = w1*driver_dists + w2*ratings_matrix \n",
    "combined = minmax(_combined, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = incomes_df['lat']\n",
    "longs = incomes_df['lng']\n",
    "incomes = incomes_df['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(lats, longs, incomes, ratings, combined, fair_dist): \n",
    "    d_locs = [[lat, lng] for lat, lng in zip(lats, longs)]\n",
    "    incomes = np.array(incomes)\n",
    "    gini = gini_index(incomes) \n",
    "    sp_idx = spatial_inequality_index(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    inc_gp = income_gap(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    return gini, sp_idx, inc_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag==0:\n",
    "    driver_dists = driver_dists \n",
    "if flag==1:\n",
    "    driver_dists = ratings_matrix \n",
    "if flag==2:\n",
    "    driver_dists = combined\n",
    "\n",
    "def num_sim_drivers(fd):\n",
    "    ''' \n",
    "    number of similar drivers (i.e., for a given driver, how many drivers are being considered for fairness comparison per)\n",
    "    '''\n",
    "    num_similar_drivers = []\n",
    "    for idx in range(len(driver_dists)):\n",
    "        curr_driver = driver_dists[idx]\n",
    "        num_sim = curr_driver[curr_driver<=fd].shape[0]\n",
    "        num_similar_drivers.append(num_sim) \n",
    "    return np.mean(num_similar_drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the metrics at different fair_dist values: \n",
    "if flag==0:\n",
    "    fair_dist = driver_dists.mean()/8 \n",
    "if flag==1:\n",
    "    fair_dist = 1 \n",
    "if flag==2:\n",
    "    fair_dist = 0.04 \n",
    "    \n",
    "def eval_results(lats, longs, incomes):\n",
    "    results = []\n",
    "    for k in range(1, 11):\n",
    "        fd = fair_dist/k \n",
    "        gini, sp_idx, inc_gp = metrics(lats, longs, incomes, ratings, combined, fd)\n",
    "        results.append([fd, num_sim_drivers(fd), gini, sp_idx, inc_gp])\n",
    "    result_df = pd.DataFrame(results)\n",
    "    cols = ['fair_dist', 'sim_drivers', 'gini', 'spatial_ineq', 'income_gap']\n",
    "    result_df.columns = cols\n",
    "    return result_df\n",
    "\n",
    "# FINAL RESULTS:\n",
    "fafm_results_df = eval_results(lats, longs, incomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Avg. Distance (or Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_this_driver(locs, v_id):\n",
    "    # driver's inital location:\n",
    "    v_init_loc = driver_idf[driver_idf['de_id']==v_id][['lat', 'lng']].values[0]\n",
    "    first_mile_dist = euclidean_distance(locs[0], v_init_loc)\n",
    "    last_mile_dist = 0\n",
    "    for idx in range(1, len(locs)):\n",
    "        prev_loc = locs[idx-1]\n",
    "        curr_loc = locs[idx] \n",
    "        last_mile_dist += euclidean_distance(prev_loc, curr_loc) \n",
    "    return first_mile_dist, last_mile_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the incomes on all 6 days for all drivers \n",
    "num_days = 6\n",
    "\n",
    "def get_cost(algo, num_days):\n",
    "    ''' \n",
    "    algo: str\n",
    "    'fm': FoodMatch \n",
    "    'fafm': FairAssign then FoodMatch\n",
    "    '''\n",
    "    cust_lats, cust_lngs = [], []\n",
    "    first_mile_cost = 0.0 # over all num_days days\n",
    "    last_mile_cost = 0.0 # over all num_days days\n",
    "    for day in range(1, num_days+1):\n",
    "        # print(day)\n",
    "        total_first_mile = 0\n",
    "        total_last_mile = 0\n",
    "        orders_data = pd.read_csv(f\"data/orders_data/{city}/orders_0{day}05.csv\") \n",
    "        sim_path = f'sim_results_{algo}{day_num}'\n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "        df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "        c_locs = df['customer_lat_lng'].values \n",
    "        df['cust_lat'] = [float(loc.split(',')[0]) for loc in c_locs]\n",
    "        df['cust_lng'] = [float(loc.split(',')[1]) for loc in c_locs]\n",
    "\n",
    "        cust_lats.append(df['cust_lat'])\n",
    "        cust_lngs.append(df['cust_lng'])\n",
    "\n",
    "        df_gb = df.groupby('vehicle_id') # Don't group on 'de_id' \n",
    "        for v_id in vehicle_ids:\n",
    "            curr_group = df_gb.get_group(v_id)\n",
    "            first_mile_dist, last_mile_dist = distance_this_driver(curr_group[['cust_lat', 'cust_lng']].values, v_id)\n",
    "            total_first_mile += first_mile_dist \n",
    "            total_last_mile += last_mile_dist\n",
    "        # avg cost for this day:\n",
    "        if len(vehicle_ids)>0: # for day 3 in city 'C': for some reason, no order gets delivered by foodmatch hence 0 vehicle ids corresponding to 'DELIVER' in 'data'\n",
    "            first_mile_cost += (total_first_mile/len(vehicle_ids))\n",
    "            last_mile_cost += (total_last_mile/len(vehicle_ids)) \n",
    "    # avg cost over all days:\n",
    "    first_mile_cost = first_mile_cost/num_days\n",
    "    last_mile_cost = last_mile_cost/num_days \n",
    "    cost = first_mile_cost + last_mile_cost \n",
    "    # print(first_mile_cost, last_mile_cost, cost)\n",
    "    return first_mile_cost, last_mile_cost, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-mile cost: distance travelled to reach the assigned zone center (proxy for zone) from home\n",
    "# last-mile cost: distance travelled to pick-up and deliver orders \n",
    "\n",
    "first_mile, last_mile, total = get_cost(algo, num_days)\n",
    "\n",
    "print(\"Assignment cost:\", first_mile)\n",
    "print(\"Delivery cost:\", last_mile)\n",
    "print(\"Total Cost:\", total) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
