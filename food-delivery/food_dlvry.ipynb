{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import warnings # need to be imported before other imports\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os \n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import glob \n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import ortools                       \n",
    "import logging\n",
    "import datetime\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser  \n",
    "import plotly.express as px  \n",
    "from shapely import geometry\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.geocoders import Nominatim  \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=26) \n",
    "matplotlib.rc('ytick', labelsize=26) \n",
    "\n",
    "plt.rcParams['font.size'] = '26'\n",
    "plt.rcParams['figure.figsize'] = (10,7.5)\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO THIS IF RUNNING FOR ONE INSTANCE ONLY\n",
    "# # COMMENT THIS CELL IF RUNNING MULTIPLE INSTANCES PARALELLY\n",
    "\n",
    "# assigns_path = \"./assign_results\"\n",
    "# if os.path.exists(assigns_path):\n",
    "#     shutil.rmtree(assigns_path)\n",
    "\n",
    "# logs_path = \"./logs\"\n",
    "# if os.path.exists(logs_path):\n",
    "#     shutil.rmtree(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE IN FINAL VERSION   \n",
    "**bash command**  \n",
    "ipython food_dlvry.ipynb city num_drivers num_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/home/daman/.local/share/jupyter/runtime/kernel-33887320-22a0-45b5-a4ce-dea0cb28156d.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m day_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m                        \u001b[38;5;66;03m# takes values in {1, 2, 3, 4, 5, 6}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m w1, w2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m                  \u001b[38;5;66;03m# w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m NUM_DRIVERS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m NUM_SIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/home/daman/.local/share/jupyter/runtime/kernel-33887320-22a0-45b5-a4ce-dea0cb28156d.json'"
     ]
    }
   ],
   "source": [
    "# REMOVE sys.argv[x] IN FINAL VERSION\n",
    "# Parameters\n",
    "city = str(sys.argv[1])            # takes values in {'A', 'B'}\n",
    "flag = 0                           # takes values in {0, 1, 2}\n",
    "day_num = 1                        # takes values in {1, 2, 3, 4, 5, 6}\n",
    "w1, w2 = 0.5, 0.5                  # w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\n",
    "NUM_DRIVERS = int(sys.argv[2])     # Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\n",
    "NUM_SIM = int(sys.argv[3])         # Number of similar drivers for each driver to be considered in the fairness constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REMOVE sys.argv[x] IN FINAL VERSION\n",
    "# # Parameters\n",
    "# city = 'B'            # takes values in {'A', 'B'}\n",
    "# flag = 0                           # takes values in {0, 1, 2}\n",
    "# day_num = 1                        # takes values in {1, 2, 3, 4, 5, 6}\n",
    "# w1, w2 = 0.5, 0.5                  # w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\n",
    "# NUM_DRIVERS = -1     # Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\n",
    "# NUM_SIM = 5         # Number of similar drivers for each driver to be considered in the fairness constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B -1 5\n"
     ]
    }
   ],
   "source": [
    "# REMOVE THIS CELL IN FINAL VERSION\n",
    "print(city, NUM_DRIVERS, NUM_SIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flag**  \n",
    "\"flag\" decides which distance metric/measure to consider:    \n",
    "0: euclidean distance (or physical distance)   \n",
    "1: rating   \n",
    "2: combination of euclidean distance and rating   \n",
    "   where 'w1' is weight given to euclidean distance and 'w2' is weight given to rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASETS\n",
    "import glob \n",
    "\n",
    "driver_files_A = sorted(glob.glob(\"data_anon/driver_locs/A/driver_data_A_day*.csv\"))\n",
    "driver_files_B = sorted(glob.glob(\"data_anon/driver_locs/B/driver_data_B_day*.csv\"))\n",
    "\n",
    "num_days = len(driver_files_A) \n",
    "assert len(driver_files_A)==len(driver_files_B), \"error in reading data or incomplete data\"\n",
    "\n",
    "driver_dfs_A = [pd.read_csv(driver_files_A[idx]) for idx in range(num_days)]\n",
    "driver_dfs_B = [pd.read_csv(driver_files_B[idx]) for idx in range(num_days)]\n",
    "driver_dfs_dict = {'A': driver_dfs_A, 'B': driver_dfs_B}\n",
    "\n",
    "zone_df_A = pd.read_csv(\"data_anon/zone_data/zone_data_A.csv\")\n",
    "zone_df_B = pd.read_csv(\"data_anon/zone_data/zone_data_B.csv\")\n",
    "zone_dfs_dict = {'A': zone_df_A, 'B': zone_df_B}\n",
    "\n",
    "income_df_A = pd.read_csv(\"data_anon/income_data/incomes_A.csv\")\n",
    "income_df_B = pd.read_csv(\"data_anon/income_data/incomes_B.csv\")\n",
    "income_dfs_dict = {'A': income_df_A, 'B': income_df_B}\n",
    "\n",
    "base_zone_A = pd.read_csv(\"data_anon/base_zones/A_base_zones.csv\")\n",
    "base_zone_B = pd.read_csv(\"data_anon/base_zones/B_base_zones.csv\")\n",
    "base_zones_dict = {'A': base_zone_A, 'B': base_zone_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "#'city' takes values in {'A'}\n",
    "\n",
    "def driver_union(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding union of all the drivers over the days \n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city] \n",
    "    num_days = len(driver_dfs)\n",
    "    \n",
    "    driver_udf = pd.concat([driver_dfs[idx] for idx in range(num_days)])\n",
    "    driver_udf = driver_udf.drop('Unnamed: 0', axis=1)\n",
    "    driver_udf = driver_udf.drop_duplicates('de_id').reset_index().drop('index', axis=1)\n",
    "    \n",
    "    return driver_udf\n",
    "\n",
    "\n",
    "def driver_intersection(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding intersection of all the drivers over the days\n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city]\n",
    "    driver_idf = reduce(lambda left,right: pd.merge(left,right,on='de_id'), driver_dfs)\n",
    "    driver_idf = driver_idf[['de_id', 'lat_x', 'lng_x']]\n",
    "    driver_idf = driver_idf.loc[:, ~driver_idf.columns.duplicated()] \n",
    "    driver_idf = driver_idf.rename(columns={'lat_x':'lat', 'lng_x':'lng'})\n",
    "    \n",
    "    return driver_idf\n",
    "\n",
    "\n",
    "def drivers_zones(drivers_dict, zones_dict):\n",
    "    \"\"\"\n",
    "    To get the data to be input to fair_clustering: \"driver_locs\" and \"zone_locs\"\n",
    "    \"\"\"\n",
    "    driver_idf = driver_intersection(drivers_dict) \n",
    "    \n",
    "    # finding \"driver_locs\":\n",
    "    driver_locs = driver_idf[['lat', 'lng']] \n",
    "    driver_locs = driver_locs.values \n",
    "    \n",
    "    # finding \"zone_locs\":\n",
    "    zone_df = zones_dict[city]\n",
    "    zone_locs = np.array(zone_df[['lat', 'lng']])\n",
    "    \n",
    "    return driver_locs, zone_locs\n",
    "\n",
    "\n",
    "def get_capacities(zones_dict):\n",
    "    \"\"\"\n",
    "    returns \"lower_caps\" and \"upper_caps\"\n",
    "    lower_caps: [1 x num_centres] array with lower capacity of each zone\n",
    "    upper_caps: [1 x num_centres] array with upper capacity of each zone\n",
    "    \"\"\"\n",
    "    zone_df = zones_dict[city] \n",
    "\n",
    "    lower_caps = 0.3*zone_df['avg_cap'].values\n",
    "    upper_caps = 1.0*zone_df['avg_cap'].values\n",
    "    \n",
    "    return lower_caps, upper_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGER\n",
    "\n",
    "import logging\n",
    "\n",
    "logs_path = \"./logs\"\n",
    "if not os.path.exists(logs_path):\n",
    "    os.mkdir(logs_path)\n",
    "\n",
    "logging.basicConfig(filename=f\"logs/{city}_{NUM_DRIVERS}_{NUM_SIM}.log\", format='%(asctime)s  %(message)s', filemode='w')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the inputs to fair_clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_udf = driver_union(driver_dfs_dict)\n",
    "driver_idf = driver_intersection(driver_dfs_dict) \n",
    "\n",
    "driver_locs, zone_locs = drivers_zones(driver_dfs_dict, zone_dfs_dict)\n",
    "num_drivers, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "lower_caps, upper_caps = get_capacities(zone_dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(d_loc, z_loc):\n",
    "    lat1, lng1 = d_loc[0], d_loc[1]\n",
    "    lat2, lng2 = z_loc[0], z_loc[1]\n",
    "    dist = np.sqrt(np.power(lat1-lat2, 2) + np.power(lng1-lng2, 2))\n",
    "    return dist\n",
    "\n",
    "def L2Distance(data):\n",
    "  # \"data\": latitude-longitude level locations \n",
    "  transposed = np.expand_dims(data, axis = 1)\n",
    "  distance = np.power(data - transposed, 2)\n",
    "  distance = np.power(np.abs(distance).sum(axis = 2), 0.5) \n",
    "  return distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of similar drivers: 20.317477876106196\n"
     ]
    }
   ],
   "source": [
    "# # REMOVE THIS IN THE FINAL VERSION\n",
    "# # used for creating sim2dnr_city\n",
    "# driver_dists = L2Distance(driver_locs)\n",
    "# fair_dist = driver_dists.mean()/19\n",
    "# # print(fair_dist)\n",
    "\n",
    "# num_similar_drivers = []\n",
    "# for idx in range(len(driver_dists)):\n",
    "#     curr_driver = driver_dists[idx]\n",
    "#     # the drivers 'similar' to this driver are the ones within fair_distance from this driver\n",
    "#     num_sim = curr_driver[curr_driver<=fair_dist].shape[0]\n",
    "#     # print(num_sim)\n",
    "#     num_similar_drivers.append(num_sim) \n",
    "\n",
    "# # print(num_similar_drivers)\n",
    "# print(\"Number of similar drivers:\", np.mean(num_similar_drivers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? REMOVE THIS CELL IN THE FINAL VERSION\n",
    "# fair_dist = driver_dists.mean()/dnr\n",
    "# dnr to NUM_SIM maps: (tells the number of similar drivers per driver for a given dnr (hence fair_dist))\n",
    "sim2dnr_A = {80:4, 60:5, 30:8, 20:11, 15:14, 10:21, 7:28, 5:53} \n",
    "sim2dnr_B = {70:8, 60:9, 30:14, 20:19, 15:23, 10:32, 7:45, 5:65}\n",
    "# these values are true only when all the drivers in driver_idf are considered !\n",
    "\n",
    "if city=='A':\n",
    "    assert NUM_SIM in sim2dnr_A.keys(), f\"Dnr for this NUM_SIM={NUM_SIM} is not saved!\\\n",
    "                                        \\nAllowed NUM_SIM values are {list(sim2dnr_A.keys())}\"\n",
    "elif city=='B':\n",
    "    assert NUM_SIM in sim2dnr_B.keys(), f\"Dnr for this NUM_SIM={NUM_SIM} is not saved!\\\n",
    "                                        \\nAllowed NUM_SIM values are {list(sim2dnr_B.keys())}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of similar drivers: 5.0\n"
     ]
    }
   ],
   "source": [
    "if NUM_DRIVERS!=-1 and NUM_DRIVERS <= driver_locs.shape[0]:\n",
    "    driver_locs = driver_locs[:NUM_DRIVERS]\n",
    "\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "\n",
    "# finding number of drivers within fair distance for each driver\n",
    "if city=='A':\n",
    "    dnr = sim2dnr_A[NUM_SIM]\n",
    "elif city=='B':\n",
    "    dnr = sim2dnr_B[NUM_SIM]\n",
    "fair_dist = driver_dists.mean()/dnr\n",
    "# print(fair_dist)\n",
    "\n",
    "num_similar_drivers = []\n",
    "for idx in range(len(driver_dists)):\n",
    "    curr_driver = driver_dists[idx]\n",
    "    # the drivers 'similar' to this driver are the ones within fair_distance from this driver\n",
    "    num_sim = curr_driver[curr_driver<=fair_dist].shape[0]\n",
    "    # print(num_sim)\n",
    "    num_similar_drivers.append(num_sim) \n",
    "\n",
    "# print(num_similar_drivers)\n",
    "print(\"Number of similar drivers:\", np.mean(num_similar_drivers))\n",
    "logger.info(f\"Number of similar drivers: {np.mean(num_similar_drivers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of lower capacities: 446.7\n",
      "Sum of (adjusted) lower capacities: 446.7\n",
      "Sum of upper capacities: 1489.0\n"
     ]
    }
   ],
   "source": [
    "# REMOVE THIS IN THE FINAL VERSION\n",
    "# Adjust lower_caps for given NUM_DRIVERS\n",
    "# Do not adjust upper_caps\n",
    "num_drivers = driver_locs.shape[0]\n",
    "print(f\"Sum of lower capacities: {lower_caps.sum()}\")\n",
    "while(lower_caps.sum()>num_drivers):\n",
    "    # print(lower_caps)\n",
    "    for idx in range(lower_caps.shape[0]):\n",
    "        lower_caps[idx] = max(lower_caps[idx]-50, 0)\n",
    "\n",
    "print(f\"Sum of (adjusted) lower capacities: {lower_caps.sum()}\")\n",
    "print(f\"Sum of upper capacities: {upper_caps.sum()}\")\n",
    "      \n",
    "assert num_drivers>lower_caps.sum() and num_drivers<upper_caps.sum(), \\\n",
    "\"This set of num_drivers, lower_caps and upper_caps will lead to an infeasible solution !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(977, 21)\n"
     ]
    }
   ],
   "source": [
    "num_drivers, num_zones = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "dz_dist = np.zeros(shape=(num_drivers, num_zones))\n",
    "for d_idx, driver in enumerate(driver_locs):\n",
    "    d_dist = np.zeros(num_zones)\n",
    "    for z_idx, zone in enumerate(zone_locs):\n",
    "        dist = euclidean_distance(driver, zone)\n",
    "        d_dist[z_idx] = dist \n",
    "    dz_dist[d_idx] = d_dist\n",
    "\n",
    "# print(dz_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pa(d_dist, k):\n",
    "    prohibited_assignments = np.zeros(shape=(num_drivers, num_zones))\n",
    "    \n",
    "    for d_idx, d_dist in enumerate(d_dist):\n",
    "        idx = np.argpartition(d_dist, k) \n",
    "        prohibited_assignments[d_idx][idx[k:]] = 1 # set the indices NOT corresponding to k-smallest elements \n",
    "    \n",
    "    return prohibited_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings to sellers:\n",
    "from scipy.stats import truncnorm\n",
    "from numpy.random import SeedSequence \n",
    "from numpy.random import default_rng\n",
    "\n",
    "def get_truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm( (low-mean)/sd, (upp-mean)/sd, loc=mean, scale=sd) \n",
    "\n",
    "def generate_ratings(num_drivers):\n",
    "    mean = 3.5\n",
    "    sd = 1\n",
    "    min_rating = 0.0\n",
    "    max_rating = 5.0\n",
    "    seedVal = 36778738061272522495168595294022739449 # arbitrary\n",
    "    rng = default_rng(seedVal)\n",
    "    dist = get_truncated_normal(mean, sd, min_rating, max_rating)\n",
    "    ratings = dist.rvs(num_drivers, random_state=rng)\n",
    "    ratings = [round(x, 1) for x in ratings]\n",
    "    return ratings\n",
    "\n",
    "def abs_difference(ratings):\n",
    "    transposed = np.expand_dims(ratings, axis=1)\n",
    "    diff = abs(ratings-transposed) \n",
    "    return diff   \n",
    "\n",
    "def minmax(distance, fair_distance):\n",
    "    num_samples = len(distance)\n",
    "    mx, mn = distance.max(), distance.min()\n",
    "    dists = distance.flatten()\n",
    "    dists = np.asarray( [((x-mn)/(mx-mn)) for x in dists] )\n",
    "    distance = dists.reshape((num_samples, num_samples))\n",
    "    fair_distance = (fair_distance-mn)/(mx-mn)\n",
    "    return distance, fair_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_ids = zone_dfs_dict[city]['zone_id']\n",
    "zone_id2idx = {zone_id: idx for idx, zone_id in enumerate(zone_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FairAssign**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will go with the default parameters of cplex:\n",
    "from cplex import Cplex\n",
    "model = Cplex()\n",
    "model.parameters.simplex.tolerances.feasibility.get(),\\\n",
    "model.parameters.simplex.tolerances.optimality.get(),\\\n",
    "model.parameters.simplex.tolerances.markowitz.get()      \n",
    "\n",
    "model.parameters.workmem.set(10240) # 10GB  \n",
    "model.parameters.emphasis.memory.set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Clustering - LPP contstraints and Cplex\n",
    "from cplex import Cplex\n",
    "# from lp_tools import *\n",
    "from lp_tools_kn import * \n",
    "\n",
    "alpha_fair = 2\n",
    "\n",
    "def fair_clustering(dataset, centres, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "  # Step 1: \t Create an instance of Cplex \n",
    "  problem = Cplex()\n",
    "  problem.parameters.simplex.tolerances.feasibility.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.optimality.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.markowitz.set(float(0.9999)) \n",
    "  problem.parameters.emphasis.memory.set(1)\n",
    "  problem.parameters.workmem.set(10240)\n",
    "\n",
    "  # Step 2: \t Declare that this is a minimization problem\n",
    "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
    "    \n",
    "  \"\"\"\n",
    "   Step 3.   Declare and  add variables to the model. \n",
    "        The function prepare_to_add_variables (dataset, centres) prepares all the required information for this stage.\n",
    "  \n",
    "    objective: a list of coefficients (float) in the linear objective function\n",
    "    lower bound: a list of floats containing the lower bounds for each variable\n",
    "    upper bound: a list of floats containing the upper bounds for each variable\n",
    "    variable_names: a list of strings that contains the name of the variables\n",
    "  \"\"\"\n",
    "  ## if working with \"lp_tools\":\n",
    "  print(\"Adding Variables...\")\n",
    "  \n",
    "  # objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
    "  ## if working with \"lp_tools_kn\": \n",
    "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres, prohibited_assignments)\n",
    "  problem.variables.add(\n",
    "      obj = objective,\n",
    "      lb = lower_bound,\n",
    "      ub = upper_bound,\n",
    "      names = variable_names\n",
    "     \n",
    "    )\n",
    "  \n",
    "  print(\"Variables Added !\")\n",
    "    \n",
    "    \n",
    "  \"\"\"\n",
    "  Step 4.   Declare and add constraints to the model.\n",
    "            There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
    "            Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
    "            The function prepare_to_add_constraints(dataset, centres) prepares the required data for this step.\n",
    "  \n",
    "   coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
    "   senses: a list of strings that identifies whether the corresponding constraint is\n",
    "           an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
    "   rhs: a list of floats corresponding to the rhs of the constraints.\n",
    "   constraint_names: a list of string corresponding to the name of the constraint\n",
    "  \"\"\"\n",
    "  print(\"Adding Constraints...\")\n",
    "    \n",
    "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair, fair_distance, ratings, flag)\n",
    "  print(\"num_constraints:\", len(senses)) \n",
    "  logger.info(f\"\\t\\t\\tnum_constraints = {len(senses)}\")\n",
    "  problem.linear_constraints.add(\n",
    "      rhs = rhs,\n",
    "      senses = senses,\n",
    "      names = row_names\n",
    "    )\n",
    "  problem.linear_constraints.set_coefficients(coefficients)\n",
    "\n",
    "  print(\"Constraints Added !\")\n",
    "    \n",
    "  # Step 5.\tSolve the problem\n",
    "  problem.solve()\n",
    "\n",
    "  result = {\n",
    "    \"status\": problem.solution.get_status(),\n",
    "    \"success\": problem.solution.get_status_string(),\n",
    "    \"objective\": problem.solution.get_objective_value(),\n",
    "    \"assignment\": problem.solution.get_values(),\n",
    "  }\n",
    "    \n",
    "  qm = problem.solution.quality_metric  \n",
    "  print(\"Solution Quality:\", problem.solution.get_float_quality([qm.max_x, qm.max_primal_infeasibility]))\n",
    "  \n",
    "  # print(\"Status:\", result['status']) # outputs a number: \"1\" for optimal solution, \"2\" for unbounded ray and \"3\" for infeasible solution\n",
    "  solution_status = result['status']\n",
    "  assert solution_status==1, \"Solution isn't optimal !\"\n",
    "\n",
    "  print(\"Status:\", problem.solution.get_status_string()) # optimal, unbounded ray, infeasible\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Assignment of drivers to the FFCs / warehouses\n",
    "import copy\n",
    "import dependent_routing as dp\n",
    "\n",
    "# configParser.read(configFilePath)\n",
    "\n",
    "num_samples, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "def fair_assignment(prob_dis, driver_loc):\n",
    "  '''Assigning the driver using the probaility distribution using dependent rounding'''  \n",
    "  \n",
    "  # \"prob_dis\" is the result of the Fair-LP program \"fair_clustering\"  \n",
    "  prob_dist = copy.deepcopy(prob_dis)\n",
    "  # print(\"prob_dist shape [num_drivers x num_ffc]:\", prob_dist.shape)\n",
    "\n",
    "  rounding = dp.DependentRounding(prob_dist)\n",
    "  rounding._buildGraph(prob_dist)\n",
    "  final_assignment = rounding.round()\n",
    "  final_assignment = np.around(final_assignment,2)\n",
    "\n",
    "  driver_df = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df['ffc_index'] = -1 # unassigned\n",
    "\n",
    "  for i in range(num_samples):\n",
    "    for j in range(num_centres):\n",
    "      if abs(final_assignment[i][j]-1) < 0.01: \n",
    "        driver_df.at[i,'ffc_index'] = j\n",
    "        \n",
    "  return driver_df, final_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(probs):\n",
    "    \"\"\"\n",
    "    To cope with bound violations which can occur upto the feasibility parameter range \n",
    "    So the lower bound of 0.0 on the probabilities can get violated and the values can go down to (0-feasibility_parameter_value)\n",
    "    \"\"\"\n",
    "    for i in range(len(probs)):\n",
    "        last_pos_index = -1\n",
    "        neg_value = 0\n",
    "        \n",
    "        for j in range(len(probs[0])):\n",
    "            assert probs[i][j] >= -1e-6 \n",
    "            \n",
    "            if probs[i][j] < 0:\n",
    "                neg_value += probs[i][j]\n",
    "                probs[i][j] = 0\n",
    "            elif probs[i][j] > 0:\n",
    "                last_pos_index = j\n",
    "\n",
    "        max_pos_index = np.argmax(probs[i])\n",
    "        probs[i][max_pos_index] += neg_value\n",
    "        \n",
    "        assert probs[i][max_pos_index] > 0\n",
    "        \n",
    "    return probs\n",
    "\n",
    "\n",
    "def picklify(ds, filepath):\n",
    "    pickling_on = open(filepath, \"wb\")\n",
    "    pickle.dump(ds, pickling_on)\n",
    "    pickling_on.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main :\n",
    "def FairAssign_solver(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "    # Fair-LP:\n",
    "    # lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    try:\n",
    "        lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    except:\n",
    "        logger.error(\"Solution Non-optimal (Unbounded Ray or Infeasible) !\")\n",
    "        return None, None\n",
    "    prob_dis = np.reshape(lp_output['assignment'][:num_samples*num_centres], (-1, num_centres))\n",
    "    \n",
    "    try:\n",
    "        prob_dist = sanityCheck(copy.deepcopy(prob_dis)) # this might raise an assertion error\n",
    "    except:\n",
    "        logger.error(\"Sanity Check Assertion !\")\n",
    "        return None, None\n",
    "    \n",
    "    # Randomized Dependent Rounding:\n",
    "    try:\n",
    "        df = fair_assignment(prob_dist, driver_locs)[0] # this might raise an assertion error\n",
    "        final_assignment = df['ffc_index'].values\n",
    "    except:\n",
    "        logger.error(\"Dependent Rounding Assertion !\")\n",
    "        return prob_dist, None\n",
    "    \n",
    "    return prob_dist, final_assignment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# drivers : 977\n",
      "# zones : 21\n",
      "Adding Variables...\n",
      "Variables Added !\n",
      "Adding Constraints...\n",
      "num_constraints: 85041\n",
      "Constraints Added !\n",
      "Version identifier: 22.1.0.0 | 2022-03-09 | 1a383f8ce\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "CPXPARAM_Emphasis_Memory                         1\n",
      "CPXPARAM_Simplex_Tolerances_Markowitz            0.99990000000000001\n",
      "CPXPARAM_Simplex_Tolerances_Optimality           1.0000000000000001e-09\n",
      "CPXPARAM_Simplex_Tolerances_Feasibility          1.0000000000000001e-09\n",
      "CPXPARAM_WorkMem                                 10240\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 9971262 columns.\n",
      "Reduced LP has 85041 rows, 61551 columns, and 348789 nonzeros.\n",
      "Presolve time = 3.77 sec. (2055.31 ticks)\n",
      "Compression time = 3.55 sec. (9.65 ticks)\n",
      "Compression reduced 560.82 MB A matrix to 66.61 MB\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =           905.150496\n",
      "Iteration:  1342   Dual objective     =     301500625.753911\n",
      "Iteration:  1967   Dual objective     =     309600619.095869\n",
      "Iteration:  3269   Dual objective     =     520000421.689637\n",
      "Iteration:  4515   Dual objective     =     520000422.006802\n",
      "Iteration:  5302   Dual objective     =     520000422.036873\n",
      "Iteration:  6023   Dual objective     =     520000422.056254\n",
      "Perturbation started.\n",
      "Iteration:  6363   Dual objective     =     520000422.056264\n",
      "Iteration:  7080   Dual objective     =     520000422.074454\n",
      "Iteration:  7732   Dual objective     =     520000422.077574\n",
      "Iteration:  8161   Dual objective     =     520000422.077615\n",
      "Iteration:  8488   Dual objective     =     520000422.078822\n",
      "Iteration:  8819   Dual objective     =     520000422.078827\n",
      "Iteration:  9366   Dual objective     =     520000422.078833\n",
      "Iteration:  9897   Dual objective     =     520000422.078833\n",
      "Removing perturbation.\n",
      "Iteration: 10092   Dual infeasibility =             0.000019\n",
      "Iteration: 10145   Dual objective     =     520000422.078715\n",
      "Reperturbation started.\n",
      "Iteration: 10448   Dual objective     =     520000422.078719\n",
      "Removing perturbation.\n",
      "Iteration: 10859    Objective     =     520000422.078719\n",
      "Solution Quality: [1.0, 1.1102230246251565e-16]\n",
      "Status: optimal\n"
     ]
    }
   ],
   "source": [
    "print(f\"# drivers : {num_drivers}\")\n",
    "print(f\"# zones : {num_zones}\")\n",
    "\n",
    "if city=='A':\n",
    "    nk_list = [num_zones//3]\n",
    "elif city=='B':\n",
    "    nk_list = [num_zones//3]\n",
    "print(\"How many nearest zones? :\", nk_list)\n",
    "\n",
    "k_list = [(x-1) for x in nk_list] # [7, 5, 3] assign only to k-nearest zones \n",
    "\n",
    "if city=='A':\n",
    "    fd_dnr = sim2dnr_A[NUM_SIM]\n",
    "elif city=='B':\n",
    "    fd_dnr = sim2dnr_B[NUM_SIM]\n",
    "\n",
    "fd_list = [(driver_dists.mean()/alpha) for alpha in [fd_dnr]] # fair_distances\n",
    "ratings = generate_ratings(num_drivers)\n",
    "\n",
    "num_runs = 1\n",
    "# k_list and fd_list contain hyperparameters\n",
    "\n",
    "k_fd_dict = {k:\\\n",
    "                {fd_idx:\\\n",
    "                    {num_run:\\\n",
    "                        {'p_dist':None, 'assignment':None}\n",
    "                        for num_run in range(num_runs)\n",
    "                    } \n",
    "                    for fd_idx in range(len(fd_list))\n",
    "                } \n",
    "            for k in k_list\n",
    "            }\n",
    "\n",
    "assign_results_path = f\"assign_results/results_{city}/\"\n",
    "if not os.path.exists(assign_results_path):\n",
    "    os.makedirs(assign_results_path) # directory to store the results of FairAssign_solver\n",
    "\n",
    "# shutil.rmtree(assign_results_path)\n",
    "# os.mkdir(assign_results_path) # doesn't work for nested directories\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for k in k_list:\n",
    "    logger.info(f\"Considering k = {k+1} nearest zones\")\n",
    "    prhbtd_assigns = get_pa(dz_dist, k)\n",
    "    \n",
    "    for f_idx, fair_distance in enumerate(fd_list):\n",
    "        logger.info(f\"\\tfair_distance = {fair_distance}\")\n",
    "        for num_run in range(num_runs):\n",
    "            logger.info(f\"\\t\\tnum_run = {num_run}\")\n",
    "            prob_dist, final_assignment = FairAssign_solver(driver_locs, zone_locs, lower_caps, upper_caps, fair_distance, prhbtd_assigns)\n",
    "            k_fd_dict[k][f_idx][num_run]['p_dist'] = prob_dist\n",
    "            k_fd_dict[k][f_idx][num_run]['assignment'] = final_assignment \n",
    "            \n",
    "    # Store intermediate results as well as fail-safe:\n",
    "    # saving current state of \"k_fd_dict\":\n",
    "    filepath = os.path.join(assign_results_path, f\"dict_k={k+1}_{NUM_DRIVERS}_{NUM_SIM}.pickle\")\n",
    "    # print(filepath)\n",
    "    picklify(k_fd_dict, filepath)\n",
    "end = time.time()\n",
    "print(f\"Execution time: {(end-start)/3600}hrs\")\n",
    "logger.info(f\"Execution time: {(end-start)/3600}hrs\")\n",
    "\n",
    "final_file_path = os.path.join(assign_results_path, f\"Assignments_{city}_{NUM_DRIVERS}_{NUM_SIM}.pickle\")\n",
    "picklify(k_fd_dict, final_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment(result_path, driver_locs, k):\n",
    "    '''\n",
    "    returns ffc_index (or zone index) for each driver based on the \n",
    "    '''\n",
    "    pickle_off = open(result_path, \"rb\")\n",
    "    assignments = pickle.load(pickle_off)\n",
    "    prob_dist = assignments[k-1][0][0]['p_dist']\n",
    "    ## get assignment by applying dependent rounding: \n",
    "    df = fair_assignment(prob_dist, driver_locs)[0] \n",
    "    final_assignment = df['ffc_index'].values\n",
    "\n",
    "    return final_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = f\"assign_results/results_{city}/Assignments_{city}_{NUM_DRIVERS}_{NUM_SIM}.pickle\" \n",
    "\n",
    "assignment = get_assignment(result_path, driver_locs, nk_list[0]) \n",
    "\n",
    "assignment_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "# original base zones\n",
    "assignment_df = pd.merge(assignment_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "assignment_df['bz_idx'] = assignment_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "assignment_df['fa_bz_idx'] = assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      de_id        lat        lng  base_zone  bz_idx  fa_bz_idx\n",
      "0  335342.0  21.033774  62.556638      369.0       2         11\n",
      "1  539137.0  21.018745  62.585875      502.0      14          0\n",
      "2  570646.0  21.032340  62.582523      502.0      14          3\n",
      "3  607721.0  21.034523  62.569361      368.0       1         10\n",
      "4  117645.0  21.029624  62.566421      368.0       1          1\n"
     ]
    }
   ],
   "source": [
    "print(assignment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3500836952.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [24]\u001b[0;36m\u001b[0m\n\u001b[0;31m    verified !\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "verified !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the probability distributions corresponding to each driver. The next step is to use a last-mile delivery algorithm on top of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "- Provide clear instructions on:\n",
    "    - how to generate the assignments and get the de_intervals file on each day \n",
    "    - what changes are needed in the FoodMatch data directory to accomodate the new assignements i.e., where to put the newly generated de_intervals\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data directories in ./data that need to be anonymized:\n",
    "- driver_locs\n",
    "- location_data \n",
    "- orders_data \n",
    "- zone_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation of locations within a given zone\n",
    "\n",
    "# generating random locations withing a zone (given the zone boundary):\n",
    "def random_loc_generator(zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  min_lat, max_lat = min(lats), max(lats)\n",
    "  min_lng, max_lng = min(longs), max(longs)\n",
    "  new_lat, new_lng = random.uniform(min_lat, max_lat), random.uniform(min_lng, max_lng) \n",
    "  return [new_lat, new_lng]\n",
    "\n",
    "# Checking if a given location lies inside a given zone:\n",
    "def path_related_preprocessing(path_bdry):\n",
    "  # exemplar path_bdry: '12.954619258010608,77.6149292592163 12.954680993923494,77.61640664016727 ....'\n",
    "  path_bdry = str(path_bdry)\n",
    "  df = pd.DataFrame({'lts':[], 'lngs':[]})\n",
    "  bdry_locs = path_bdry.split()\n",
    "  lats, longs = [], []\n",
    "  for loc in bdry_locs:\n",
    "    lat, lng = loc.split(',')\n",
    "    lats.append(float(lat))\n",
    "    longs.append(float(lng))\n",
    "  return lats, longs\n",
    "\n",
    "def loc_in_zone(loc, zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  polygon = geometry.MultiPoint(coords).convex_hull\n",
    "  Point_X, Point_Y = loc[0], loc[1]\n",
    "  point = geometry.Point(Point_X, Point_Y)\n",
    "  return point.within(polygon)\n",
    "\n",
    "# code to generate 'm' locations that lie within a given zone:\n",
    "def generate_locs(m, zone_bdry):\n",
    "    new_locs = []\n",
    "    num_generated = 0\n",
    "    while num_generated < m:\n",
    "        new_loc = random_loc_generator(zone_bdry)\n",
    "        sanity_check = loc_in_zone(new_loc, zone_bdry)\n",
    "        if sanity_check:\n",
    "            num_generated += 1\n",
    "            new_locs.append(new_loc)\n",
    "    return new_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_csv(f\"data_anon/location_data/location_{city}.csv\", header=None)\n",
    "location_df = location_df.rename(columns={0:'node_id', 1:'lat', 2:'lng'})\n",
    "# location_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random assignment while maintaining only upper capacity bounds of the zones:\n",
    "def random_dist(day_num, driver_locs, upper_caps):\n",
    "    random.seed(1234+day_num)\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rand_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    rand_df['bz_idx_rand'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = random.randint(1, num_zones)-1\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = random.randint(1, num_zones)-1\n",
    "        rand_df.at[i, 'bz_idx_rand'] = zone \n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return rand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df = random_dist(driver_locs, upper_caps)\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = rand_df['bz_idx_rand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Round Robin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoundRobin assignment while maintaining upper capacity bounds of zones only:\n",
    "def round_robin_dist(day_num, driver_locs):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rr_df = pd.DataFrame(driver_locs, columns=['lat', 'lng'])\n",
    "    rr_df['bz_idx_rr'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = (day_num+1) % num_zones\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = (zone+1) % num_zones \n",
    "        rr_df.at[i, 'bz_idx_rr'] = int(zone)\n",
    "        temp_upper_cap[zone] -= 1 \n",
    "    return rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_df = round_robin_dist(day_num, driver_locs)\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = rr_df['bz_idx_rr']\n",
    "# temp_df = temp_df.sort_values('de_id').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIPA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first day of FoodMatch as the first day of LIPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires simulation results of previous day\n",
    "local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "pre = 'A'\n",
    "sim_path = f'sim_results/sim_results_{city}/sim.results{pre}lipa{day_num}'\n",
    "print(sim_path)\n",
    "\n",
    "data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "    \n",
    "data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "for d_id in driver_idf['de_id']:\n",
    "    try:\n",
    "        day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "    except:\n",
    "        # handles the cases for which d_id is not present in data_deliver \n",
    "        continue\n",
    "                \n",
    "local_incomes_df[f'day{day_num}'] = local_incomes_df['de_id'].map(day_incomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous day incomes are used to determine the next day's assignment:\n",
    "fm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "fm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "fm_inc_df = copy.deepcopy(local_incomes_df)\n",
    "fm_inc_df = pd.merge(fm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "# find day 1 incomes of drivers (FairAssign):\n",
    "prev_incomes_df = copy.deepcopy(fm_inc_df[['de_id', 'lat', 'lng', 'day1']])\n",
    "driver_prev_incomes = prev_incomes_df['day1'].values\n",
    "\n",
    "# find day 1 number of orders in each zone (FairAssign):\n",
    "orders_data = pd.read_csv(f\"data/orders_data/{city}/orders_0{day_num}05.csv\") \n",
    "sim_path = 'sim_results/sim_results_'+city+'/sim.resultsAlipa'+str(day_num)\n",
    "   \n",
    "data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "    \n",
    "vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "\n",
    "cust_zones = df['customer_zone'].unique()\n",
    "cust_zones_gb = df.groupby('customer_zone')\n",
    "\n",
    "orders_per_zone = {key:0 for key in cust_zones if key in zone_ids.values}\n",
    "for key in cust_zones:\n",
    "    if key in zone_ids.values:\n",
    "        orders_per_zone[key] = cust_zones_gb.get_group(key).shape[0] \n",
    "\n",
    "# orders_per_zone = {k: v for k, v in sorted(orders_per_zone.items(), key=lambda item: item[1])}\n",
    "orders_per_zone = {k:v for k, v in sorted(orders_per_zone.items())}\n",
    "zone_prev_incomes = [v for k, v in orders_per_zone.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIPA while maintaining upper capacity bounds of zones only:\n",
    "def lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "\n",
    "    lipa_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    lipa_df['bz_idx_lipa'] = -1\n",
    "\n",
    "    temp_upper_cap = copy.deepcopy(upper_caps)\n",
    "\n",
    "    driver_idx_inc = np.argsort(np.array(driver_prev_incomes))\n",
    "    zone_idx_inc = np.argsort(np.array(zone_prev_incomes))\n",
    "  \n",
    "    j = num_zones-1\n",
    "    for i in driver_idx_inc:\n",
    "        zone = zone_idx_inc[j]\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            j = j-1\n",
    "            zone = zone_idx_inc[j]\n",
    "        lipa_df.at[i, 'bz_idx_lipa'] = zone\n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return lipa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipa_df = lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes)\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = lipa_df['bz_idx_lipa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating modified files for current day 'day_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files corresponding to driver_idf:\n",
    "de_idf_ids = driver_idf['de_id']\n",
    "\n",
    "de_idf_files = []\n",
    "for d_id in de_idf_ids:\n",
    "    file_name = 'data/de_data/'+city +'_de_data/'+str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    de_idf_files.append(file_name)\n",
    "# de_idf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the node_ids:\n",
    "orig_node_ids = []\n",
    "na = 0 # number of files in de_idf_files which are not present in de_intervals/\n",
    "for file in de_idf_files:\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        na += 1\n",
    "        d_id = int(file.split('/')[-1][:-4])\n",
    "        to_drop_idx = temp_df[temp_df['de_id']==d_id].index\n",
    "        temp_df = temp_df.drop(to_drop_idx)\n",
    "        print(to_drop_idx)\n",
    "        continue \n",
    "    # get starting node ids corresponding to all shifts, it will be useful for random generation for unswappable drivers\n",
    "    num_shifts = int(file_df.shape[0]/2)\n",
    "    node_id = [int(file_df.iloc[x*2].values[0].split()[1]) for x in range(num_shifts)]\n",
    "    orig_node_ids.append(node_id)\n",
    "\n",
    "temp_df[\"node_id\"] = orig_node_ids \n",
    "temp_df[\"fa_node_id\"] = temp_df['node_id']\n",
    "\n",
    "print(f\"{na} intersection drivers not found in ../{day_num}/de_intervals/\")\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_files = []\n",
    "for d_id in driver_idf['de_id']:\n",
    "    file_name = 'data/de_data/'+ city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    intersection_files.append(file_name)\n",
    "\n",
    "old_dir_path = f'data/de_data/{city}_de_data/{day_num}/de_int_old'\n",
    "new_dir_path = f'data/de_data/{city}_de_data/{day_num}/de_int_new'\n",
    "\n",
    "if os.path.exists(old_dir_path):\n",
    "    shutil.rmtree(old_dir_path)\n",
    "    \n",
    "if os.path.exists(new_dir_path):\n",
    "    shutil.rmtree(new_dir_path)\n",
    "\n",
    "os.mkdir(old_dir_path)\n",
    "os.mkdir(new_dir_path)\n",
    "\n",
    "for file in intersection_files:\n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        continue \n",
    "    old_path = f'Code/data/de_data/{city}_de_data/{day_num}/de_int_old/{d_id}.csv'\n",
    "    file_df.to_csv(old_path, index=False)\n",
    "    new_path = f'data/de_data/{city}_de_data/{day_num}/de_int_new/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWAPPING LOGIC:\n",
    "# The swapping logic is applicable only to the drivers for which the fa_bz_idx is different from bz_idx\n",
    "rel_df = temp_df[temp_df['bz_idx'] != temp_df['fa_bz_idx']].reset_index()\n",
    "print(rel_df.shape[0])\n",
    "rel_df['paired_de'+str(day_num)] = None\n",
    "\n",
    "# first assign swappable nodes:\n",
    "bz_nodes_dict = {zone_idx:{'freq':0, 'nodes':[], 'paired_de_id':[]} for zone_idx, _ in enumerate(zone_ids)} # it'll contain the frequency of each base zone in rel_df['bz_idx'] as well as the corresponding node_ids in a list\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    z_id = int(rel_df.iloc[idx]['bz_idx'])\n",
    "    n_id = int(rel_df.iloc[idx]['node_id'][0])\n",
    "    paired_did = int(rel_df.iloc[idx]['de_id']) # remove later?\n",
    "    bz_nodes_dict[z_id]['freq'] += 1\n",
    "    bz_nodes_dict[z_id]['nodes'].append(n_id)\n",
    "    bz_nodes_dict[z_id]['paired_de_id'].append(paired_did) # remove later?\n",
    "bz_nodes_dict_store = copy.deepcopy(bz_nodes_dict)\n",
    "# IF a required zone_id in fa_bz_idx is present in bz_nodes_dict then use that data point\n",
    "# ELSE generate a random location in the zone corresponding to the zone_id\n",
    "num_random = 0 # number of drivers for whom random generation of location was done to get the corresponding start node\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    print(idx, end=' ')\n",
    "    z_id = int(rel_df.iloc[idx]['fa_bz_idx'])\n",
    "    if(bz_nodes_dict[z_id]['freq'] > 0):\n",
    "        rel_df.loc[idx, 'fa_node_id'] = bz_nodes_dict[z_id]['nodes'][0]\n",
    "        rel_df.loc[idx, 'paired_de'] = bz_nodes_dict[z_id]['paired_de_id'][0]\n",
    "        # print(bz_nodes_dict[z_id]['paired_de_id'][0])\n",
    "        bz_nodes_dict[z_id]['freq'] -= 1\n",
    "        bz_nodes_dict[z_id]['nodes'].pop(0)\n",
    "        bz_nodes_dict[z_id]['paired_de_id'].pop(0) # remove later?\n",
    "    else:\n",
    "        # randomly generate a location in the zone assigned by FairAssign\n",
    "        zone_df = zone_dfs_dict[city]\n",
    "        # zone_bdry = zone_df[zone_df['zone_id']==z_id]['path'].values[0] # Wrong ! bcz z_id is the index of zone_id\n",
    "        zone_bdry = zone_df.iloc[z_id]['path']\n",
    "        new_loc = generate_locs(1, zone_bdry)[0] \n",
    "        # shift new_loc: convert to anonymized coordinates\n",
    "        if city=='A':\n",
    "            new_loc[0] -= 2.0\n",
    "            new_loc[1] -= 10.0\n",
    "        # based on new_loc, get the closest node_id from location_df\n",
    "        min_dist = 1e9\n",
    "        n_id = -1\n",
    "        for i in range(location_df.shape[0]):\n",
    "            node_loc = [ location_df.iloc[i]['lat'], location_df.iloc[i]['lng'] ]\n",
    "            curr_dist = euclidean_distance(new_loc, node_loc)\n",
    "            if(curr_dist <= min_dist):\n",
    "                min_dist = curr_dist \n",
    "                n_id = location_df.iloc[i]['node_id']     \n",
    "        num_random += 1\n",
    "        num_shifts = int(len(rel_df.iloc[idx]['node_id']))\n",
    "        n_idz = [int(n_id)]*num_shifts\n",
    "        rel_df.at[idx, 'fa_node_id'] = n_idz\n",
    "print()\n",
    "print(f\"{rel_df.shape[0]-num_random} data points out of {rel_df.shape[0]} could be swapped !\")\n",
    "print(f\"{num_random} data points were randomly generated !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_files = []\n",
    "for d_id in temp_df['de_id']:\n",
    "    file_name = 'data/de_data/'+city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    intersection_files.append(file_name)\n",
    "\n",
    "for file in intersection_files:\n",
    "    # d_id = file[40:-4]\n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    file_df = pd.read_csv(file)\n",
    "\n",
    "    old_path = f'data/de_data/{city}_de_data/{day_num}/de_int_old/{d_id}.csv'\n",
    "    file_df.to_csv(old_path, index=False)\n",
    "\n",
    "    new_path = f'Code/data/de_data/{city}_de_data/{day_num}/de_int_new/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the de_interval profiles of swappable or paired drivers are to be swapped \n",
    "# and that of drivers for whom random locations are generated, the profile is to be modified\n",
    "rel_drivers = rel_df['de_id']\n",
    "\n",
    "rel_files = []\n",
    "for d_id in rel_drivers:\n",
    "    file_name = 'data/de_data/'+city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    rel_files.append(file_name)\n",
    "\n",
    "for file in rel_files:\n",
    "    # d_id = file[40:-4] \n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    file_df = pd.read_csv(file)\n",
    "    \n",
    "    swap_node = rel_df[rel_df['de_id']==int(d_id)].paired_de.values[0]\n",
    "    # for those drivers who could be swapped:\n",
    "    if not np.isnan(swap_node):\n",
    "        swap_with_file = 'data/de_data/' + city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(swap_node)) + '.csv'\n",
    "        file_df = pd.read_csv(swap_with_file)\n",
    "    # for those drivers whose starting nodes for each shift were randomly generated\n",
    "    else:\n",
    "        num_shifts = int(file_df.shape[0]/2)\n",
    "        new_start_nodes = rel_df[rel_df['de_id']==int(d_id)].fa_node_id.values[0]\n",
    "        for i in range(num_shifts):\n",
    "            new_node = new_start_nodes[i]\n",
    "            file_df.iloc[i*2] = str(file_df.iloc[0].values[0].split()[0])+ ' ' + str(new_node) \n",
    "\n",
    "    new_path = f'data/de_data/{city}_de_data/{day_num}/de_int_new/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS:\n",
    "\n",
    "def gini_index(incomes):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum() \n",
    "    inc_sum = 0.0\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            inc_sum += abs(incomes[i]-incomes[j])\n",
    "    gini = inc_sum / (2*num*total)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def avg_distance(zone_labels, driver_locs, zone_locs):\n",
    "    \"\"\"\n",
    "    returns the 'cost' of the assignment\n",
    "    zone_labels: indices of the assigned zones\n",
    "    a zone_label 'z' has location zone_locs[z]\n",
    "    \"\"\"\n",
    "    driver_dists = L2Distance(driver_locs) \n",
    "    num = len(zone_labels)\n",
    "    dist = 0.0 \n",
    "    for i in range(num):\n",
    "        assigned_zone = zone_labels[i]\n",
    "        driver_loc, zone_loc = driver_locs[i], zone_locs[int(assigned_zone)]\n",
    "        driver_zone_dist = euclidean_distance(driver_loc, zone_loc)\n",
    "        dist += np.sqrt(driver_zone_dist)\n",
    "    avg_dist = dist/num\n",
    "    return avg_dist                       \n",
    "    \n",
    "\n",
    "def spatial_inequality_index(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "    if flag==0:\n",
    "        driver_dists = L2Distance(driver_locs)\n",
    "    if flag==1:\n",
    "        driver_dists = abs_difference(ratings)\n",
    "    if flag==2:\n",
    "        driver_dists = combined\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    term_i = 0.0    \n",
    "    for i in range(num):\n",
    "        sum_j = 0.0\n",
    "        num_j = 1e-9    \n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j] <= fair_distance and driver_dists[i][j]>0:\n",
    "                num_j += 1\n",
    "                sum_j += abs(incomes[i]-incomes[j])   \n",
    "        term_i += (sum_j / num_j) \n",
    "    \n",
    "    spin_idx = term_i / total \n",
    "    # spin_idx = round(spin_idx, 2)\n",
    "    return spin_idx \n",
    "\n",
    "\n",
    "def income_gap(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "    \"\"\" \n",
    "    difference between incomes between any two drivers per unit distance (within fair_distance) \n",
    "    \"\"\"\n",
    "    alpha = 100\n",
    "\n",
    "    if flag==0:\n",
    "        driver_dists = L2Distance(driver_locs)\n",
    "    if flag==1:\n",
    "        driver_dists = abs_difference(ratings)\n",
    "    if flag==2:\n",
    "        driver_dists = combined\n",
    "    driver_dists = driver_dists * alpha\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    terms = 0.0\n",
    "    num_pair_drivers = 1e-7 # NOT 0 => to avoid division by 0\n",
    "    for i in range(num-1):\n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j]>0:\n",
    "                num_pair_drivers += 1\n",
    "                terms += (abs(incomes[i]-incomes[j])/driver_dists[i][j])\n",
    "    inc_gap = terms/num_pair_drivers\n",
    "    # inc_gap = round(inc_gap, 2)\n",
    "    return inc_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the FoodMatch simulation results for all 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incomes_df(algo):\n",
    "    # get the incomes on all 6 days for all drivers \n",
    "    '''\n",
    "    input string: algo\n",
    "    'fm' : FoodMatch\n",
    "    'fafm' : FairAssign + FoodMatch\n",
    "    '''\n",
    "    local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "    local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "    num_days = 6\n",
    "    day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "    pre = 'A'\n",
    "    for day in range(1, num_days+1):\n",
    "        sim_path = f'sim_results/sim_results_{city}/sim.results{pre}{algo}{day}'\n",
    "        print(sim_path)\n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        # print(data_deliver)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "        data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "        for d_id in driver_idf['de_id']:\n",
    "            try:\n",
    "                day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "            except:\n",
    "                # handles the cases for which d_id is not present in data_deliver \n",
    "                continue  \n",
    "        local_incomes_df[f'day{day}'] = local_incomes_df['de_id'].map(day_incomes)\n",
    "    \n",
    "    cols = ['day1', 'day2', 'day3', 'day4', 'day5', 'day6'] \n",
    "    local_incomes_df['num_orders'] = local_incomes_df[cols].sum(axis=1) \n",
    "\n",
    "    return copy.deepcopy(local_incomes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only FoodMatch: \n",
    "fm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "fm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "fm_incomes_df = get_incomes_df('fm')\n",
    "fm_incomes_df = pd.merge(fm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "# FairAssign then FoodMatch:\n",
    "fafm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "fafm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "fafm_incomes_df = get_incomes_df('fafm')\n",
    "fafm_incomes_df = pd.merge(fafm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "fm_inc_df = copy.deepcopy(fm_incomes_df)\n",
    "fafm_inc_df = copy.deepcopy(fafm_incomes_df)\n",
    "fm_inc_df = pd.merge(fm_incomes_df, fafm_incomes_df[['de_id']])\n",
    "fafm_inc_df = pd.merge(fafm_incomes_df, fm_inc_df[['de_id']])\n",
    "assert fm_inc_df.shape[0]==fafm_inc_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats_fm = fm_inc_df['lat']\n",
    "longs_fm = fm_inc_df['lng']\n",
    "\n",
    "lats_fafm = fafm_inc_df['lat']\n",
    "longs_fafm = fafm_inc_df['lng'] \n",
    "\n",
    "fm_incomes = fm_inc_df['num_orders']\n",
    "fafm_incomes = fafm_inc_df['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(lats, longs, incomes, ratings, combined, fair_dist): \n",
    "    d_locs = [[lat, lng] for lat, lng in zip(lats, longs)]\n",
    "    incomes = np.array(incomes)\n",
    "    gini = gini_index(incomes) \n",
    "    sp_idx = spatial_inequality_index(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    inc_gp = income_gap(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    return gini, sp_idx, inc_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag==0:\n",
    "    driver_dists = driver_dists \n",
    "if flag==1:\n",
    "    driver_dists = ratings_matrix \n",
    "if flag==2:\n",
    "    driver_dists = combined\n",
    "\n",
    "def num_sim_drivers(fd):\n",
    "    ''' \n",
    "    number of similar drivers (i.e., for a given driver, how many drivers are being considered for fairness comparison per)\n",
    "    '''\n",
    "    num_similar_drivers = []\n",
    "    for idx in range(len(driver_dists)):\n",
    "        curr_driver = driver_dists[idx]\n",
    "        num_sim = curr_driver[curr_driver<=fd].shape[0]\n",
    "        num_similar_drivers.append(num_sim) \n",
    "    return np.mean(num_similar_drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the metrics at different fair_dist values: \n",
    "if flag==0:\n",
    "    fair_dist = driver_dists.mean()/8 # actual: cd \n",
    "if flag==1:\n",
    "    fair_dist = 1 # 0.5\n",
    "if flag==2:\n",
    "    fair_dist = 0.04 \n",
    "    \n",
    "def eval_results(lats, longs, incomes):\n",
    "    results = []\n",
    "    for k in range(1, 11):\n",
    "        fd = fair_dist/k \n",
    "        gini, sp_idx, inc_gp = metrics(lats, longs, incomes, ratings, combined, fd)\n",
    "        results.append([fd, num_sim_drivers(fd), gini, sp_idx, inc_gp])\n",
    "    result_df = pd.DataFrame(results)\n",
    "    cols = ['fair_dist', 'sim_drivers', 'gini', 'spatial_ineq', 'income_gap']\n",
    "    result_df.columns = cols\n",
    "    return result_df\n",
    "\n",
    "# FINAL RESULTS:\n",
    "fm_results_df = eval_results(lats_fm, longs_fm, fm_incomes) \n",
    "fafm_results_df = eval_results(lats_fafm, longs_fafm, fafm_incomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Avg. Distance (or Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_this_driver(locs, v_id):\n",
    "    # driver's inital location:\n",
    "    v_init_loc = driver_idf[driver_idf['de_id']==v_id][['lat', 'lng']].values[0]\n",
    "    first_mile_dist = euclidean_distance(locs[0], v_init_loc)\n",
    "    last_mile_dist = 0\n",
    "    for idx in range(1, len(locs)):\n",
    "        prev_loc = locs[idx-1]\n",
    "        curr_loc = locs[idx] \n",
    "        last_mile_dist += euclidean_distance(prev_loc, curr_loc) \n",
    "    return first_mile_dist, last_mile_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the incomes on all 6 days for all drivers \n",
    "num_days = 6\n",
    "\n",
    "def get_cost(algo, num_days):\n",
    "    ''' \n",
    "    algo: str\n",
    "    'fm': FoodMatch \n",
    "    'fafm': FairAssign then FoodMatch\n",
    "    '''\n",
    "    cust_lats, cust_lngs = [], []\n",
    "    first_mile_cost = 0.0 # over all num_days days\n",
    "    last_mile_cost = 0.0 # over all num_days days\n",
    "    for day in range(1, num_days+1):\n",
    "        total_first_mile = 0\n",
    "        total_last_mile = 0\n",
    "        orders_data = pd.read_csv(f\"data/orders_data/{city}/orders_0{day}05.csv\") \n",
    "        sim_path = f'sim_results/sim_results_{city}/sim.resultsA{algo}(day)'\n",
    "        \n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "        df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "        c_locs = df['customer_lat_lng'].values \n",
    "        df['cust_lat'] = [float(loc.split(',')[0]) for loc in c_locs]\n",
    "        df['cust_lng'] = [float(loc.split(',')[1]) for loc in c_locs]\n",
    "\n",
    "        cust_lats.append(df['cust_lat'])\n",
    "        cust_lngs.append(df['cust_lng'])\n",
    "\n",
    "        df_gb = df.groupby('vehicle_id') # Don't group on 'de_id' \n",
    "        for v_id in vehicle_ids:\n",
    "            curr_group = df_gb.get_group(v_id)\n",
    "            first_mile_dist, last_mile_dist = distance_this_driver(curr_group[['cust_lat', 'cust_lng']].values, v_id)\n",
    "            total_first_mile += first_mile_dist \n",
    "            total_last_mile += last_mile_dist\n",
    "        # avg cost for this day:\n",
    "        first_mile_cost += (total_first_mile/len(vehicle_ids))\n",
    "        last_mile_cost += (total_last_mile/len(vehicle_ids)) \n",
    "    # avg cost over all days:\n",
    "    first_mile_cost = first_mile_cost/num_days\n",
    "    last_mile_cost = last_mile_cost/num_days \n",
    "    cost = first_mile_cost + last_mile_cost \n",
    "    # print(first_mile_cost, last_mile_cost, cost)\n",
    "    return first_mile_cost, last_mile_cost, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costs with only FoodMatch:\n",
    "first_mile, last_mile, total = get_cost('fm', num_days)\n",
    "print(total) \n",
    "\n",
    "# costs with FairAssign + FoodMatch: \n",
    "first_mile_, last_mile_, total_ = get_cost('fafm', num_days)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
