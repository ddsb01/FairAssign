{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "- Update driver_locs to have anonymized coordinates only \n",
    "- change 'ahm' to 'cityA' everywhere\n",
    "- Provide clear instructions on:\n",
    "    - how to generate the assignments and get the de_intervals file on each day \n",
    "    - what changes are needed in the FoodMatch data directory to accomodate the new assignements i.e., where to put the newly generated de_intervals\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "# IMPORTING STUFF\n",
    "\n",
    "import os \n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import glob \n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import ortools                       \n",
    "import logging\n",
    "import datetime\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser  \n",
    "import plotly.express as px  \n",
    "from shapely import geometry\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.geocoders import Nominatim  \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=26) \n",
    "matplotlib.rc('ytick', labelsize=26) \n",
    "\n",
    "plt.rcParams['font.size'] = '26'\n",
    "plt.rcParams['figure.figsize'] = (10,7.5)\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'ahm'\n",
    "\n",
    "flag = 0\n",
    "w1, w2 = 0.5, 0.5\n",
    "'''\n",
    "\"flag\" decides which distance metric/measure to consider:\n",
    "0: euclidean distance (or physical distance)\n",
    "1: rating\n",
    "2: combination of euclidean distance and rating\n",
    "   where 'w1' is weight given to euclidean distance and 'w2' is weight given to rating\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASETS\n",
    "import glob \n",
    "\n",
    "driver_files_ahm = sorted(glob.glob(\"data/driver_locs/ahm/driver_data_ahm_day*.csv\"))\n",
    "num_days = len(driver_files_ahm) \n",
    "driver_dfs_ahm = [pd.read_csv(driver_files_ahm[idx]) for idx in range(num_days)]\n",
    "driver_dfs_dict = {'ahm': driver_dfs_ahm}\n",
    "zone_df_ahm = pd.read_csv(\"data/zone_data/zone_data_ahm.csv\")\n",
    "zone_dfs_dict = {'ahm': zone_df_ahm}\n",
    "income_df_ahm = pd.read_csv(\"data/income_data/incomes_ahm.csv\")\n",
    "income_dfs_dict = {'ahm': income_df_ahm} \n",
    "base_zone_ahm = pd.read_csv(\"data/base_zones/ahm_base_zones.csv\")\n",
    "base_zones_dict = {'ahm': base_zone_ahm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "#'city' takes values in {'ahm', 'blr', 'del'}\n",
    "\n",
    "def driver_union(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding union of all the drivers over the days \n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city] \n",
    "    num_days = len(driver_dfs)\n",
    "    \n",
    "    driver_udf = pd.concat([driver_dfs[idx] for idx in range(num_days)])\n",
    "    driver_udf = driver_udf.drop('Unnamed: 0', axis=1)\n",
    "    driver_udf = driver_udf.drop_duplicates('de_id').reset_index().drop('index', axis=1)\n",
    "    \n",
    "    return driver_udf\n",
    "\n",
    "\n",
    "def driver_intersection(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding intersection of all the drivers over the days\n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city]\n",
    "    driver_idf = reduce(lambda left,right: pd.merge(left,right,on='de_id'), driver_dfs)\n",
    "    driver_idf = driver_idf[['de_id', 'lat_x', 'lng_x']]\n",
    "    driver_idf = driver_idf.loc[:, ~driver_idf.columns.duplicated()] \n",
    "    driver_idf = driver_idf.rename(columns={'lat_x':'lat', 'lng_x':'lng'})\n",
    "    \n",
    "    return driver_idf\n",
    "\n",
    "\n",
    "\n",
    "def drivers_zones(drivers_dict, zones_dict):\n",
    "    \"\"\"\n",
    "    To get the data to be input to fair_clustering: \"driver_locs\" and \"zone_locs\"\n",
    "    \"\"\"\n",
    "    driver_idf = driver_intersection(drivers_dict) \n",
    "    \n",
    "    # finding \"driver_locs\":\n",
    "    driver_locs = driver_idf[['lat', 'lng']] \n",
    "    driver_locs = driver_locs.values \n",
    "    \n",
    "    # finding \"zone_locs\":\n",
    "    zone_df = zones_dict[city]\n",
    "    zone_locs = np.array(zone_df[['lat', 'lng']])\n",
    "    \n",
    "    return driver_locs, zone_locs\n",
    "\n",
    "\n",
    "def get_capacities(zones_dict):\n",
    "    \"\"\"\n",
    "    returns \"lower_caps\" and \"upper_caps\"\n",
    "    lower_caps: [1 x num_centres] array with lower capacity of each zone\n",
    "    upper_caps: [1 x num_centres] array with upper capacity of each zone\n",
    "    \"\"\"\n",
    "    zone_df = zones_dict[city] \n",
    "\n",
    "    lower_caps = 0.3*zone_df['avg_cap'].values\n",
    "    upper_caps = 1.0*zone_df['avg_cap'].values\n",
    "    \n",
    "    return lower_caps, upper_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGER\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=city+\".log\", format='%(asctime)s  %(message)s', filemode='w')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the inputs to fair_clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_udf = driver_union(driver_dfs_dict)\n",
    "driver_idf = driver_intersection(driver_dfs_dict) \n",
    "\n",
    "driver_locs, zone_locs = drivers_zones(driver_dfs_dict, zone_dfs_dict)\n",
    "num_drivers, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "lower_caps, upper_caps = get_capacities(zone_dfs_dict)\n",
    "assert driver_locs.shape[0]>lower_caps.sum() and driver_locs.shape[0]<upper_caps.sum(), \\\n",
    "\"This set of num_drivers, lower_caps and upper_caps will lead to an infeasible solution !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(d_loc, z_loc):\n",
    "    lat1, lng1 = d_loc[0], d_loc[1]\n",
    "    lat2, lng2 = z_loc[0], z_loc[1]\n",
    "\n",
    "    dist = np.sqrt(np.power(lat1-lat2, 2) + np.power(lng1-lng2, 2))\n",
    "    return dist\n",
    "\n",
    "def L2Distance(data):\n",
    "  # \"data\": latitude-longitude level locations \n",
    "  transposed = np.expand_dims(data, axis = 1)\n",
    "  distance = np.power(data - transposed, 2)\n",
    "  distance = np.power(np.abs(distance).sum(axis = 2), 0.5) \n",
    "  return distance \n",
    "\n",
    "driver_dists = L2Distance(driver_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_drivers, num_zones = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "dz_dist = np.zeros(shape=(num_drivers, num_zones))\n",
    "for d_idx, driver in enumerate(driver_locs):\n",
    "    d_dist = np.zeros(num_zones)\n",
    "    for z_idx, zone in enumerate(zone_locs):\n",
    "        dist = euclidean_distance(driver, zone)\n",
    "        d_dist[z_idx] = dist \n",
    "    dz_dist[d_idx] = d_dist\n",
    "\n",
    "dz_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pa(d_dist, k):\n",
    "    prohibited_assignments = np.zeros(shape=(num_drivers, num_zones))\n",
    "    \n",
    "    for d_idx, d_dist in enumerate(dz_dist):\n",
    "        idx = np.argpartition(d_dist, k) \n",
    "        prohibited_assignments[d_idx][idx[k:]] = 1 # set the indices NOT corresponding to k-smallest elements \n",
    "    \n",
    "    return prohibited_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings to sellers:\n",
    "from scipy.stats import truncnorm\n",
    "from numpy.random import SeedSequence \n",
    "from numpy.random import default_rng\n",
    "\n",
    "def get_truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm( (low-mean)/sd, (upp-mean)/sd, loc=mean, scale=sd) \n",
    "\n",
    "def generate_ratings(num_drivers):\n",
    "    mean = 3.5\n",
    "    sd = 1\n",
    "    min_rating = 0.0\n",
    "    max_rating = 5.0\n",
    "    seedVal = 36778738061272522495168595294022739449 # arbitrary\n",
    "    rng = default_rng(seedVal)\n",
    "    dist = get_truncated_normal(mean, sd, min_rating, max_rating)\n",
    "    ratings = dist.rvs(num_drivers, random_state=rng)\n",
    "    ratings = [round(x, 1) for x in ratings]\n",
    "    return ratings\n",
    "\n",
    "def abs_difference(ratings):\n",
    "    transposed = np.expand_dims(ratings, axis=1)\n",
    "    diff = abs(ratings-transposed) \n",
    "    return diff   \n",
    "\n",
    "def minmax(distance, fair_distance):\n",
    "    num_samples = len(distance)\n",
    "    mx, mn = distance.max(), distance.min()\n",
    "    dists = distance.flatten()\n",
    "    dists = np.asarray( [((x-mn)/(mx-mn)) for x in dists] )\n",
    "    distance = dists.reshape((num_samples, num_samples))\n",
    "    fair_distance = (fair_distance-mn)/(mx-mn)\n",
    "    return distance, fair_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_ids = zone_dfs_dict[city]['zone_id']\n",
    "zone_id2idx = {zone_id: idx for idx, zone_id in enumerate(zone_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_csv(f\"data/location_data/location_{city}.csv\", header=None)\n",
    "location_df = location_df.rename(columns={0:'node_id', 1:'lat', 2:'lng'})\n",
    "location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation of locations within a given zone\n",
    "\n",
    "# generating random locations withing a zone (given the zone boundary):\n",
    "def random_loc_generator(zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  min_lat, max_lat = min(lats), max(lats)\n",
    "  min_lng, max_lng = min(longs), max(longs)\n",
    "  new_lat, new_lng = random.uniform(min_lat, max_lat), random.uniform(min_lng, max_lng) \n",
    "  return [new_lat, new_lng]\n",
    "\n",
    "# Checking if a given location lies inside a given zone:\n",
    "def path_related_preprocessing(path_bdry):\n",
    "  # exemplar path_bdry: '12.954619258010608,77.6149292592163 12.954680993923494,77.61640664016727 ....'\n",
    "  path_bdry = str(path_bdry)\n",
    "  df = pd.DataFrame({'lts':[], 'lngs':[]})\n",
    "  bdry_locs = path_bdry.split()\n",
    "  lats, longs = [], []\n",
    "  for loc in bdry_locs:\n",
    "    lat, lng = loc.split(',')\n",
    "    lats.append(float(lat))\n",
    "    longs.append(float(lng))\n",
    "  return lats, longs\n",
    "\n",
    "def loc_in_zone(loc, zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  polygon = geometry.MultiPoint(coords).convex_hull\n",
    "  Point_X, Point_Y = loc[0], loc[1]\n",
    "  point = geometry.Point(Point_X, Point_Y)\n",
    "  return point.within(polygon)\n",
    "\n",
    "# code to generate 'm' locations that lie within a given zone:\n",
    "def generate_locs(m, zone_bdry):\n",
    "    new_locs = []\n",
    "    num_generated = 0\n",
    "    while num_generated < m:\n",
    "        new_loc = random_loc_generator(zone_bdry)\n",
    "        sanity_check = loc_in_zone(new_loc, zone_bdry)\n",
    "        if sanity_check:\n",
    "            num_generated += 1\n",
    "            new_locs.append(new_loc)\n",
    "    return new_locs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHMS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FairAssign**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will go with the default parameters of cplex:\n",
    "from cplex import Cplex\n",
    "model = Cplex()\n",
    "model.parameters.simplex.tolerances.feasibility.get(),\\\n",
    "model.parameters.simplex.tolerances.optimality.get(),\\\n",
    "model.parameters.simplex.tolerances.markowitz.get()      \n",
    "\n",
    "model.parameters.workmem.set(10240) # 10GB  \n",
    "model.parameters.emphasis.memory.set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Clustering - LPP contstraints and Cplex\n",
    "from cplex import Cplex\n",
    "# from lp_tools import *\n",
    "from lp_tools_kn import * \n",
    "\n",
    "alpha_fair = 2\n",
    "\n",
    "def fair_clustering(dataset, centres, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "  # Step 1: \t Create an instance of Cplex \n",
    "  problem = Cplex()\n",
    "  problem.parameters.simplex.tolerances.feasibility.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.optimality.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.markowitz.set(float(0.9999)) \n",
    "  problem.parameters.emphasis.memory.set(1)\n",
    "  problem.parameters.workmem.set(10240)\n",
    "\n",
    "  # Step 2: \t Declare that this is a minimization problem\n",
    "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
    "    \n",
    "  \"\"\"\n",
    "   Step 3.   Declare and  add variables to the model. \n",
    "        The function prepare_to_add_variables (dataset, centres) prepares all the required information for this stage.\n",
    "  \n",
    "    objective: a list of coefficients (float) in the linear objective function\n",
    "    lower bound: a list of floats containing the lower bounds for each variable\n",
    "    upper bound: a list of floats containing the upper bounds for each variable\n",
    "    variable_names: a list of strings that contains the name of the variables\n",
    "  \"\"\"\n",
    "  ## if working with \"lp_tools\":\n",
    "  print(\"Adding Variables...\")\n",
    "  \n",
    "  # objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
    "  ## if working with \"lp_tools_kn\": \n",
    "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres, prohibited_assignments)\n",
    "  problem.variables.add(\n",
    "      obj = objective,\n",
    "      lb = lower_bound,\n",
    "      ub = upper_bound,\n",
    "      names = variable_names\n",
    "     \n",
    "    )\n",
    "  \n",
    "  print(\"Variables Added !\")\n",
    "    \n",
    "    \n",
    "  \"\"\"\n",
    "  Step 4.   Declare and add constraints to the model.\n",
    "            There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
    "            Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
    "            The function prepare_to_add_constraints(dataset, centres) prepares the required data for this step.\n",
    "  \n",
    "   coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
    "   senses: a list of strings that identifies whether the corresponding constraint is\n",
    "           an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
    "   rhs: a list of floats corresponding to the rhs of the constraints.\n",
    "   constraint_names: a list of string corresponding to the name of the constraint\n",
    "  \"\"\"\n",
    "  print(\"Adding Constraints...\")\n",
    "    \n",
    "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair, fair_distance, ratings, flag)\n",
    "  print(\"num_constraints:\", len(senses)) \n",
    "  logger.info(f\"\\t\\t\\tnum_constraints = {len(senses)}\")\n",
    "  problem.linear_constraints.add(\n",
    "      rhs = rhs,\n",
    "      senses = senses,\n",
    "      names = row_names\n",
    "    )\n",
    "  problem.linear_constraints.set_coefficients(coefficients)\n",
    "\n",
    "  print(\"Constraints Added !\")\n",
    "    \n",
    "  # Step 5.\tSolve the problem\n",
    "  problem.solve()\n",
    "\n",
    "  result = {\n",
    "    \"status\": problem.solution.get_status(),\n",
    "    \"success\": problem.solution.get_status_string(),\n",
    "    \"objective\": problem.solution.get_objective_value(),\n",
    "    \"assignment\": problem.solution.get_values(),\n",
    "  }\n",
    "    \n",
    "  qm = problem.solution.quality_metric  \n",
    "  print(\"Solution Quality:\", problem.solution.get_float_quality([qm.max_x, qm.max_primal_infeasibility]))\n",
    "  \n",
    "  # print(\"Status:\", result['status']) # outputs a number: \"1\" for optimal solution, \"2\" for unbounded ray and \"3\" for infeasible solution\n",
    "  solution_status = result['status']\n",
    "  assert solution_status==1, \"Solution isn't optimal !\"\n",
    "\n",
    "  print(\"Status:\", problem.solution.get_status_string()) # optimal, unbounded ray, infeasible\n",
    "\n",
    "  return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Assignment of drivers to the FFCs / warehouses\n",
    "import copy\n",
    "import dependent_routing as dp\n",
    "\n",
    "# configParser.read(configFilePath)\n",
    "\n",
    "num_samples, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "def fair_assignment(prob_dis,driver_loc):\n",
    "  '''Assigning the driver using the probaility distribution using dependent rounding'''  \n",
    "  \n",
    "  # \"prob_dis\" is the result of the Fair-LP program \"fair_clustering\"  \n",
    "  prob_dist = copy.deepcopy(prob_dis)\n",
    "  # print(\"prob_dist shape [num_drivers x num_ffc]:\", prob_dist.shape)\n",
    "\n",
    "  rounding = dp.DependentRounding(prob_dist)\n",
    "  rounding._buildGraph(prob_dist)\n",
    "  final_assignment = rounding.round()\n",
    "  final_assignment = np.around(final_assignment,2)\n",
    "\n",
    "  driver_df = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df['ffc_index'] = -1 # unassigned\n",
    "\n",
    "  for i in range(num_samples):\n",
    "    for j in range(num_centres):\n",
    "      if abs(final_assignment[i][j]-1) < 0.01: \n",
    "        driver_df.at[i,'ffc_index'] = j\n",
    "        \n",
    "  return driver_df, final_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(probs):\n",
    "    \"\"\"\n",
    "    To cope with bound violations which can occur upto the feasibility parameter range \n",
    "    So the lower bound of 0.0 on the probabilities can get violated and the values can go down to (0-feasibility_parameter_value)\n",
    "    \"\"\"\n",
    "    for i in range(len(probs)):\n",
    "        last_pos_index = -1\n",
    "        neg_value = 0\n",
    "        \n",
    "        for j in range(len(probs[0])):\n",
    "            assert probs[i][j] >= -1e-6 \n",
    "            \n",
    "            if probs[i][j] < 0:\n",
    "                neg_value += probs[i][j]\n",
    "                probs[i][j] = 0\n",
    "            elif probs[i][j] > 0:\n",
    "                last_pos_index = j\n",
    "\n",
    "        max_pos_index = np.argmax(probs[i])\n",
    "        probs[i][max_pos_index] += neg_value\n",
    "        \n",
    "        assert probs[i][max_pos_index] > 0\n",
    "        \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main :\n",
    "def FairAssign_solver(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "    # Fair-LP:\n",
    "    try:\n",
    "        lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    except:\n",
    "        logger.error(\"Solution Non-optimal (Unbounded Ray or Infeasible) !\")\n",
    "        return None, None\n",
    "    prob_dis = np.reshape(lp_output['assignment'][:num_samples*num_centres], (-1, num_centres))\n",
    "    \n",
    "    try:\n",
    "        prob_dist = sanityCheck(copy.deepcopy(prob_dis)) # this might raise an assertion error\n",
    "    except:\n",
    "        logger.error(\"Sanity Check Assertion !\")\n",
    "        return None, None\n",
    "    \n",
    "    # Randomized Dependent Rounding:\n",
    "    try:\n",
    "        df = fair_assignment(prob_dist, driver_locs)[0] # this might raise an assertion error\n",
    "        final_assignment = df['ffc_index'].values\n",
    "    except:\n",
    "        logger.error(\"Dependent Rounding Assertion !\")\n",
    "        return prob_dist, None\n",
    "    \n",
    "    return prob_dist, final_assignment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [(x-1) for x in [7, 5, 3]] \n",
    "fd_list = [(driver_dists.mean()/alpha) for alpha in [28, 14, 8]] \n",
    "num_runs = 5\n",
    "# k_list and fd_list contain hyperparameters\n",
    "\n",
    "k_fd_dict = {k:\\\n",
    "                {fd_idx:\\\n",
    "                    {num_run:\\\n",
    "                        {'p_dist':None, 'assignment':None}\n",
    "                        for num_run in range(num_runs)\n",
    "                    } \n",
    "                    for fd_idx in range(len(fd_list))\n",
    "                } \n",
    "            for k in k_list\n",
    "            }\n",
    "\n",
    "os.mkdir(\"./assign_results\") # directory to store the results of FairAssign_solver\n",
    "\n",
    "for k in k_list:\n",
    "    print(k)\n",
    "    logger.info(f\"Considering k = {k+1} nearest zones\")\n",
    "    prhbtd_assigns = get_pa(dz_dist, k)\n",
    "    \n",
    "    for f_idx, fair_distance in enumerate(fd_list):\n",
    "        logger.info(f\"\\tfair_distance = {fair_distance}\")\n",
    "        for num_run in range(num_runs):\n",
    "            logger.info(f\"\\t\\tnum_run = {num_run}\")\n",
    "            prob_dist, final_assignment = FairAssign_solver(driver_locs, zone_locs, lower_caps, upper_caps, fair_distance, prhbtd_assigns)\n",
    "            k_fd_dict[k][f_idx][num_run]['p_dist'] = prob_dist\n",
    "            k_fd_dict[k][f_idx][num_run]['assignment'] = final_assignment \n",
    "    # Store intermediate results as well as fail-safe:\n",
    "    # saving current state of \"k_fd_dict\":\n",
    "    pickling_on = open(f\"dict_k={k+1}.pickle\", \"wb\")\n",
    "    pickle.dump(k_fd_dict, pickling_on)\n",
    "    pickling_on.close() \n",
    "\n",
    "pickling_on = open(\"Assignments_ratings_\"+city+\".pickle\", \"wb\")\n",
    "pickle.dump(k_fd_dict, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment(result_path, driver_locs, k):\n",
    "    '''\n",
    "    returns ffc_index (or zone index) for each driver based on the \n",
    "    '''\n",
    "    pickle_off = open(result_path, \"rb\")\n",
    "    assignments = pickle.load(pickle_off)\n",
    "    prob_dist = assignments[k-1][0][0]['p_dist']\n",
    "    ## get assignment by applying dependent rounding: \n",
    "    df = fair_assignment(prob_dist, driver_locs)[0] \n",
    "    final_assignment = df['ffc_index'].values\n",
    "\n",
    "    return final_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = f\"assign_results/results_{city}/Assignments_{city}.pickle\" \n",
    "\n",
    "assignment = get_assignment(result_path, driver_locs, 7) \n",
    "\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random assignment while maintaining only upper capacity bounds of the zones:\n",
    "def random_dist(day_num, driver_locs, upper_caps):\n",
    "    random.seed(1234+day_num)\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rand_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    rand_df['bz_idx_rand'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = random.randint(1, num_zones)-1\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = random.randint(1, num_zones)-1\n",
    "        rand_df.at[i, 'bz_idx_rand'] = zone \n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return rand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df = random_dist(driver_locs, upper_caps)\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = rand_df['bz_idx_rand']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Round Robin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoundRobin assignment while maintaining upper capacity bounds of zones only:\n",
    "def round_robin_dist(day_num, driver_locs):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rr_df = pd.DataFrame(driver_locs, columns=['lat', 'lng'])\n",
    "    rr_df['bz_idx_rr'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = (day_num+1) % num_zones\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = (zone+1) % num_zones \n",
    "        rr_df.at[i, 'bz_idx_rr'] = int(zone)\n",
    "        temp_upper_cap[zone] -= 1 \n",
    "    return rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_df = round_robin_dist(day_num, driver_locs)\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = rr_df['bz_idx_rr']\n",
    "# temp_df = temp_df.sort_values('de_id').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIPA**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first day of FoodMatch as the first day of LIPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires simulation results of previous day\n",
    "local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "pre = 'A'\n",
    "sim_path = f'sim_results/sim_results_{city}/sim.results{pre}lipa{day_num}'\n",
    "print(sim_path)\n",
    "\n",
    "data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "    \n",
    "data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "for d_id in driver_idf['de_id']:\n",
    "    try:\n",
    "        day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "    except:\n",
    "        # handles the cases for which d_id is not present in data_deliver \n",
    "        continue\n",
    "                \n",
    "local_incomes_df[f'day{day_num}'] = local_incomes_df['de_id'].map(day_incomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous day incomes are used to determine the next day's assignment:\n",
    "fm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "fm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "fm_inc_df = copy.deepcopy(local_incomes_df)\n",
    "fm_inc_df = pd.merge(fm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "# find day 1 incomes of drivers (FairAssign):\n",
    "prev_incomes_df = copy.deepcopy(fm_inc_df[['de_id', 'lat', 'lng', 'day1']])\n",
    "driver_prev_incomes = prev_incomes_df['day1'].values\n",
    "\n",
    "# find day 1 number of orders in each zone (FairAssign):\n",
    "orders_data = pd.read_csv(f\"data/orders_data/{city}/orders_0{day_num}05.csv\") \n",
    "sim_path = 'sim_results/sim_results_'+city+'/sim.resultsAlipa'+str(day_num)\n",
    "   \n",
    "data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "    \n",
    "vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "\n",
    "cust_zones = df['customer_zone'].unique()\n",
    "cust_zones_gb = df.groupby('customer_zone')\n",
    "\n",
    "orders_per_zone = {key:0 for key in cust_zones if key in zone_ids.values}\n",
    "for key in cust_zones:\n",
    "    if key in zone_ids.values:\n",
    "        orders_per_zone[key] = cust_zones_gb.get_group(key).shape[0] \n",
    "\n",
    "# orders_per_zone = {k: v for k, v in sorted(orders_per_zone.items(), key=lambda item: item[1])}\n",
    "orders_per_zone = {k:v for k, v in sorted(orders_per_zone.items())}\n",
    "zone_prev_incomes = [v for k, v in orders_per_zone.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIPA while maintaining upper capacity bounds of zones only:\n",
    "def lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "\n",
    "    lipa_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    lipa_df['bz_idx_lipa'] = -1\n",
    "\n",
    "    temp_upper_cap = copy.deepcopy(upper_caps)\n",
    "\n",
    "    driver_idx_inc = np.argsort(np.array(driver_prev_incomes))\n",
    "    zone_idx_inc = np.argsort(np.array(zone_prev_incomes))\n",
    "  \n",
    "    j = num_zones-1\n",
    "    for i in driver_idx_inc:\n",
    "        zone = zone_idx_inc[j]\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            j = j-1\n",
    "            zone = zone_idx_inc[j]\n",
    "        lipa_df.at[i, 'bz_idx_lipa'] = zone\n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return lipa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipa_df = lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes)\n",
    "temp_df = copy.deepcopy(driver_idf) \n",
    "# original base zones\n",
    "temp_df = pd.merge(temp_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "temp_df['bz_idx'] = temp_df['base_zone'].map(zone_id2idx)\n",
    "# base zones assigned by Fair Assign\n",
    "temp_df['fa_bz_idx'] = lipa_df['bz_idx_lipa']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating modified files for current day 'day_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files corresponding to driver_idf:\n",
    "de_idf_ids = driver_idf['de_id']\n",
    "\n",
    "de_idf_files = []\n",
    "for d_id in de_idf_ids:\n",
    "    file_name = 'data/de_data/'+city +'_de_data/'+str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    de_idf_files.append(file_name)\n",
    "# de_idf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the node_ids:\n",
    "orig_node_ids = []\n",
    "na = 0 # number of files in de_idf_files which are not present in de_intervals/\n",
    "for file in de_idf_files:\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        na += 1\n",
    "        d_id = int(file.split('/')[-1][:-3])\n",
    "        to_drop_idx = temp_df[temp_df['de_id']==d_id].index\n",
    "        temp_df = temp_df.drop(to_drop_idx)\n",
    "        print(to_drop_idx)\n",
    "        continue \n",
    "    # get starting node ids corresponding to all shifts, it will be useful for random generation for unswappable drivers\n",
    "    num_shifts = int(file_df.shape[0]/2)\n",
    "    node_id = [int(file_df.iloc[x*2].values[0].split()[1]) for x in range(num_shifts)]\n",
    "    orig_node_ids.append(node_id)\n",
    "\n",
    "temp_df[\"node_id\"] = orig_node_ids \n",
    "temp_df[\"fa_node_id\"] = temp_df['node_id']\n",
    "\n",
    "print(f\"{na} intersection drivers not found in ../{day_num}/de_intervals/\")\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_files = []\n",
    "for d_id in driver_idf['de_id']:\n",
    "    file_name = 'data/de_data/'+ city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    intersection_files.append(file_name)\n",
    "\n",
    "old_dir_path = f'data/de_data/{city}_de_data/{day_num}/de_int_old'\n",
    "new_dir_path = f'data/de_data/{city}_de_data/{day_num}/de_int_new'\n",
    "\n",
    "if os.path.exists(old_dir_path):\n",
    "    shutil.rmtree(old_dir_path)\n",
    "    \n",
    "if os.path.exists(new_dir_path):\n",
    "    shutil.rmtree(new_dir_path)\n",
    "\n",
    "os.mkdir(old_dir_path)\n",
    "os.mkdir(new_dir_path)\n",
    "\n",
    "for file in intersection_files:\n",
    "    d_id = file[40:-4]\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        continue \n",
    "    old_path = f'Code/data/de_data/{city}_de_data/{day_num}/de_int_old/{d_id}.csv'\n",
    "    file_df.to_csv(old_path, index=False)\n",
    "    new_path = f'data/de_data/{city}_de_data/{day_num}/de_int_new/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWAPPING LOGIC:\n",
    "# The swapping logic is applicable only to the drivers for which the fa_bz_idx is different from bz_idx\n",
    "rel_df = temp_df[temp_df['bz_idx'] != temp_df['fa_bz_idx']].reset_index()\n",
    "print(rel_df.shape[0])\n",
    "rel_df['paired_de'+str(day_num)] = None\n",
    "\n",
    "# first assign swappable nodes:\n",
    "bz_nodes_dict = {zone_idx:{'freq':0, 'nodes':[], 'paired_de_id':[]} for zone_idx, _ in enumerate(zone_ids)} # it'll contain the frequency of each base zone in rel_df['bz_idx'] as well as the corresponding node_ids in a list\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    z_id = int(rel_df.iloc[idx]['bz_idx'])\n",
    "    n_id = int(rel_df.iloc[idx]['node_id'][0])\n",
    "    paired_did = int(rel_df.iloc[idx]['de_id']) # remove later?\n",
    "    bz_nodes_dict[z_id]['freq'] += 1\n",
    "    bz_nodes_dict[z_id]['nodes'].append(n_id)\n",
    "    bz_nodes_dict[z_id]['paired_de_id'].append(paired_did) # remove later?\n",
    "bz_nodes_dict_store = copy.deepcopy(bz_nodes_dict)\n",
    "# IF a required zone_id in fa_bz_idx is present in bz_nodes_dict then use that data point\n",
    "# ELSE generate a random location in the zone corresponding to the zone_id\n",
    "num_random = 0 # number of drivers for whom random generation of location was done to get the corresponding start node\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    print(idx, end=' ')\n",
    "    z_id = int(rel_df.iloc[idx]['fa_bz_idx'])\n",
    "    if(bz_nodes_dict[z_id]['freq'] > 0):\n",
    "        rel_df.loc[idx, 'fa_node_id'] = bz_nodes_dict[z_id]['nodes'][0]\n",
    "        rel_df.loc[idx, 'paired_de'] = bz_nodes_dict[z_id]['paired_de_id'][0]\n",
    "        # print(bz_nodes_dict[z_id]['paired_de_id'][0])\n",
    "        bz_nodes_dict[z_id]['freq'] -= 1\n",
    "        bz_nodes_dict[z_id]['nodes'].pop(0)\n",
    "        bz_nodes_dict[z_id]['paired_de_id'].pop(0) # remove later?\n",
    "    else:\n",
    "        # randomly generate a location in the zone assigned by FairAssign\n",
    "        zone_df = zone_dfs_dict[city]\n",
    "        # zone_bdry = zone_df[zone_df['zone_id']==z_id]['path'].values[0] # Wrong ! bcz z_id is the index of zone_id\n",
    "        zone_bdry = zone_df.iloc[z_id]['path']\n",
    "        new_loc = generate_locs(1, zone_bdry)[0] \n",
    "        # shift new_loc: convert to anonymized coordinates\n",
    "        if city=='ahm':\n",
    "            new_loc[0] -= 2.0\n",
    "            new_loc[1] -= 10.0\n",
    "        if city=='blr':\n",
    "            new_loc[0] += 3.0\n",
    "            new_loc[1] -= 13.0\n",
    "        if city=='del':\n",
    "            new_loc[0] -= 12.0\n",
    "            new_loc[1] -= 13.0\n",
    "        # based on new_loc, get the closest node_id from location_df\n",
    "        min_dist = 1e9\n",
    "        n_id = -1\n",
    "        for i in range(location_df.shape[0]):\n",
    "            node_loc = [ location_df.iloc[i]['lat'], location_df.iloc[i]['lng'] ]\n",
    "            curr_dist = euclidean_distance(new_loc, node_loc)\n",
    "            if(curr_dist <= min_dist):\n",
    "                min_dist = curr_dist \n",
    "                n_id = location_df.iloc[i]['node_id']     \n",
    "        num_random += 1\n",
    "        num_shifts = int(len(rel_df.iloc[idx]['node_id']))\n",
    "        n_idz = [int(n_id)]*num_shifts\n",
    "        rel_df.at[idx, 'fa_node_id'] = n_idz\n",
    "print()\n",
    "print(f\"{rel_df.shape[0]-num_random} data points out of {rel_df.shape[0]} could be swapped !\")\n",
    "print(f\"{num_random} data points were randomly generated !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_files = []\n",
    "for d_id in temp_df['de_id']:\n",
    "    file_name = 'data/de_data/'+city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    intersection_files.append(file_name)\n",
    "\n",
    "for file in intersection_files:\n",
    "    # d_id = file[40:-4]\n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    file_df = pd.read_csv(file)\n",
    "\n",
    "    old_path = f'data/de_data/{city}_de_data/{day_num}/de_int_old/{d_id}.csv'\n",
    "    file_df.to_csv(old_path, index=False)\n",
    "\n",
    "    new_path = f'Code/data/de_data/{city}_de_data/{day_num}/de_int_new/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the de_interval profiles of swappable or paired drivers are to be swapped \n",
    "# and that of drivers for whom random locations are generated, the profile is to be modified\n",
    "rel_drivers = rel_df['de_id']\n",
    "\n",
    "rel_files = []\n",
    "for d_id in rel_drivers:\n",
    "    file_name = 'data/de_data/'+city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    rel_files.append(file_name)\n",
    "\n",
    "for file in rel_files:\n",
    "    # d_id = file[40:-4] \n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    file_df = pd.read_csv(file)\n",
    "    \n",
    "    swap_node = rel_df[rel_df['de_id']==int(d_id)].paired_de.values[0]\n",
    "    # for those drivers who could be swapped:\n",
    "    if not np.isnan(swap_node):\n",
    "        swap_with_file = 'data/de_data/' + city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(swap_node)) + '.csv'\n",
    "        file_df = pd.read_csv(swap_with_file)\n",
    "    # for those drivers whose starting nodes for each shift were randomly generated\n",
    "    else:\n",
    "        num_shifts = int(file_df.shape[0]/2)\n",
    "        new_start_nodes = rel_df[rel_df['de_id']==int(d_id)].fa_node_id.values[0]\n",
    "        for i in range(num_shifts):\n",
    "            new_node = new_start_nodes[i]\n",
    "            file_df.iloc[i*2] = str(file_df.iloc[0].values[0].split()[0])+ ' ' + str(new_node) \n",
    "\n",
    "    new_path = f'data/de_data/{city}_de_data/{day_num}/de_int_new/{d_id}.csv'\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS:\n",
    "\n",
    "def gini_index(incomes):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum() \n",
    "    inc_sum = 0.0\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            inc_sum += abs(incomes[i]-incomes[j])\n",
    "    gini = inc_sum / (2*num*total)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def avg_distance(zone_labels, driver_locs, zone_locs):\n",
    "    \"\"\"\n",
    "    returns the 'cost' of the assignment\n",
    "    zone_labels: indices of the assigned zones\n",
    "    a zone_label 'z' has location zone_locs[z]\n",
    "    \"\"\"\n",
    "    driver_dists = L2Distance(driver_locs) \n",
    "    num = len(zone_labels)\n",
    "    dist = 0.0 \n",
    "    for i in range(num):\n",
    "        assigned_zone = zone_labels[i]\n",
    "        driver_loc, zone_loc = driver_locs[i], zone_locs[int(assigned_zone)]\n",
    "        driver_zone_dist = euclidean_distance(driver_loc, zone_loc)\n",
    "        dist += np.sqrt(driver_zone_dist)\n",
    "    avg_dist = dist/num\n",
    "    return avg_dist                       \n",
    "    \n",
    "\n",
    "def spatial_inequality_index(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "    if flag==0:\n",
    "        driver_dists = L2Distance(driver_locs)\n",
    "    if flag==1:\n",
    "        driver_dists = abs_difference(ratings)\n",
    "    if flag==2:\n",
    "        driver_dists = combined\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    term_i = 0.0    \n",
    "    for i in range(num):\n",
    "        sum_j = 0.0\n",
    "        num_j = 1e-9    \n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j] <= fair_distance and driver_dists[i][j]>0:\n",
    "                num_j += 1\n",
    "                sum_j += abs(incomes[i]-incomes[j])   \n",
    "        term_i += (sum_j / num_j) \n",
    "    \n",
    "    spin_idx = term_i / total \n",
    "    # spin_idx = round(spin_idx, 2)\n",
    "    return spin_idx \n",
    "\n",
    "\n",
    "def income_gap(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "    \"\"\" \n",
    "    difference between incomes between any two drivers per unit distance (within fair_distance) \n",
    "    \"\"\"\n",
    "    alpha = 100\n",
    "\n",
    "    if flag==0:\n",
    "        driver_dists = L2Distance(driver_locs)\n",
    "    if flag==1:\n",
    "        driver_dists = abs_difference(ratings)\n",
    "    if flag==2:\n",
    "        driver_dists = combined\n",
    "    driver_dists = driver_dists * alpha\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    terms = 0.0\n",
    "    num_pair_drivers = 1e-7 # NOT 0 => to avoid division by 0\n",
    "    for i in range(num-1):\n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j]>0:\n",
    "                num_pair_drivers += 1\n",
    "                terms += (abs(incomes[i]-incomes[j])/driver_dists[i][j])\n",
    "    inc_gap = terms/num_pair_drivers\n",
    "    # inc_gap = round(inc_gap, 2)\n",
    "    return inc_gap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the FoodMatch simulation results for all 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incomes_df(algo):\n",
    "    # get the incomes on all 6 days for all drivers \n",
    "    '''\n",
    "    input string: algo\n",
    "    'fm' : FoodMatch\n",
    "    'fafm' : FairAssign + FoodMatch\n",
    "    '''\n",
    "    local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "    local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "    num_days = 6\n",
    "    day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "    pre = 'A'\n",
    "    for day in range(1, num_days+1):\n",
    "        sim_path = f'sim_results/sim_results_{city}/sim.results{pre}{algo}{day}'\n",
    "        print(sim_path)\n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        # print(data_deliver)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "        data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "        for d_id in driver_idf['de_id']:\n",
    "            try:\n",
    "                day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "            except:\n",
    "                # handles the cases for which d_id is not present in data_deliver \n",
    "                continue  \n",
    "        local_incomes_df[f'day{day}'] = local_incomes_df['de_id'].map(day_incomes)\n",
    "    \n",
    "    cols = ['day1', 'day2', 'day3', 'day4', 'day5', 'day6'] \n",
    "    local_incomes_df['num_orders'] = local_incomes_df[cols].sum(axis=1) \n",
    "\n",
    "    return copy.deepcopy(local_incomes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only FoodMatch: \n",
    "fm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "fm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "fm_incomes_df = get_incomes_df('fm')\n",
    "fm_incomes_df = pd.merge(fm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "# FairAssign then FoodMatch:\n",
    "fafm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "fafm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "fafm_incomes_df = get_incomes_df('fafm')\n",
    "fafm_incomes_df = pd.merge(fafm_incomes_df, driver_idf, on='de_id')\n",
    "\n",
    "fm_inc_df = copy.deepcopy(fm_incomes_df)\n",
    "fafm_inc_df = copy.deepcopy(fafm_incomes_df)\n",
    "fm_inc_df = pd.merge(fm_incomes_df, fafm_incomes_df[['de_id']])\n",
    "fafm_inc_df = pd.merge(fafm_incomes_df, fm_inc_df[['de_id']])\n",
    "assert fm_inc_df.shape[0]==fafm_inc_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats_fm = fm_inc_df['lat']\n",
    "longs_fm = fm_inc_df['lng']\n",
    "\n",
    "lats_fafm = fafm_inc_df['lat']\n",
    "longs_fafm = fafm_inc_df['lng'] \n",
    "\n",
    "fm_incomes = fm_inc_df['num_orders']\n",
    "fafm_incomes = fafm_inc_df['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(lats, longs, incomes, ratings, combined, fair_dist): \n",
    "    d_locs = [[lat, lng] for lat, lng in zip(lats, longs)]\n",
    "    incomes = np.array(incomes)\n",
    "    gini = gini_index(incomes) \n",
    "    sp_idx = spatial_inequality_index(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    inc_gp = income_gap(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    return gini, sp_idx, inc_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag==0:\n",
    "    driver_dists = driver_dists \n",
    "if flag==1:\n",
    "    driver_dists = ratings_matrix \n",
    "if flag==2:\n",
    "    driver_dists = combined\n",
    "\n",
    "def num_sim_drivers(fd):\n",
    "    ''' \n",
    "    number of similar drivers (i.e., for a given driver, how many drivers are being considered for fairness comparison per)\n",
    "    '''\n",
    "    num_similar_drivers = []\n",
    "    for idx in range(len(driver_dists)):\n",
    "        curr_driver = driver_dists[idx]\n",
    "        num_sim = curr_driver[curr_driver<=fd].shape[0]\n",
    "        num_similar_drivers.append(num_sim) \n",
    "    return np.mean(num_similar_drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the metrics at different fair_dist values: \n",
    "if flag==0:\n",
    "    fair_dist = driver_dists.mean()/8 # actual: cd \n",
    "if flag==1:\n",
    "    fair_dist = 1 # 0.5\n",
    "if flag==2:\n",
    "    fair_dist = 0.04 # actual: 0.04\n",
    "    \n",
    "def eval_results(lats, longs, incomes):\n",
    "    results = []\n",
    "    for k in range(1, 11):\n",
    "        fd = fair_dist/k \n",
    "        gini, sp_idx, inc_gp = metrics(lats, longs, incomes, ratings, combined, fd)\n",
    "        results.append([fd, num_sim_drivers(fd), gini, sp_idx, inc_gp])\n",
    "    result_df = pd.DataFrame(results)\n",
    "    cols = ['fair_dist', 'sim_drivers', 'gini', 'spatial_ineq', 'income_gap']\n",
    "    result_df.columns = cols\n",
    "    return result_df\n",
    "\n",
    "# FINAL RESULTS:\n",
    "fm_results_df = eval_results(lats_fm, longs_fm, fm_incomes) \n",
    "fafm_results_df = eval_results(lats_fafm, longs_fafm, fafm_incomes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Avg. Distance (or Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_this_driver(locs, v_id):\n",
    "    # driver's inital location:\n",
    "    v_init_loc = driver_idf[driver_idf['de_id']==v_id][['lat', 'lng']].values[0]\n",
    "    first_mile_dist = euclidean_distance(locs[0], v_init_loc)\n",
    "    last_mile_dist = 0\n",
    "    for idx in range(1, len(locs)):\n",
    "        prev_loc = locs[idx-1]\n",
    "        curr_loc = locs[idx] \n",
    "        last_mile_dist += euclidean_distance(prev_loc, curr_loc) \n",
    "    return first_mile_dist, last_mile_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the incomes on all 6 days for all drivers \n",
    "num_days = 6\n",
    "\n",
    "def get_cost(algo, num_days):\n",
    "    ''' \n",
    "    algo: str\n",
    "    'fm': FoodMatch \n",
    "    'fafm': FairAssign then FoodMatch\n",
    "    '''\n",
    "    cust_lats, cust_lngs = [], []\n",
    "    first_mile_cost = 0.0 # over all num_days days\n",
    "    last_mile_cost = 0.0 # over all num_days days\n",
    "    for day in range(1, num_days+1):\n",
    "        total_first_mile = 0\n",
    "        total_last_mile = 0\n",
    "        orders_data = pd.read_csv(f\"data/orders_data/{city}/orders_0{day}05.csv\") \n",
    "        sim_path = f'sim_results/sim_results_{city}/sim.resultsA{algo}(day)'\n",
    "        \n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "        df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "        c_locs = df['customer_lat_lng'].values \n",
    "        df['cust_lat'] = [float(loc.split(',')[0]) for loc in c_locs]\n",
    "        df['cust_lng'] = [float(loc.split(',')[1]) for loc in c_locs]\n",
    "\n",
    "        cust_lats.append(df['cust_lat'])\n",
    "        cust_lngs.append(df['cust_lng'])\n",
    "\n",
    "        df_gb = df.groupby('vehicle_id') # Don't group on 'de_id' \n",
    "        for v_id in vehicle_ids:\n",
    "            curr_group = df_gb.get_group(v_id)\n",
    "            first_mile_dist, last_mile_dist = distance_this_driver(curr_group[['cust_lat', 'cust_lng']].values, v_id)\n",
    "            total_first_mile += first_mile_dist \n",
    "            total_last_mile += last_mile_dist\n",
    "        # avg cost for this day:\n",
    "        first_mile_cost += (total_first_mile/len(vehicle_ids))\n",
    "        last_mile_cost += (total_last_mile/len(vehicle_ids)) \n",
    "    # avg cost over all days:\n",
    "    first_mile_cost = first_mile_cost/num_days\n",
    "    last_mile_cost = last_mile_cost/num_days \n",
    "    cost = first_mile_cost + last_mile_cost \n",
    "    # print(first_mile_cost, last_mile_cost, cost)\n",
    "    return first_mile_cost, last_mile_cost, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costs with only FoodMatch:\n",
    "first_mile, last_mile, total = get_cost('fm', num_days)\n",
    "print(total) \n",
    "\n",
    "# costs with FairAssign + FoodMatch: \n",
    "first_mile_, last_mile_, total_ = get_cost('fafm', num_days)\n",
    "print(total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
