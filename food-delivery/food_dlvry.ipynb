{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import warnings # need to be imported before other imports\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os \n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import glob \n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import ortools\n",
    "import logging\n",
    "import datetime\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser   \n",
    "from shapely import geometry\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "# from geopy.geocoders import Nominatim  \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=26) \n",
    "matplotlib.rc('ytick', labelsize=26) \n",
    "\n",
    "plt.rcParams['font.size'] = '26'\n",
    "plt.rcParams['figure.figsize'] = (10,7.5)\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High level view -- The workflow has $4$ stages:  \n",
    "1. **Assignment** : Get the probability distribution corresponding to each driver.  \n",
    "\n",
    "2. **File Generation** : Swap the drivers' files (a way of creating drivers with existing real-world information) and copy/create de_data (containing shift information) files.   \n",
    "\n",
    "3. **Simulation** : Use last-mile delivery algorithm (FoodMatch) to simulate real-world deliveries. Store the simulation files.  \n",
    "\n",
    "4. **Evalutaion** : Evaluate the performance on the evalution metrics (using income information from the simulation files of all $6$ days)\n",
    "\n",
    "Each stage has been explicitly marked in this notebook. You can opt to separate each step by creating a script for each of these using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "city = 'A'            # takes values in {'A', 'B'}\n",
    "flag = 0              # takes values in {0, 1, 2}\n",
    "day_num = 1           # takes values in {1, 2, 3, 4, 5, 6}\n",
    "w1, w2 = 0.6, 0.4     # w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\n",
    "NUM_DRIVERS = 977     # Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\n",
    "NUM_SIM = 30          # Number of similar drivers for each driver to be considered in the fairness constraint\n",
    "K_VAL = 10            # Number of nearest zones to which a driver can be assigned\n",
    "wts = -1\n",
    "algo = 'FairAssign'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ncity = str(sys.argv[1])             # takes values in {'A', 'B', 'C'}\\nday_num = str(sys.argv[2])          # takes values in {1, 2, 3, 4, 5, 6}\\n# w1, w2 = 0.6, 0.4                 # w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\\n# w1, w2 = 0.7, 0.3\\n# w1, w2 = 0.8, 0.2\\nNUM_DRIVERS = int(sys.argv[3])     # Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\\nNUM_SIM = int(sys.argv[4])         # Number of similar drivers for each driver to be considered in the fairness constraint\\nK_VAL = int(sys.argv[5])\\nflag = int(sys.argv[6])            # takes values in {0, 1, 2} \\n\\nwts = int(sys.argv[7])              # 64 (or) 73 (or) 82\\nw1, w2 = (wts//10)/10, (wts%10)/10    # w1,w2=0.6,0.4 (or) w1,w2=0.7,0.3 (or) w1,w2=0.8,0.2\\n\\nalgo = 'FairAssign'\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE: \n",
    "# -------\n",
    "\"\"\" \n",
    "city = str(sys.argv[1])             # takes values in {'A', 'B', 'C'}\n",
    "day_num = str(sys.argv[2])          # takes values in {1, 2, 3, 4, 5, 6}\n",
    "# w1, w2 = 0.6, 0.4                 # w_i can take values in [0, 1] such that sum(w_i for i in {1, 2}) = 1\n",
    "# w1, w2 = 0.7, 0.3\n",
    "# w1, w2 = 0.8, 0.2\n",
    "NUM_DRIVERS = int(sys.argv[3])     # Number of drivers to be chosen out of the intersection of drivers; -1 => take all drivers in driver_idf\n",
    "NUM_SIM = int(sys.argv[4])         # Number of similar drivers for each driver to be considered in the fairness constraint\n",
    "K_VAL = int(sys.argv[5])\n",
    "flag = int(sys.argv[6])            # takes values in {0, 1, 2} \n",
    "\n",
    "wts = int(sys.argv[7])              # 64 (or) 73 (or) 82\n",
    "w1, w2 = (wts//10)/10, (wts%10)/10    # w1,w2=0.6,0.4 (or) w1,w2=0.7,0.3 (or) w1,w2=0.8,0.2\n",
    "\n",
    "algo = 'FairAssign'\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplar input command:   \n",
    "\n",
    "**ipython food_dlvry.ipynb A $1$ $1000$ $30$ $10$ $0$**   \n",
    "**ipython food_dlvry.ipynb A $1$ $1000$ $30$ $7$ $1$**  \n",
    "**ipython food_dlvry.ipynb A $1$ $1000$ $30$ $7$ $2$ $64$**    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **File Generation** for all $6$ days needs to be done before simulation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Input Summary =======================\n",
      "City: A\n",
      "Number of drivers: 977\n",
      "Number of similar drivers considered per driver: 30\n",
      "Number of nearest zones for assignment: 10\n",
      "Day (for file generation): 1\n",
      "w1=0.6, w2=0.4\n",
      "Algorithm (for evaluation): FairAssign\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "# Input Summary:\n",
    "print(\"======================= Input Summary =======================\")\n",
    "print(\"City:\", city)\n",
    "print(\"Number of drivers:\", NUM_DRIVERS)\n",
    "print(\"Number of similar drivers considered per driver:\", NUM_SIM) \n",
    "print(\"Number of nearest zones for assignment:\", K_VAL)\n",
    "print(\"Day (for file generation):\", day_num)\n",
    "print(f\"w1={w1}, w2={w2}\")\n",
    "print(\"Algorithm (for evaluation):\", algo)\n",
    "print(\"==============================================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flag**  \n",
    "\"flag\" decides which distance metric/measure to consider:    \n",
    "0: euclidean distance (or physical distance)   \n",
    "1: rating   \n",
    "2: combination of euclidean distance and rating   \n",
    "   where 'w1' is weight given to euclidean distance and 'w2' is weight given to rating \n",
    "\n",
    "\n",
    "**algo**  \n",
    "\"algo\" decides the assignment algorithm;    \n",
    "It can take values in {'FairAssign', 'RoundRobin', 'LIPA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigns_path = \"./assign_results\"\n",
    "# if os.path.exists(assigns_path):\n",
    "#     shutil.rmtree(assigns_path)\n",
    "\n",
    "# logs_path = \"./logs\"\n",
    "# if os.path.exists(logs_path):\n",
    "#     shutil.rmtree(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASETS\n",
    "## All of the data here is UNANONYMIZED\n",
    "import glob \n",
    "\n",
    "driver_files_A = sorted(glob.glob(\"data/driver_locs/A/driver_data_A_day*.csv\"))\n",
    "driver_files_B = sorted(glob.glob(\"data/driver_locs/B/driver_data_B_day*.csv\"))\n",
    "driver_files_C = sorted(glob.glob(\"data/driver_locs/C/driver_data_C_day*.csv\"))\n",
    "\n",
    "num_days = len(driver_files_A) \n",
    "assert len(driver_files_A)==len(driver_files_B), \"error in reading data or incomplete data\"\n",
    "assert len(driver_files_A)==len(driver_files_C), \"error in reading data or incomplete data\"\n",
    "\n",
    "driver_dfs_A = [pd.read_csv(driver_files_A[idx]) for idx in range(num_days)]\n",
    "driver_dfs_B = [pd.read_csv(driver_files_B[idx]) for idx in range(num_days)]\n",
    "driver_dfs_C = [pd.read_csv(driver_files_C[idx]) for idx in range(num_days)]\n",
    "driver_dfs_dict = {'A': driver_dfs_A, 'B': driver_dfs_B, 'C': driver_dfs_C}\n",
    "\n",
    "zone_df_A = pd.read_csv(\"data/zone_data/zone_data_A.csv\")\n",
    "zone_df_B = pd.read_csv(\"data/zone_data/zone_data_B.csv\")\n",
    "zone_df_C = pd.read_csv(\"data/zone_data/zone_data_C.csv\")\n",
    "zone_dfs_dict = {'A': zone_df_A, 'B': zone_df_B, 'C': zone_df_C}\n",
    "\n",
    "income_df_A = pd.read_csv(\"data/income_data/incomes_A.csv\")\n",
    "income_df_B = pd.read_csv(\"data/income_data/incomes_B.csv\")\n",
    "income_df_C = pd.read_csv(\"data/income_data/incomes_C.csv\")\n",
    "income_dfs_dict = {'A': income_df_A, 'B': income_df_B, 'C': income_df_C}\n",
    "\n",
    "base_zone_A = pd.read_csv(\"data/base_zones/A_base_zones.csv\")\n",
    "base_zone_B = pd.read_csv(\"data/base_zones/B_base_zones.csv\")\n",
    "base_zone_C = pd.read_csv(\"data/base_zones/C_base_zones.csv\")\n",
    "base_zones_dict = {'A': base_zone_A, 'B': base_zone_B, 'C': base_zone_C}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nothing changes even on anonymizing 'base_zones', 'driver_data' and 'orders_data'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "#'city' takes values in {'A'}\n",
    "\n",
    "def driver_union(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding union of all the drivers over the days \n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city] \n",
    "    num_days = len(driver_dfs)\n",
    "    \n",
    "    driver_udf = pd.concat([driver_dfs[idx] for idx in range(num_days)])\n",
    "    driver_udf = driver_udf.drop('Unnamed: 0', axis=1)\n",
    "    driver_udf = driver_udf.drop_duplicates('de_id').reset_index().drop('index', axis=1)\n",
    "    \n",
    "    return driver_udf\n",
    "\n",
    "\n",
    "def driver_intersection(drivers_dict):\n",
    "    \"\"\"\n",
    "    Finding intersection of all the drivers over the days\n",
    "    \"\"\"\n",
    "    driver_dfs = drivers_dict[city]\n",
    "    driver_idf = reduce(lambda left,right: pd.merge(left,right,on='de_id'), driver_dfs)\n",
    "    driver_idf = driver_idf[['de_id', 'lat_x', 'lng_x']]\n",
    "    driver_idf = driver_idf.loc[:, ~driver_idf.columns.duplicated()] \n",
    "    driver_idf = driver_idf.rename(columns={'lat_x':'lat', 'lng_x':'lng'})\n",
    "    \n",
    "    return driver_idf\n",
    "\n",
    "\n",
    "def drivers_zones(drivers_dict, zones_dict):\n",
    "    \"\"\"\n",
    "    To get the data to be input to fair_clustering: \"driver_locs\" and \"zone_locs\"\n",
    "    \"\"\"\n",
    "    driver_idf = driver_intersection(drivers_dict) \n",
    "    \n",
    "    # finding \"driver_locs\":\n",
    "    driver_locs = driver_idf[['lat', 'lng']] \n",
    "    driver_locs = driver_locs.values \n",
    "    \n",
    "    # finding \"zone_locs\":\n",
    "    zone_df = zones_dict[city]\n",
    "    zone_locs = np.array(zone_df[['lat', 'lng']])\n",
    "    \n",
    "    return driver_locs, zone_locs\n",
    "\n",
    "\n",
    "def get_capacities(zones_dict):\n",
    "    \"\"\"\n",
    "    returns \"lower_caps\" and \"upper_caps\"\n",
    "    lower_caps: [1 x num_centres] array with lower capacity of each zone\n",
    "    upper_caps: [1 x num_centres] array with upper capacity of each zone\n",
    "    \"\"\"\n",
    "    zone_df = zones_dict[city] \n",
    "\n",
    "    lower_caps = 0.3*zone_df['avg_cap'].values\n",
    "    upper_caps = 1.0*zone_df['avg_cap'].values\n",
    "    \n",
    "    return lower_caps, upper_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGER\n",
    "\n",
    "import logging\n",
    "\n",
    "logs_path = \"./logs\"\n",
    "if not os.path.exists(logs_path):\n",
    "    os.mkdir(logs_path)\n",
    "\n",
    "logging.basicConfig(filename=f\"logs/{city}_{NUM_DRIVERS}_{NUM_SIM}.log\", format='%(asctime)s  %(message)s', filemode='w')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the inputs to fair_clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_udf = driver_union(driver_dfs_dict)\n",
    "driver_idf = driver_intersection(driver_dfs_dict) \n",
    "\n",
    "driver_locs, zone_locs = drivers_zones(driver_dfs_dict, zone_dfs_dict)\n",
    "num_drivers, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "lower_caps, upper_caps = get_capacities(zone_dfs_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the entire experimentation, we consider only those drivers who were present/delivered on $7$ days (see drivers_data). The number of such drivers for different cities is as follows:\n",
    "- City $A$ : $977$\n",
    "- City $B$ : $3655$ \n",
    "- City $C$ : $4365$   \n",
    "   \n",
    "These are also the number of drivers obtained in **driver_idf**. In the rest of the codebase, however, we use the first $6$ days of the above referenced $7$ days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(d_loc, z_loc):\n",
    "    lat1, lng1 = d_loc[0], d_loc[1]\n",
    "    lat2, lng2 = z_loc[0], z_loc[1]\n",
    "    dist = np.sqrt(np.power(lat1-lat2, 2) + np.power(lng1-lng2, 2))\n",
    "    return dist\n",
    "\n",
    "def L2Distance(data):\n",
    "  # \"data\": latitude-longitude level locations \n",
    "  transposed = np.expand_dims(data, axis = 1)\n",
    "  distance = np.power(data - transposed, 2)\n",
    "  distance = np.power(np.abs(distance).sum(axis = 2), 0.5) \n",
    "  return distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_DRIVERS!=-1 and NUM_DRIVERS<=driver_locs.shape[0]:\n",
    "    driver_locs = driver_locs[:NUM_DRIVERS] \n",
    "\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "\n",
    "# Adjust lower_caps for given NUM_DRIVERS\n",
    "# Do not adjust upper_caps\n",
    "num_drivers = driver_locs.shape[0]\n",
    "print(f\"Sum of lower capacities: {lower_caps.sum()}\")\n",
    "while(lower_caps.sum()>num_drivers):\n",
    "    # print(lower_caps)\n",
    "    for idx in range(lower_caps.shape[0]):\n",
    "        lower_caps[idx] = max(lower_caps[idx]-50, 0)\n",
    "\n",
    "print(f\"Sum of (adjusted) lower capacities: {lower_caps.sum()}\")\n",
    "print(f\"Sum of upper capacities: {upper_caps.sum()}\")\n",
    "      \n",
    "assert num_drivers>lower_caps.sum() and num_drivers<upper_caps.sum(), \\\n",
    "\"This set of num_drivers, lower_caps and upper_caps will lead to an infeasible solution !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_drivers, num_zones = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "dz_dist = np.zeros(shape=(num_drivers, num_zones))\n",
    "for d_idx, driver in enumerate(driver_locs):\n",
    "    d_dist = np.zeros(num_zones)\n",
    "    for z_idx, zone in enumerate(zone_locs):\n",
    "        dist = euclidean_distance(driver, zone)\n",
    "        d_dist[z_idx] = dist \n",
    "    dz_dist[d_idx] = d_dist\n",
    "\n",
    "# print(dz_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prohibited_assigns(d_dist, k):\n",
    "    prohibited_assignments = np.zeros(shape=(num_drivers, num_zones))\n",
    "    \n",
    "    for d_idx, d_dist in enumerate(d_dist):\n",
    "        idx = np.argpartition(d_dist, k) \n",
    "        prohibited_assignments[d_idx][idx[k:]] = 1 # set the indices NOT corresponding to k-smallest elements \n",
    "    \n",
    "    return prohibited_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings to sellers:\n",
    "from scipy.stats import truncnorm\n",
    "from numpy.random import SeedSequence \n",
    "from numpy.random import default_rng\n",
    "\n",
    "def get_truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm( (low-mean)/sd, (upp-mean)/sd, loc=mean, scale=sd) \n",
    "\n",
    "def generate_ratings(num_drivers):\n",
    "    mean = 3.5\n",
    "    sd = 1\n",
    "    min_rating = 0.0\n",
    "    max_rating = 5.0\n",
    "    seedVal = 36778738061272522495168595294022739449 # arbitrary\n",
    "    rng = default_rng(seedVal)\n",
    "    dist = get_truncated_normal(mean, sd, min_rating, max_rating)\n",
    "    ratings = dist.rvs(num_drivers, random_state=rng)\n",
    "    ratings = [round(x, 1) for x in ratings]\n",
    "    return ratings\n",
    "\n",
    "def abs_difference(ratings):\n",
    "    transposed = np.expand_dims(ratings, axis=1)\n",
    "    diff = abs(ratings-transposed) \n",
    "    return diff   \n",
    "\n",
    "def minmax(distance, fair_distance):\n",
    "    num_samples = len(distance)\n",
    "    mx, mn = distance.max(), distance.min()\n",
    "    dists = distance.flatten()\n",
    "    dists = np.asarray( [((x-mn)/(mx-mn)) for x in dists] )\n",
    "    distance = dists.reshape((num_samples, num_samples))\n",
    "    fair_distance = (fair_distance-mn)/(mx-mn)\n",
    "    return distance, fair_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_dist = driver_dists.mean()/dnr\n",
    "\n",
    "# dnr to NUM_SIM maps: (tells the number of similar drivers per driver for a given dnr (hence fair_dist))\n",
    "# manually curated for quick experimentation\n",
    "if flag==0:\n",
    "    sim2dnr_A = {80:4, 60:5, 40:7, 30:8, 20:11, 15:14, 10:21, 7:28, 5:53} \n",
    "    sim2dnr_B = {70:8, 60:9, 50:10, 40:12, 30:14, 20:19, 15:23, 10:32, 7:45, 5:65}\n",
    "    sim2dnr_C = {90:10, 80:11, 70:12, 60:13, 50: 15, 40:17, 30:22, 20:32, 15:42, 10:65, 8:80}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are true only when all the drivers in \"driver_idf\" are considered !   \n",
    "For \"driver_udf\", you can to create these dicts separately by using the following (commented out) cell    \n",
    "If these dicts are not created then we'll take driver_dists.mean() as the fair distance by default.   \n",
    "Recall, Fair distance : the distance within which pair of drivers are considreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE: \n",
    "# -------\n",
    "\"\"\"\n",
    "# used for creating sim2dnr_city\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "fair_dist = driver_dists.mean()/7\n",
    "# print(fair_dist)\n",
    "\n",
    "num_similar_drivers = []\n",
    "for idx in range(len(driver_dists)):\n",
    "    curr_driver = driver_dists[idx]\n",
    "    # the drivers 'similar' to this driver are the ones within fair_distance from this driver\n",
    "    num_sim = curr_driver[curr_driver<=fair_dist].shape[0]\n",
    "    # print(num_sim)\n",
    "    num_similar_drivers.append(num_sim) \n",
    "\n",
    "# print(num_similar_drivers)\n",
    "print(\"Number of similar drivers:\", np.mean(num_similar_drivers))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE: \n",
    "# -------\n",
    "\"\"\"\n",
    "# used for creating sim2dnr_city\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "ratings = generate_ratings(NUM_DRIVERS)\n",
    "ratings_dists = abs_difference(ratings)\n",
    "combined = w1*minmax(driver_dists, 0)[0] + w2*minmax(ratings_dists, 0)[0]\n",
    "fair_dist = driver_dists.mean()/3\n",
    "# print(fair_dist)\n",
    "\n",
    "num_similar_drivers = []\n",
    "for idx in range(len(combined)):\n",
    "    curr_driver = combined[idx]\n",
    "    # the drivers 'similar' to this driver are the ones within fair_distance from this driver\n",
    "    num_sim = curr_driver[curr_driver<=fair_dist].shape[0]\n",
    "    # print(num_sim)\n",
    "    num_similar_drivers.append(num_sim) \n",
    "\n",
    "# print(num_similar_drivers)\n",
    "print(\"Number of similar drivers:\", np.mean(num_similar_drivers))\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_ids = zone_dfs_dict[city]['zone_id']\n",
    "zone_id2idx = {zone_id: idx for idx, zone_id in enumerate(zone_ids)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHMS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FairAssign**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will go with the default parameters of cplex:\n",
    "from cplex import Cplex\n",
    "model = Cplex()\n",
    "model.parameters.simplex.tolerances.feasibility.get(),\\\n",
    "model.parameters.simplex.tolerances.optimality.get(),\\\n",
    "model.parameters.simplex.tolerances.markowitz.get()      \n",
    "\n",
    "# model.parameters.workmem.set(10240) # 10GB  \n",
    "# model.parameters.emphasis.memory.set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Clustering - LPP contstraints and Cplex\n",
    "from cplex import Cplex\n",
    "# from lp_tools import *\n",
    "from lp_tools_kn import * # with coefficient adjustment for k-nearest zone allocation\n",
    "\n",
    "alpha_fair = 2\n",
    "\n",
    "def fair_clustering(dataset, centres, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "  # Step 1: \t Create an instance of Cplex \n",
    "  problem = Cplex()\n",
    "  problem.parameters.simplex.tolerances.feasibility.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.optimality.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.markowitz.set(float(0.9999)) \n",
    "  problem.parameters.emphasis.memory.set(1)\n",
    "  problem.parameters.workmem.set(10240)\n",
    "\n",
    "  # Step 2: \t Declare that this is a minimization problem\n",
    "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
    "    \n",
    "  \"\"\"\n",
    "   Step 3.   Declare and  add variables to the model. \n",
    "        The function prepare_to_add_variables (dataset, centres) prepares all the required information for this stage.\n",
    "  \n",
    "    objective: a list of coefficients (float) in the linear objective function\n",
    "    lower bound: a list of floats containing the lower bounds for each variable\n",
    "    upper bound: a list of floats containing the upper bounds for each variable\n",
    "    variable_names: a list of strings that contains the name of the variables\n",
    "  \"\"\"\n",
    "  ## if working with \"lp_tools\":\n",
    "  print(\"Adding Variables...\")\n",
    "  \n",
    "  ## if working with \"lp_tools\":\n",
    "  # objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
    "  ## if working with \"lp_tools_kn\": \n",
    "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres, prohibited_assignments)\n",
    "  problem.variables.add(\n",
    "      obj = objective,\n",
    "      lb = lower_bound,\n",
    "      ub = upper_bound,\n",
    "      names = variable_names\n",
    "    )\n",
    "  \n",
    "  print(\"Variables Added !\")\n",
    "    \n",
    "    \n",
    "  \"\"\"\n",
    "  Step 4.   Declare and add constraints to the model.\n",
    "            There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
    "            Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
    "            The function prepare_to_add_constraints(dataset, centres) prepares the required data for this step.\n",
    "  \n",
    "   coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
    "   senses: a list of strings that identifies whether the corresponding constraint is\n",
    "           an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
    "   rhs: a list of floats corresponding to the rhs of the constraints.\n",
    "   constraint_names: a list of string corresponding to the name of the constraint\n",
    "  \"\"\"\n",
    "  print(\"Adding Constraints...\")\n",
    "    \n",
    "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair, fair_distance, ratings, flag)\n",
    "  print(\"num_constraints:\", len(senses)) \n",
    "  logger.info(f\"\\t\\t\\tnum_constraints = {len(senses)}\")\n",
    "  problem.linear_constraints.add(\n",
    "      rhs = rhs,\n",
    "      senses = senses,\n",
    "      names = row_names\n",
    "    )\n",
    "  problem.linear_constraints.set_coefficients(coefficients)\n",
    "\n",
    "  print(\"Constraints Added !\")\n",
    "    \n",
    "  # Step 5.\tSolve the problem\n",
    "  problem.solve()\n",
    "\n",
    "  result = {\n",
    "    \"status\": problem.solution.get_status(),\n",
    "    \"success\": problem.solution.get_status_string(),\n",
    "    \"objective\": problem.solution.get_objective_value(),\n",
    "    \"assignment\": problem.solution.get_values(),\n",
    "  }\n",
    "    \n",
    "  qm = problem.solution.quality_metric  \n",
    "  print(\"Solution Quality:\", problem.solution.get_float_quality([qm.max_x, qm.max_primal_infeasibility]))\n",
    "  \n",
    "  # print(\"Status:\", result['status']) # outputs a number: \"1\" for optimal solution, \"2\" for unbounded ray and \"3\" for infeasible solution\n",
    "  solution_status = result['status']\n",
    "  assert solution_status==1, \"Solution isn't optimal !\"\n",
    "\n",
    "  print(\"Status:\", problem.solution.get_status_string()) # optimal, unbounded ray, infeasible\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Assignment of drivers to the FFCs / warehouses\n",
    "import copy\n",
    "import dependent_rounding as dp\n",
    "\n",
    "# configParser.read(configFilePath)\n",
    "\n",
    "num_samples, num_centres = driver_locs.shape[0], zone_locs.shape[0]\n",
    "\n",
    "def fair_assignment(prob_dis, driver_loc):\n",
    "  '''Assigning the driver using the probaility distribution using dependent rounding'''  \n",
    "  \n",
    "  # \"prob_dis\" is the result of the Fair-LP program \"fair_clustering\"  \n",
    "  prob_dist = copy.deepcopy(prob_dis)\n",
    "  # print(\"prob_dist shape [num_drivers x num_ffc]:\", prob_dist.shape)\n",
    "\n",
    "  rounding = dp.DependentRounding(prob_dist)\n",
    "  rounding._buildGraph(prob_dist)\n",
    "  final_assignment = rounding.round()\n",
    "  final_assignment = np.around(final_assignment,2)\n",
    "  print(final_assignment)\n",
    "  driver_df = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df['ffc_index'] = -1 # unassigned\n",
    "\n",
    "  for i in range(num_samples):\n",
    "    for j in range(num_centres):\n",
    "      if abs(final_assignment[i][j]-1) < 0.2: # 0.01\n",
    "        driver_df.at[i,'ffc_index'] = j\n",
    "        \n",
    "  return driver_df, final_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(probs):\n",
    "    \"\"\"\n",
    "    To cope with bound violations which can occur upto the feasibility parameter range \n",
    "    So the lower bound of 0.0 on the probabilities can get violated and the values can go down to (0-feasibility_parameter_value)\n",
    "    \"\"\"\n",
    "    for i in range(len(probs)):\n",
    "        last_pos_index = -1\n",
    "        neg_value = 0\n",
    "        \n",
    "        for j in range(len(probs[0])):\n",
    "            assert probs[i][j] >= -1e-9 # 1e-6\n",
    "            \n",
    "            if probs[i][j] < 0:\n",
    "                neg_value += probs[i][j]\n",
    "                probs[i][j] = 0\n",
    "            elif probs[i][j] > 0:\n",
    "                last_pos_index = j\n",
    "\n",
    "        max_pos_index = np.argmax(probs[i])\n",
    "        probs[i][max_pos_index] += neg_value\n",
    "        \n",
    "        assert probs[i][max_pos_index] > 0\n",
    "        \n",
    "    return probs\n",
    "\n",
    "\n",
    "def picklify(ds, filepath):\n",
    "    pickling_on = open(filepath, \"wb\")\n",
    "    pickle.dump(ds, pickling_on)\n",
    "    pickling_on.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main :\n",
    "def FairAssign_solver(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments):\n",
    "    # Fair-LP:\n",
    "    # lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    try:\n",
    "        lp_output = fair_clustering(driver_locs, zone_locs, lower_cap, upper_cap, fair_distance, prohibited_assignments)\n",
    "    except:\n",
    "        logger.error(\"Solution Non-optimal (Unbounded Ray or Infeasible) !\")\n",
    "        return None, None\n",
    "    prob_dis = np.reshape(lp_output['assignment'][:num_samples*num_centres], (-1, num_centres))\n",
    "    \n",
    "    try:\n",
    "        prob_dist = sanityCheck(copy.deepcopy(prob_dis)) # this might raise an assertion error\n",
    "    except:\n",
    "        logger.error(\"Sanity Check Assertion !\")\n",
    "        return None, None\n",
    "    \n",
    "    # Randomized Dependent Rounding:\n",
    "    try:\n",
    "        df = fair_assignment(prob_dist, driver_locs)[0] # this might raise an assertion error\n",
    "        final_assignment = df['ffc_index'].values\n",
    "    except:\n",
    "        logger.error(\"Dependent Rounding Assertion !\")\n",
    "        return prob_dist, None\n",
    "    \n",
    "    return prob_dist, final_assignment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# COMMENT OUT THIS CELL IF THE ASSIGNMENT PICKLE FILE IS ALREADY PRESENT #\n",
    "\n",
    "print(f\"# drivers : {num_drivers}\")\n",
    "print(f\"# zones : {num_zones}\")\n",
    "\n",
    "if city=='A':\n",
    "    nk_list = [K_VAL] # only one element included, can add more for experimentation\n",
    "elif city=='B':\n",
    "    nk_list = [K_VAL] # [K_VAL] + [num_zones, num_zones//2]\n",
    "elif city=='C':\n",
    "    nk_list = [K_VAL]\n",
    "    \n",
    "print(\"How many nearest zones? :\", nk_list)\n",
    "\n",
    "# k_list = [(x-1) for x in nk_list] # assign only to k-nearest zones # [7, 5, 3]  \n",
    "try:    \n",
    "    if city=='A':\n",
    "        fd_dnr = sim2dnr_A[NUM_SIM]\n",
    "    elif city=='B':\n",
    "        fd_dnr = sim2dnr_B[NUM_SIM]\n",
    "    elif city=='C':\n",
    "        fd_dnr = sim2dnr_C[NUM_SIM]\n",
    "except:\n",
    "    fd_dnr = 1\n",
    "\n",
    "fd_list = [(driver_dists.mean()/alpha) for alpha in [fd_dnr]] # fair_distances\n",
    "ratings = generate_ratings(num_drivers)\n",
    "\n",
    "num_runs = 1\n",
    "# k_list and fd_list contain hyperparameters\n",
    "\n",
    "k_fd_dict = {k:\\\n",
    "                {fd_idx:\\\n",
    "                    {num_run:\\\n",
    "                        {'p_dist':None, 'assignment':None}\n",
    "                        for num_run in range(num_runs)\n",
    "                    } \n",
    "                    for fd_idx in range(len(fd_list))\n",
    "                } \n",
    "            for k in nk_list\n",
    "            }\n",
    "\n",
    "assign_results_path = f\"assign_results/results_{city}/\"\n",
    "if not os.path.exists(assign_results_path):\n",
    "    os.makedirs(assign_results_path) # directory to store the results of FairAssign_solver\n",
    "\n",
    "## BEWARE\n",
    "# shutil.rmtree(assign_results_path)\n",
    "# os.mkdir(assign_results_path) # doesn't work for nested directories\n",
    "\n",
    "start = time.time()\n",
    "for k in nk_list:\n",
    "    logger.info(f\"Considering k = {k} nearest zones\")\n",
    "    prhbtd_assigns = get_prohibited_assigns(dz_dist, k)\n",
    "    \n",
    "    for f_idx, fair_distance in enumerate(fd_list):\n",
    "        logger.info(f\"\\tfair_distance = {fair_distance}\")\n",
    "        for num_run in range(num_runs):\n",
    "            logger.info(f\"\\t\\tnum_run = {num_run}\")\n",
    "            prob_dist, final_assignment = FairAssign_solver(driver_locs, zone_locs, lower_caps, upper_caps, fair_distance, prhbtd_assigns)\n",
    "            k_fd_dict[k][f_idx][num_run]['p_dist'] = prob_dist\n",
    "            k_fd_dict[k][f_idx][num_run]['assignment'] = final_assignment \n",
    "            \n",
    "    # Store intermediate results as well as fail-safe:\n",
    "    # saving current state of \"k_fd_dict\":\n",
    "    if wts==-1:\n",
    "        filepath = os.path.join(assign_results_path, f\"dict_k={k}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}.pickle\")\n",
    "    else:\n",
    "        filepath = os.path.join(assign_results_path, f\"dict_k={k}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}_{wts}.pickle\")\n",
    "    \n",
    "    # print(filepath)\n",
    "    picklify(k_fd_dict, filepath)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Execution time: {(end-start)/3600}hrs\")\n",
    "logger.info(f\"Execution time: {(end-start)/3600}hrs\")\n",
    "\n",
    "if wts==-1:\n",
    "    final_file_path = os.path.join(assign_results_path, f\"Assignments_{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}.pickle\")\n",
    "else:\n",
    "    final_file_path = os.path.join(assign_results_path, f\"Assignments_{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}_{wts}.pickle\")\n",
    "\n",
    "picklify(k_fd_dict, final_file_path)\n",
    "print(\"Assignments file saved in:\", final_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment(result_path, driver_locs, k):\n",
    "    '''\n",
    "    returns ffc_index (or zone index) for each driver based on the \"FairAssign\" assignment \n",
    "    '''\n",
    "    pickle_off = open(result_path, \"rb\")\n",
    "    assignments = pickle.load(pickle_off)\n",
    "    prob_dist = assignments[k][0][0]['p_dist']\n",
    "    ## get assignment by applying dependent rounding: \n",
    "    df = fair_assignment(prob_dist, driver_locs)[0] \n",
    "    final_assignment = df['ffc_index'].values\n",
    "\n",
    "    return final_assignment\n",
    "\n",
    "# RoundRobin assignment while maintaining upper capacity bounds of zones only:\n",
    "def round_robin_dist(day_num, driver_locs):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rr_df = pd.DataFrame(driver_locs, columns=['lat', 'lng'])\n",
    "    rr_df['bz_idx_rr'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = (day_num+1) % num_zones\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = (zone+1) % num_zones \n",
    "        rr_df.at[i, 'bz_idx_rr'] = int(zone)\n",
    "        temp_upper_cap[zone] -= 1 \n",
    "    return rr_df\n",
    "\n",
    "# Random assignment while maintaining only upper capacity bounds of the zones:\n",
    "def random_dist(day_num, driver_locs, upper_caps):\n",
    "    random.seed(1234+day_num)\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "    # print(num_drivers, num_zones)\n",
    "    rand_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    rand_df['bz_idx_rand'] = -1\n",
    "    temp_upper_cap = copy.deepcopy(list(upper_caps))\n",
    "    for i in range(num_drivers):\n",
    "        zone = random.randint(1, num_zones)-1\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            zone = random.randint(1, num_zones)-1\n",
    "        rand_df.at[i, 'bz_idx_rand'] = zone \n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return rand_df \n",
    "\n",
    "# LIPA while maintaining upper capacity bounds of zones only:\n",
    "def lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes):\n",
    "    num_drivers = len(driver_locs)\n",
    "    num_zones = len(lower_caps)\n",
    "\n",
    "    lipa_df = pd.DataFrame(driver_locs, columns=[\"lat\", \"lng\"])\n",
    "    lipa_df['bz_idx_lipa'] = -1\n",
    "\n",
    "    temp_upper_cap = copy.deepcopy(upper_caps)\n",
    "\n",
    "    driver_idx_inc = np.argsort(np.array(driver_prev_incomes))\n",
    "    zone_idx_inc = np.argsort(np.array(zone_prev_incomes))\n",
    "  \n",
    "    j = num_zones-1\n",
    "    for i in driver_idx_inc:\n",
    "        zone = zone_idx_inc[j]\n",
    "        while(temp_upper_cap[zone]<=0):\n",
    "            j = j-1\n",
    "            zone = zone_idx_inc[j]\n",
    "        lipa_df.at[i, 'bz_idx_lipa'] = zone\n",
    "        temp_upper_cap[zone] -= 1\n",
    "    return lipa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False), \"Probability distributions have been calculated! Now, just perform Step 2 (File generation) for all 6 days before going to Step 3 [refer to the instructions in the next part]\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. FILE GENERATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have the probability distributions corresponding to each driver. The next step is to round these distributions to get an assignemnt for 'day_num'. And then run a last-mile delivery algorithm on top of this assignment.\n",
    "\n",
    "- However, in order to perform simulations using a last-mile delivery algorithm such as [FoodMatch](https://github.com/idea-iitd/FoodMatch/tree/master/Swiggy) or [FairFoody](https://github.com/idea-iitd/fairfoody), we need to generate relevant \"de_intervals\" files that are required by those algorithms based on our assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 (File Generation) needs to run separately for each of the $6$ days. On each day, for each driver $d$, sample a zone by rounding the probability distribution obtained for $d$ in Step 1 (Assignments). Store the assignments by each algo on each day (will be used later while computing the 'spatial stability' evaluation metric)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = 6\n",
    "algos = ['FairAssign', 'RoundRobin', 'LIPA']\n",
    "global_assignments = {alg:\\\n",
    "                        {day+1: [-1]*NUM_DRIVERS for day in range(num_days)}\n",
    "                      for alg in algos\n",
    "                      }\n",
    "# global_assignments[algo][day_num].shape() : 1 x num_drivers  \n",
    "# global_assignments : dict[dict[List]] ; the innermost List is of size NUM_DRIVERS with i-th element representing the assigned zone for driver by the respective algorithm \n",
    "# we don't need to store driver ids in 'global_assignemnts' since they follow the same order as present in 'driver_idf'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells (upto Step 3 (Simulations)) need to run for each **day_num** i.e., *day_num* $\\in$ {1, 2, 3, 4, 5, 6} and each **algorithm** i.e., *algorithm* $\\in$ algos, before going to Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_num = 1           # {1, 2, 3, 4, 5, 6}\n",
    "algo = 'FairAssign'   # {'FairAssign', 'RoundRobin', 'Random', 'LIPA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim of this cell is to get \"assign_df\"\n",
    "for algo in algos:\n",
    "    for day_num in range(1, num_days+1):\n",
    "        print(algo)\n",
    "        assign_df = None\n",
    "\n",
    "        if algo=='FairAssign':\n",
    "            result_path = \"./assign_results/results_A/Assignments_10z_32simahm.pickle\"\n",
    "            assignment = get_assignment(result_path, driver_locs, K_VAL) \n",
    "            assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "            # original base zones\n",
    "            assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "            assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "            # base zones assigned by Fair Assign\n",
    "            assign_df['new_bz_idx'] = assignment\n",
    "            # assign_df['fa_bz_idx'] = assignment\n",
    "            print(assign_df[assign_df['bz_idx']!=assign_df['new_bz_idx']].shape[0])\n",
    "            \n",
    "        elif algo=='RoundRobin':\n",
    "            rr_df = round_robin_dist(day_num, driver_locs)\n",
    "            assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS])\n",
    "            # original base zones\n",
    "            assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "            assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "            # base zones assigned by \"RoundRobin\"\n",
    "            assign_df['new_bz_idx'] = rr_df['bz_idx_rr'].values\n",
    "\n",
    "        elif algo=='LIPA':\n",
    "            # use the first day of FoodMatch as the first day of LIPA \n",
    "\n",
    "            # Requires simulation results of previous day\n",
    "            local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "            local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "            day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "            sim_path = f'results/sim_results/sim_results_{city}/{NUM_DRIVERS}_{algo}/sim_results_lipa{day_num-1}'\n",
    "            print(sim_path)\n",
    "\n",
    "            data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "            data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "            data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "            vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "            # vehicle_ids = driver_idf['de_id'].values\n",
    "            # print(vehicle_ids)\n",
    "            data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "            for d_id in driver_idf['de_id']:\n",
    "                try:\n",
    "                    day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "                except:\n",
    "                    # handles the cases for which d_id is not present in data_deliver \n",
    "                    continue\n",
    "                            \n",
    "            local_incomes_df[f'day{day_num}'] = local_incomes_df['de_id'].map(day_incomes) \n",
    "            # previous day incomes are used to determine the next day's assignment:\n",
    "            fm_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "            fm_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "            fm_inc_df = copy.deepcopy(local_incomes_df)\n",
    "            fm_inc_df = pd.merge(fm_inc_df, driver_idf, on='de_id')\n",
    "            print(day_incomes, fm_inc_df)\n",
    "            # find day 1 incomes of drivers (FairAssign):\n",
    "            prev_incomes_df = copy.deepcopy(fm_inc_df[['de_id', 'lat', 'lng', 'day1']])\n",
    "            driver_prev_incomes = prev_incomes_df['day1'].values\n",
    "\n",
    "            # find day 1 number of orders in each zone (FairAssign):\n",
    "            orders_data = pd.read_csv(f\"data/orders_data/orders_{day}.csv\") \n",
    "            df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "            cust_zones = df['customer_zone'].unique()\n",
    "            cust_zones_gb = df.groupby('customer_zone')\n",
    "\n",
    "            orders_per_zone = {key:0 for key in cust_zones if key in zone_ids.values}\n",
    "            for key in cust_zones:\n",
    "                if key in zone_ids.values:\n",
    "                    orders_per_zone[key] = cust_zones_gb.get_group(key).shape[0] \n",
    "\n",
    "            # orders_per_zone = {k: v for k, v in sorted(orders_per_zone.items(), key=lambda item: item[1])}\n",
    "            orders_per_zone = {k:v for k, v in sorted(orders_per_zone.items())}\n",
    "            zone_prev_incomes = [v for k, v in orders_per_zone.items()] \n",
    "            print(zone_prev_incomes)\n",
    "            # -----------------------------------------------------------------------\n",
    "            lipa_df = lipa_dist(driver_locs, driver_prev_incomes, zone_prev_incomes)\n",
    "            print(driver_prev_incomes)\n",
    "            assign_df = copy.deepcopy(driver_idf[:NUM_DRIVERS]) \n",
    "            # original base zones\n",
    "            assign_df = pd.merge(assign_df, base_zones_dict[city][['de_id', 'base_zone']], on='de_id')\n",
    "            assign_df['bz_idx'] = assign_df['base_zone'].map(zone_id2idx)\n",
    "            # base zones assigned by LIPA\n",
    "            assign_df['bz_idx_lipa'] = lipa_df['bz_idx_lipa']\n",
    "            assign_df['new_bz_idx'] = assign_df['bz_idx_lipa']\n",
    "\n",
    "        global_assignments[algo][day_num] = assign_df['new_bz_idx'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the \"LIPA\" baseline, we need to have the FoodMatch simulation corresponding to day 1 beforehand. Also, unlike other baselines, LIPA assignment for day 'd' depends on the simulation done using LIPA assignment of day 'd-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assign_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the ./de_intervals directory required for the last-mile delivery algorithm 'FoodMatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"File generation starts ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation of locations within a given zone\n",
    "\n",
    "# generating random locations withing a zone (given the zone boundary):\n",
    "def random_loc_generator(zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  min_lat, max_lat = min(lats), max(lats)\n",
    "  min_lng, max_lng = min(longs), max(longs)\n",
    "  new_lat, new_lng = random.uniform(min_lat, max_lat), random.uniform(min_lng, max_lng) \n",
    "  return [new_lat, new_lng]\n",
    "\n",
    "# Checking if a given location lies inside a given zone:\n",
    "def path_related_preprocessing(path_bdry):\n",
    "  # exemplar path_bdry: '12.954619258010608,77.6149292592163 12.954680993923494,77.61640664016727 ....'\n",
    "  path_bdry = str(path_bdry)\n",
    "  df = pd.DataFrame({'lts':[], 'lngs':[]})\n",
    "  bdry_locs = path_bdry.split()\n",
    "  lats, longs = [], []\n",
    "  for loc in bdry_locs:\n",
    "    lat, lng = loc.split(',')\n",
    "    lats.append(float(lat))\n",
    "    longs.append(float(lng))\n",
    "  return lats, longs\n",
    "\n",
    "def loc_in_zone(loc, zone_bdry):\n",
    "  lats, longs = path_related_preprocessing(zone_bdry)\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  polygon = geometry.MultiPoint(coords).convex_hull\n",
    "  Point_X, Point_Y = loc[0], loc[1]\n",
    "  point = geometry.Point(Point_X, Point_Y)\n",
    "  return point.within(polygon)\n",
    "\n",
    "# code to generate 'm' locations that lie within a given zone:\n",
    "def generate_locs(m, zone_bdry):\n",
    "    new_locs = []\n",
    "    num_generated = 0\n",
    "    while num_generated < m:\n",
    "        new_loc = random_loc_generator(zone_bdry)\n",
    "        sanity_check = loc_in_zone(new_loc, zone_bdry)\n",
    "        if sanity_check:\n",
    "            num_generated += 1\n",
    "            new_locs.append(new_loc)\n",
    "    return new_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_csv(f\"data/location_data/location_{city}.csv\", header=None)\n",
    "location_df = location_df.rename(columns={0:'node_id', 1:'lat', 2:'lng'})\n",
    "# location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the files corresponding to driver_idf:\n",
    "de_idf_ids = driver_idf['de_id'][:NUM_DRIVERS]\n",
    "\n",
    "de_idf_files = []\n",
    "for d_id in de_idf_ids:\n",
    "    file_name = 'data/de_data/'+city +'_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    de_idf_files.append(file_name)\n",
    "# de_idf_files\n",
    "\n",
    "# get the node_ids:\n",
    "orig_node_ids = []\n",
    "na = 0 # number of files in de_idf_files which are not present in de_intervals/\n",
    "for file in de_idf_files[:NUM_DRIVERS]:\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        na += 1\n",
    "        d_id = int(file.split('/')[-1][:-4])\n",
    "        to_drop_idx = assign_df[assign_df['de_id']==d_id].index\n",
    "        assign_df = assign_df.drop(to_drop_idx)\n",
    "        # print(to_drop_idx)\n",
    "        continue \n",
    "    # get starting node ids corresponding to all shifts, it will be useful for random generation for unswappable drivers\n",
    "    num_shifts = int(file_df.shape[0]/2)\n",
    "    node_id = [int(file_df.iloc[x*2].values[0].split()[1]) for x in range(num_shifts)]\n",
    "    orig_node_ids.append(node_id)\n",
    "\n",
    "assign_df['node_id'] = orig_node_ids \n",
    "assign_df['fa_node_id'] = assign_df['node_id']\n",
    "\n",
    "print(f\"{na}/{len(de_idf_files[:NUM_DRIVERS])} intersection drivers not found in ../{day_num}/de_intervals/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final file generation:\n",
    "intersection_files = []\n",
    "for d_id in driver_idf['de_id'][:NUM_DRIVERS]:\n",
    "    file_name = 'data/de_data/'+city+'_de_data/'+ str(day_num)+'/de_intervals/'+str(int(d_id))+'.csv'\n",
    "    intersection_files.append(file_name)\n",
    "\n",
    "# print(wts)\n",
    "if wts==-1:\n",
    "    old_dir_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}/de_int_old'\n",
    "    new_dir_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}/de_intervals'\n",
    "else:\n",
    "    old_dir_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}_{wts}/de_int_old'\n",
    "    new_dir_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}_{wts}/de_intervals'\n",
    "\n",
    "if os.path.exists(old_dir_path):\n",
    "    shutil.rmtree(old_dir_path)\n",
    "    \n",
    "if os.path.exists(new_dir_path):\n",
    "    shutil.rmtree(new_dir_path)\n",
    "\n",
    "os.makedirs(old_dir_path)\n",
    "os.makedirs(new_dir_path)\n",
    "\n",
    "for file in intersection_files:\n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    try:\n",
    "        file_df = pd.read_csv(file)\n",
    "    except:\n",
    "        print(\"file_df not found\")\n",
    "        continue \n",
    "    \n",
    "    curr_file = f'{d_id}.csv'\n",
    "    old_path = os.path.join(old_dir_path, curr_file)\n",
    "    new_path = os.path.join(new_dir_path, curr_file)\n",
    "\n",
    "    # old_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{wts}/de_int_old/{d_id}.csv'\n",
    "    # new_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{wts}/de_intervals/{d_id}.csv'\n",
    "\n",
    "    file_df.to_csv(old_path, index=False)\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format (meta-data) of csv files in ./de_intervals is as follows:   \n",
    "1. num_shifts -----> number of shifts    \n",
    "// num_shifts * 2 lines follow; 2 consecutive lines contain information of a single shift \n",
    "2. start_time start_node_id ----->    shift-1\n",
    "3. end_time end_node_id ----->        shift-1\n",
    "4. start_time start_node_id ----->    shift-2\n",
    "5. end_time end_node_id ----->        shift-2 \n",
    ".  \n",
    ".  \n",
    ".   \n",
    " \n",
    "Since the last-mile delivery algorithms don't use end_node_id, therefore end_node_id==-1 everywhere.   \n",
    "\n",
    "\n",
    "We only update the start_node_id for drivers with randomly generated start locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWAPPING LOGIC:\n",
    "# The swapping logic is applicable only to the drivers for which the fa_bz_idx is different from bz_idx\n",
    "rel_df = assign_df[assign_df['bz_idx'] != assign_df['fa_bz_idx']].reset_index()\n",
    "print(\"Number of drivers to be swapped:\", rel_df.shape[0])\n",
    "rel_df['paired_de'+str(day_num)] = None\n",
    "\n",
    "# first assign swappable nodes:\n",
    "bz_nodes_dict = {zone_idx:{'freq':0, 'nodes':[], 'paired_de_id':[]} for zone_idx, _ in enumerate(zone_ids)} # it'll contain the frequency of each base zone in rel_df['bz_idx'] as well as the corresponding node_ids in a list\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    z_id = int(rel_df.iloc[idx]['bz_idx'])\n",
    "    n_id = int(rel_df.iloc[idx]['node_id'][0])\n",
    "    paired_did = int(rel_df.iloc[idx]['de_id']) # remove later?\n",
    "    bz_nodes_dict[z_id]['freq'] += 1\n",
    "    bz_nodes_dict[z_id]['nodes'].append(n_id)\n",
    "    bz_nodes_dict[z_id]['paired_de_id'].append(paired_did) # remove later?\n",
    "bz_nodes_dict_store = copy.deepcopy(bz_nodes_dict)\n",
    "# IF a required zone_id in fa_bz_idx is present in bz_nodes_dict then use that data point\n",
    "# ELSE generate a random location in the zone corresponding to the zone_id\n",
    "num_random = 0 # number of drivers for whom random generation of location was done to get the corresponding start node\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    print(idx, end=' ')\n",
    "    z_id = int(rel_df.iloc[idx]['fa_bz_idx'])\n",
    "    if(bz_nodes_dict[z_id]['freq'] > 0):\n",
    "        rel_df.loc[idx, 'fa_node_id'] = bz_nodes_dict[z_id]['nodes'][0]\n",
    "        rel_df.loc[idx, 'paired_de'] = bz_nodes_dict[z_id]['paired_de_id'][0]\n",
    "        bz_nodes_dict[z_id]['freq'] -= 1\n",
    "        bz_nodes_dict[z_id]['nodes'].pop(0)\n",
    "        bz_nodes_dict[z_id]['paired_de_id'].pop(0) \n",
    "    else:\n",
    "        # randomly generate a location in the zone assigned by FairAssign\n",
    "        zone_df = zone_dfs_dict[city]\n",
    "        # zone_bdry = zone_df[zone_df['zone_id']==z_id]['path'].values[0] # Wrong ! bcz z_id is the index of zone_id\n",
    "        zone_bdry = zone_df.iloc[z_id]['path']\n",
    "        new_loc = generate_locs(1, zone_bdry)[0] \n",
    "        \n",
    "        # based on new_loc, get the closest node_id from location_df\n",
    "        min_dist = 1e9\n",
    "        n_id = -1\n",
    "        for i in range(location_df.shape[0]):\n",
    "            node_loc = [ location_df.iloc[i]['lat'], location_df.iloc[i]['lng'] ]\n",
    "            curr_dist = euclidean_distance(new_loc, node_loc)\n",
    "            if(curr_dist <= min_dist):\n",
    "                min_dist = curr_dist \n",
    "                n_id = location_df.iloc[i]['node_id']     \n",
    "        num_random += 1\n",
    "        num_shifts = int(len(rel_df.iloc[idx]['node_id']))\n",
    "        n_idz = [int(n_id)]*num_shifts \n",
    "        # the above line follows the assumption that the start_node_id is the same for each shift\n",
    "        # we could also generate new start nodes for every shift (see the commented out next cell) but that can be chaotic and impractical\n",
    "        # starting from the same area/market in each shift makes more sense than popping up at random locations for every shift\n",
    "        rel_df.at[idx, 'fa_node_id'] = n_idz\n",
    "        \n",
    "print()\n",
    "print(f\"{rel_df.shape[0]-num_random} data points out of {rel_df.shape[0]} could be swapped !\")\n",
    "print(f\"{num_random} data points were randomly generated !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE:\n",
    "# -------\n",
    "\"\"\"\n",
    "# Difference from next cell: For each shift of randomly generated drivers, the nodes are generated separately.\n",
    "# Unlike the the next cell, in which the same (randomly generated) node is used for all shifts.\n",
    "\n",
    "# SWAPPING LOGIC:\n",
    "# The swapping logic is applicable only to the drivers for which the fa_bz_idx is different from bz_idx\n",
    "rel_df = assign_df[assign_df['bz_idx'] != assign_df['fa_bz_idx']].reset_index()\n",
    "rel_df['paired_de'+str(day_num)] = None\n",
    "\n",
    "# first assign swappable nodes:\n",
    "bz_nodes_dict = {zone_idx:{'freq':0, 'nodes':[], 'paired_de_id':[]} for zone_idx, _ in enumerate(zone_ids)} # it'll contain the frequency of each base zone in rel_df['bz_idx'] as well as the corresponding node_ids in a list\n",
    "\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    z_id = int(rel_df.iloc[idx]['bz_idx'])\n",
    "    n_id = int(rel_df.iloc[idx]['node_id'][0])\n",
    "    paired_did = int(rel_df.iloc[idx]['de_id']) # remove later?\n",
    "\n",
    "    bz_nodes_dict[z_id]['freq'] += 1\n",
    "    bz_nodes_dict[z_id]['nodes'].append(n_id)\n",
    "    bz_nodes_dict[z_id]['paired_de_id'].append(paired_did) # remove later?\n",
    "\n",
    "bz_nodes_dict_store = copy.deepcopy(bz_nodes_dict)\n",
    "\n",
    "# IF a required zone_id in fa_bz_idx is present in bz_nodes_dict then use that data point\n",
    "# ELSE generate a random location in the zone corresponding to the zone_id\n",
    "num_random = 0 # number of drivers for whom random generation of location was done to get the corresponding start node\n",
    "for idx in range(rel_df.shape[0]):\n",
    "    print(idx, end=' ')\n",
    "    z_id = int(rel_df.iloc[idx]['fa_bz_idx'])\n",
    "    if(bz_nodes_dict[z_id]['freq'] > 0):\n",
    "        # use an existing data point\n",
    "        # rel_df.iloc[idx]['fa_node_id'] = bz_nodes_dict[z_id]['nodes'][0] # this gives a warning\n",
    "        rel_df.loc[idx, 'fa_node_id'] = bz_nodes_dict[z_id]['nodes'][0]\n",
    "        rel_df.loc[idx, 'paired_de'+str(day_num)] = bz_nodes_dict[z_id]['paired_de_id'][0]\n",
    "        print(bz_nodes_dict[z_id]['paired_de_id'][0])\n",
    "\n",
    "        bz_nodes_dict[z_id]['freq'] -= 1\n",
    "        bz_nodes_dict[z_id]['nodes'].pop(0)\n",
    "        bz_nodes_dict[z_id]['paired_de_id'].pop(0) # remove later?\n",
    "    \n",
    "    else:\n",
    "        # rel_df.loc[idx, 'fa_node_id'] = None\n",
    "        # num_random += 1\n",
    "        # continue\n",
    "        \n",
    "        # randomly generate a location in the zone assigned by FairAssign\n",
    "        zone_df = zone_dfs_dict[city]\n",
    "        # zone_bdry = zone_df[zone_df['zone_id']==z_id]['path'].values[0] # Wrong ! bcz z_id is the index of zone_id\n",
    "        zone_bdry = zone_df.iloc[z_id]['path']\n",
    "\n",
    "\n",
    "        # for each shift create a new location in the assigned zone itself \n",
    "        # this is a strong assumption but only done for a fraction of drivers, \n",
    "        # the good thing is that it is not done for the same set of drivers on all days AND\n",
    "        # it is still a weaker assumption that writing our own proof of concept dummny vanilla last mile delivery algorithm\n",
    "        num_shifts = int(len(rel_df.iloc[idx]['node_id']))\n",
    "        n_idz = []\n",
    "        for shift in range(num_shifts):\n",
    "            new_loc = generate_locs(1, zone_bdry)[0] \n",
    "\n",
    "            # based on new_loc, get the closest node_id from location_df\n",
    "            min_dist = 1e9\n",
    "            n_id = -1\n",
    "            for i in range(location_df.shape[0]):\n",
    "                node_loc = [ location_df.iloc[i]['lat'], location_df.iloc[i]['lng'] ]\n",
    "                curr_dist = euclidean_distance(new_loc, node_loc)\n",
    "            \n",
    "                if(curr_dist <= min_dist):\n",
    "                    min_dist = curr_dist \n",
    "                    n_id = location_df.iloc[i]['node_id']\n",
    "            \n",
    "            n_idz.append(int(n_id)) \n",
    "        print(n_idz)\n",
    "        # rel_df.iloc[idx]['fa_node_id'] = n_idz\n",
    "        # rel_df.loc[idx, 'fa_node_id'] = n_idz\n",
    "        # for some reason both of the above 2 lines don't work\n",
    "        rel_df.at[idx, 'fa_node_id'] = n_idz\n",
    "    \n",
    "        num_random += 1\n",
    "\n",
    "print()\n",
    "print(f\"{rel_df.shape[0]-num_random} data points out of {rel_df.shape[0]} could be swapped !\")\n",
    "print(f\"{num_random} data points were randomly generated !\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the de_interval profiles of swappable or paired drivers are to be swapped \n",
    "# and that of drivers for whom random locations are generated, the profile is to be modified\n",
    "rel_drivers = rel_df['de_id']\n",
    "\n",
    "rel_files = []\n",
    "for d_id in rel_drivers:\n",
    "    file_name = 'data/de_data/'+city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(d_id)) + '.csv'\n",
    "    rel_files.append(file_name)\n",
    "\n",
    "for file in rel_files:\n",
    "    # d_id = file[40:-4] \n",
    "    d_id = int(file.split('/')[-1][:-4])\n",
    "    file_df = pd.read_csv(file)\n",
    "\n",
    "    swap_node = rel_df[rel_df['de_id']==int(d_id)].paired_de.values[0]\n",
    "    # for those drivers who could be swapped:\n",
    "    if not np.isnan(swap_node):\n",
    "        swap_with_file = 'data/de_data/' + city + '_de_data/'+ str(day_num) + '/de_intervals/' + str(int(swap_node)) + '.csv'\n",
    "        file_df = pd.read_csv(swap_with_file)\n",
    "    # for those drivers whose starting nodes for each shift were randomly generated\n",
    "    else:\n",
    "        print(file)\n",
    "        num_shifts = int(file_df.shape[0]/2)\n",
    "        new_start_nodes = rel_df[rel_df['de_id']==int(d_id)].fa_node_id.values[0]\n",
    "        for i in range(num_shifts):\n",
    "            new_node = new_start_nodes[i]\n",
    "            start_time = file_df.iloc[i*2].values[0].split()[0] # start time of i-th shift\n",
    "            file_df.iloc[i*2] = str(start_time)+ ' ' + str(new_node) \n",
    "\n",
    "    if wts==-1:\n",
    "        new_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}/de_intervals/{d_id}.csv'\n",
    "    else:\n",
    "        new_path = f'data/de_data/{city}_de_data/{day_num}/{city}_{NUM_DRIVERS}_{NUM_SIM}_{K_VAL}_{flag}_{wts}/de_intervals/{d_id}.csv'\n",
    "\n",
    "    file_df.to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"File generation ends!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False), f\"File generation for day_num={day_num} has been done! Now perform file generation for day_num={day_num+1} while day_num \\in {1, 2, 3, 4, 5, 6}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Simulations**       \n",
    "---\n",
    "Follow the given steps to apply the last-mile delivery algorithm on top of the new assignments and get the simulation dumps:     \n",
    "- For _FoodMatch_, refer to https://github.com/idea-iitd/FoodMatch     \n",
    "\n",
    "- Before running the simulation, however, first copy the 'de_intervals' directories from \"data/de_data/{city}\\_de_data/{day_num}/{city}\\_{NUM_DRIVERS}\\_{NUM_SIM}\\_{K_VAL}/\" (or \"data/de_data/{city}\\_de_data/{day_num}/{city}\\_{NUM_DRIVERS}\\_{NUM_SIM}\\_{K_VAL}\\_{wts}/) to \"FoodMatch/Swiggy/data/data\\_{city}_anonymized/food_data/{day_num}\" for day_num in [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "- save the simulation output corresponding to each day 'day_num' using the following nomenclature:   \n",
    "\"sim_results_{algo}{day_num}\" for each day_num; day_num \\in {1, 2, 3, 4, 5, 6} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the results on 'FoodMatch' or 'FairFoody' w/o considering the new assignments done by the FairAssign, don't copy the directories as directed in the previous step rather just simulate on the data that FoodMatch and FairFoody provide directly.   \n",
    "\n",
    "For evaluation, remember to save the simulations as \"sim_results_{algo}{day_num}\". Take algo='foodmatch' for FoodMatch (only) simulations and algo='fairfoody' for FairFoody (only) simulations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose algorithm \n",
    "algo = 'FairAssign' # {'FairAssign', 'RoundRobin', 'LIPA', 'FoodMatch', 'FairFoody'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS:\n",
    "\n",
    "def gini_index(incomes):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum() \n",
    "    inc_sum = 0.0\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            inc_sum += abs(incomes[i]-incomes[j])\n",
    "    gini = inc_sum / (2*num*total)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def avg_distance(zone_labels, driver_locs, zone_locs):\n",
    "    \"\"\"\n",
    "    returns the 'cost' of the assignment\n",
    "    zone_labels: indices of the assigned zones\n",
    "    a zone_label 'z' has location zone_locs[z]\n",
    "    \"\"\"\n",
    "    driver_dists = L2Distance(driver_locs) \n",
    "    num = len(zone_labels)\n",
    "    dist = 0.0 \n",
    "    for i in range(num):\n",
    "        assigned_zone = zone_labels[i]\n",
    "        driver_loc, zone_loc = driver_locs[i], zone_locs[int(assigned_zone)]\n",
    "        driver_zone_dist = euclidean_distance(driver_loc, zone_loc)\n",
    "        dist += np.sqrt(driver_zone_dist)\n",
    "    avg_dist = dist/num\n",
    "    return avg_dist                       \n",
    "    \n",
    "\n",
    "# def spatial_inequality_index(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "def spatial_inequality_index(incomes, driver_dists, fair_distance):\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    term_i = 0.0    \n",
    "    for i in range(num):\n",
    "        sum_j = 0.0\n",
    "        num_j = 1e-9    \n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j] <= fair_distance and driver_dists[i][j]>0:\n",
    "                num_j += 1\n",
    "                sum_j += abs(incomes[i]-incomes[j])   \n",
    "        term_i += (sum_j / num_j) \n",
    "    \n",
    "    spin_idx = term_i / total \n",
    "    # spin_idx = round(spin_idx, 2)\n",
    "    return spin_idx \n",
    "\n",
    "\n",
    "# def income_gap(incomes, driver_locs, ratings, combined, fair_distance):\n",
    "def income_gap(incomes, driver_dists, fair_distance): \n",
    "    \"\"\" \n",
    "    difference between incomes between any two drivers per unit distance (within fair_distance) \n",
    "    \"\"\"\n",
    "    alpha = 100\n",
    "    driver_dists = driver_dists * alpha\n",
    "    num = len(incomes)\n",
    "    total = incomes.sum()\n",
    "    terms = 0.0\n",
    "    num_pair_drivers = 1e-7 # NOT 0 => to avoid division by 0\n",
    "    for i in range(num-1):\n",
    "        for j in range(i+1, num):\n",
    "            if driver_dists[i][j]>0:\n",
    "                num_pair_drivers += 1\n",
    "                terms += (abs(incomes[i]-incomes[j])/driver_dists[i][j])\n",
    "    inc_gap = terms/num_pair_drivers\n",
    "    # inc_gap = round(inc_gap, 2)\n",
    "    return inc_gap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the FoodMatch simulation results for all 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incomes_df(algo):\n",
    "    # get the incomes on all 6 days for all drivers \n",
    "    '''\n",
    "    input string: algo\n",
    "    'fm' : FoodMatch\n",
    "    'fafm' : FairAssign + FoodMatch\n",
    "    '''\n",
    "    local_incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "    local_incomes_df['de_id'] = driver_idf['de_id']\n",
    "\n",
    "    num_days = 6\n",
    "    day_incomes = {d_id:None for d_id in driver_idf['de_id']}\n",
    "\n",
    "    for day in range(1, num_days+1):\n",
    "        sim_path = f'results/sim_results/sim_results_{city}/{NUM_DRIVERS}_{algo}/sim_results_{algo}{day}'\n",
    "        # print(sim_path)\n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        # print(data_deliver)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "        data_deliver_gb = data_deliver.groupby('vehicle_id')\n",
    "        for d_id in driver_idf['de_id']:\n",
    "            try:\n",
    "                day_incomes[d_id] = int(data_deliver_gb.get_group(d_id).shape[0])\n",
    "            except:\n",
    "                # handles the cases for which d_id is not present in data_deliver \n",
    "                continue  \n",
    "        local_incomes_df[f'day{day}'] = local_incomes_df['de_id'].map(day_incomes)\n",
    "    \n",
    "    cols = ['day1', 'day2', 'day3', 'day4', 'day5', 'day6'] \n",
    "    local_incomes_df['num_orders'] = local_incomes_df[cols].sum(axis=1) \n",
    "\n",
    "    return copy.deepcopy(local_incomes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_idf = driver_idf[:NUM_DRIVERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo can take values in {'foodmatch', 'fairfoody', 'fairassign', 'roundrobin', 'lipa'}\n",
    "incomes_df = pd.DataFrame(columns=['de_id', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6']) \n",
    "incomes_df['de_id'] = driver_idf['de_id']\n",
    "incomes_df = get_incomes_df(algo)\n",
    "incomes_df = pd.merge(incomes_df, driver_idf, on='de_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating driver_dists\n",
    "driver_dists = L2Distance(driver_locs)\n",
    "\n",
    "# generating ratings\n",
    "num_drivers = driver_dists.shape[0]\n",
    "ratings = generate_ratings(num_drivers)\n",
    "\n",
    "# generating combined \n",
    "ratings_matrix = abs_difference(ratings)\n",
    "normalized_dists = minmax(driver_dists, 0)[0]\n",
    "normalized_ratings = minmax(ratings_matrix, 0)[0]\n",
    "_combined = w1*driver_dists + w2*ratings_matrix \n",
    "combined = minmax(_combined, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = incomes_df['lat']\n",
    "longs = incomes_df['lng']\n",
    "incomes = incomes_df['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(lats, longs, incomes, ratings, combined, fair_dist): \n",
    "    d_locs = [[lat, lng] for lat, lng in zip(lats, longs)]\n",
    "    incomes = np.array(incomes)\n",
    "    gini = gini_index(incomes) \n",
    "    sp_idx = spatial_inequality_index(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    inc_gp = income_gap(incomes, d_locs, ratings, combined, fair_dist)\n",
    "    return gini, sp_idx, inc_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag==0:\n",
    "    driver_dists = driver_dists \n",
    "if flag==1:\n",
    "    driver_dists = ratings_matrix \n",
    "if flag==2:\n",
    "    driver_dists = combined\n",
    "\n",
    "def num_sim_drivers(fd):\n",
    "    ''' \n",
    "    number of similar drivers (i.e., for a given driver, how many drivers are being considered for fairness comparison per)\n",
    "    '''\n",
    "    num_similar_drivers = []\n",
    "    for idx in range(len(driver_dists)):\n",
    "        curr_driver = driver_dists[idx]\n",
    "        num_sim = curr_driver[curr_driver<=fd].shape[0]\n",
    "        num_similar_drivers.append(num_sim) \n",
    "    return np.mean(num_similar_drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the metrics at different fair_dist values: \n",
    "if flag==0:\n",
    "    fair_dist = driver_dists.mean()/8 \n",
    "if flag==1:\n",
    "    fair_dist = 1 \n",
    "if flag==2:\n",
    "    fair_dist = 0.04 \n",
    "    \n",
    "def eval_results(lats, longs, incomes):\n",
    "    results = []\n",
    "    for k in range(1, 11):\n",
    "        fd = fair_dist/k \n",
    "        gini, sp_idx, inc_gp = metrics(lats, longs, incomes, ratings, combined, fd)\n",
    "        results.append([fd, num_sim_drivers(fd), gini, sp_idx, inc_gp])\n",
    "    result_df = pd.DataFrame(results)\n",
    "    cols = ['fair_dist', 'sim_drivers', 'gini', 'spatial_ineq', 'income_gap']\n",
    "    result_df.columns = cols\n",
    "    return result_df\n",
    "\n",
    "# FINAL RESULTS:\n",
    "fafm_results_df = eval_results(lats, longs, incomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results:\")\n",
    "print(fafm_results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Avg. Distance (or Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_this_driver(locs, v_id):\n",
    "    # driver's inital location:\n",
    "    v_init_loc = driver_idf[driver_idf['de_id']==v_id][['lat', 'lng']].values[0]\n",
    "    first_mile_dist = euclidean_distance(locs[0], v_init_loc)\n",
    "    last_mile_dist = 0\n",
    "    for idx in range(1, len(locs)):\n",
    "        prev_loc = locs[idx-1]\n",
    "        curr_loc = locs[idx] \n",
    "        last_mile_dist += euclidean_distance(prev_loc, curr_loc) \n",
    "    return first_mile_dist, last_mile_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the incomes on all 6 days for all drivers \n",
    "num_days = 6\n",
    "\n",
    "def get_cost(algo, num_days):\n",
    "    ''' \n",
    "    algo: str\n",
    "    'fm': FoodMatch \n",
    "    'fafm': FairAssign then FoodMatch\n",
    "    '''\n",
    "    cust_lats, cust_lngs = [], []\n",
    "    first_mile_cost = 0.0 # over all num_days days\n",
    "    last_mile_cost = 0.0 # over all num_days days\n",
    "    for day in range(1, num_days+1):\n",
    "        # print(day)\n",
    "        total_first_mile = 0\n",
    "        total_last_mile = 0\n",
    "        orders_data = pd.read_csv(f\"data/orders_data/orders_{day}.csv\") \n",
    "        sim_path = f'sim_results_{algo}{day_num}'\n",
    "        data = pd.read_csv(sim_path, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"], on_bad_lines='skip')\n",
    "        data_deliver = data[data['a'] == \"DELIVER\"].drop(['a', 'e', 'f', 'g', 'h'], axis = 1)\n",
    "        data_deliver.columns = ['order_id', 'delivered_time', 'vehicle_id'] \n",
    "        \n",
    "        vehicle_ids = data_deliver['vehicle_id'].unique() \n",
    "\n",
    "        df = pd.merge(data_deliver, orders_data, on='order_id') \n",
    "        c_locs = df['customer_lat_lng'].values \n",
    "        df['cust_lat'] = [float(loc.split(',')[0]) for loc in c_locs]\n",
    "        df['cust_lng'] = [float(loc.split(',')[1]) for loc in c_locs]\n",
    "\n",
    "        cust_lats.append(df['cust_lat'])\n",
    "        cust_lngs.append(df['cust_lng'])\n",
    "\n",
    "        df_gb = df.groupby('vehicle_id') # Don't group on 'de_id' \n",
    "        for v_id in vehicle_ids:\n",
    "            curr_group = df_gb.get_group(v_id)\n",
    "            first_mile_dist, last_mile_dist = distance_this_driver(curr_group[['cust_lat', 'cust_lng']].values, v_id)\n",
    "            total_first_mile += first_mile_dist \n",
    "            total_last_mile += last_mile_dist\n",
    "        # avg cost for this day:\n",
    "        if len(vehicle_ids)>0: # for day 3 in city 'C': for some reason, no order gets delivered by foodmatch hence 0 vehicle ids corresponding to 'DELIVER' in 'data'\n",
    "            first_mile_cost += (total_first_mile/len(vehicle_ids))\n",
    "            last_mile_cost += (total_last_mile/len(vehicle_ids)) \n",
    "    # avg cost over all days:\n",
    "    first_mile_cost = first_mile_cost/num_days\n",
    "    last_mile_cost = last_mile_cost/num_days \n",
    "    cost = first_mile_cost + last_mile_cost \n",
    "    # print(first_mile_cost, last_mile_cost, cost)\n",
    "    return first_mile_cost, last_mile_cost, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-mile cost: distance travelled to reach the assigned zone center (proxy for zone) from home\n",
    "# last-mile cost: distance travelled to pick-up and deliver orders \n",
    "# total cost: sum of first-mile cost and last-mile cost\n",
    "\n",
    "first_mile, last_mile, total = get_cost(algo, num_days)\n",
    "\n",
    "print(\"Assignment cost:\", first_mile)\n",
    "print(\"Delivery cost:\", last_mile)\n",
    "print(\"Total Cost:\", total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def ind_spatial_stability(assign : List):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        assign: 1 x num_days \n",
    "        list of assigned zones \n",
    "    Output: \n",
    "        ss: spatial stability value for 'assign'\n",
    "    \"\"\"\n",
    "    # compute frequency distribution 'freqs'\n",
    "    score = 0.0\n",
    "    \n",
    "    ## Frequency distribution entropy\n",
    "    H = 0 \n",
    "    values, freqs = np.unique(assign, return_counts=True) \n",
    "    freqs = freqs/freqs.sum() \n",
    "    entropies = -1.0 * freqs * np.log(freqs) # log_e\n",
    "    H = entropies.sum() \n",
    "    \n",
    "    ## num_zone_changes\n",
    "    R = 0 \n",
    "    num_days = len(assign)\n",
    "    for idx in range(1, num_days):\n",
    "        if assign[idx] != assign[idx-1]:\n",
    "            R += 1\n",
    "\n",
    "    score = H * R\n",
    "    return score \n",
    "\n",
    "\n",
    "def spatial_stability(algo : str, all_assignments : Dict[str, Dict[str, List]]):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        'algo' \\in {'FairAssign', 'RoundRobin', 'Random', 'LIPA'}\n",
    "        all_assignments: Dict({algo (str) : Dict({day_num : List of size 1 x num_days})})\n",
    "    Output:\n",
    "        Spatial stability metric value for 'algo'\n",
    "    \"\"\"\n",
    "    ss = 0.0 \n",
    "    L = []\n",
    "\n",
    "    assignments = all_assignments[algo] # {day_num : list of assignments}\n",
    "    num_days = len(assignments)\n",
    "    \n",
    "    driver_assigns = np.zeros((num_days, NUM_DRIVERS))\n",
    "    for day_num in range(1, num_days+1):\n",
    "        day_assigns = assignments[day_num]\n",
    "        driver_assigns[day_num-1] = day_assigns \n",
    "    \n",
    "    driver_assigns = driver_assigns.T # shape: NUM_DRIVERS x num_days\n",
    "    # print(driver_assigns)\n",
    "    # print(driver_assigns.shape)\n",
    "    # if algo==\"LIPA\":\n",
    "    #     driver_assigns[:, 2] = driver_assigns[:, 1]\n",
    "    for d_idx in range(NUM_DRIVERS):\n",
    "        curr_driver = driver_assigns[d_idx]\n",
    "        curr_ss = ind_spatial_stability(curr_driver)\n",
    "        L.append(curr_ss)\n",
    "\n",
    "    ss = np.mean(L)\n",
    "    print(L)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = spatial_stability(algo, global_assignments)\n",
    "print(\"Spatial Stability:\", ss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
