{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import csv\n",
    "import math \n",
    "import time\n",
    "import random\n",
    "import ortools \n",
    "import datetime \n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.cluster import KMeans \n",
    "from geopy.geocoders import Nominatim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up configParser:\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = 'config.cfg' \n",
    "configParser.read(configFilePath)\n",
    "print(\"Configuration file read done !\")\n",
    "\n",
    "\n",
    "# File paths\n",
    "customer_dir = configParser.get('file-paths','customer_dir')\n",
    "geolocation_dir = configParser.get('file-paths','geolocation_dir')\n",
    "seller_dir = configParser.get('file-paths','seller_dir')\n",
    "order_dir = configParser.get('file-paths','order_dir')\n",
    "order_item_dir = configParser.get('file-paths','order_item_dir')\n",
    "print(\"File Paths obtained !\")\n",
    "\n",
    "# Creating dataframes\n",
    "customer_df = pd.read_csv(customer_dir)\n",
    "location_df = pd.read_csv(geolocation_dir)\n",
    "seller_df = pd.read_csv(seller_dir)\n",
    "order_df = pd.read_csv(order_dir)\n",
    "order_item_df = pd.read_csv(order_item_dir)\n",
    "\n",
    "# more work for location dataframe:\n",
    "state_or_city = configParser.get('dataset-generation','state_or_city') \n",
    "city_dataset = configParser.get('dataset-generation','city')           \n",
    "state_dataset = configParser.get('dataset-generation','state')         \n",
    "equal_or_not = int(configParser.get('dataset-generation','equal_to'))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**flag** :  \n",
    "It decides which distance metric/measure to consider:   \n",
    "0: euclidean distance (or physical distance)   \n",
    "1: rating   \n",
    "2: combination of euclidean distance and rating    \n",
    "   for which, 'w1' is weight given to euclidean distance and 'w2' is weight given to distance    \n",
    "   \n",
    "---\n",
    "\n",
    "**Hyperparameters** :\n",
    "- 'state' variable in config.cfg selects the dataset. It can take values in {'RJ', 'MG', 'SP'}\n",
    "- 'fair_distance' variable in config.cfg controls the fair distance considered in the LP. The results presented in the paper correspond to the following fair_distance values:\n",
    "    - 'fair_distance' for (flag=0 | flag=1 | flag=2) \n",
    "        - RJ : ( 0.30 | 1.00 | 0.50 )\n",
    "        - MG : ( 0.60 | 0.80 | 0.10 )\n",
    "        - SP : ( 1.25 | 1.00 | 0.50 )\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(1234)\n",
    "# plt.rcParams['figure.figsize'] = (10,7.5) \n",
    "\n",
    "# k = 10      # for k-nearest ffc/zone allocaiton\n",
    "# flag = 0    # controls fairness similarity measure\n",
    "# w1 = 0.7    # weight given to euclidean distance\n",
    "# w2 = 0.3    # weight given to ratings\n",
    "\n",
    "# state_dataset = 'SP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if state_dataset=='RJ':\n",
    "    if flag==0:\n",
    "        fair_distance = 0.30\n",
    "    elif flag==1:\n",
    "        fair_distance = 1.0\n",
    "    elif flag==2:\n",
    "        fair_distance = 0.50\n",
    "\n",
    "elif state_dataset=='SP':\n",
    "    if flag==0:\n",
    "        fair_distance = 0.60 \n",
    "    elif flag==1:\n",
    "        fair_distance = 0.80\n",
    "    elif flag==2:\n",
    "        fair_distance = 0.10\n",
    "\n",
    "elif state_dataset=='MG':\n",
    "    if flag==0:\n",
    "        fair_distance = 1.25\n",
    "    elif flag==1:\n",
    "        fair_distance = 1.0\n",
    "    elif flag==2:\n",
    "        fair_distance = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info: \n",
    "\n",
    "if state_or_city == 'state':\n",
    "  if equal_or_not == 1:\n",
    "    location_df = location_df[location_df['geolocation_state']==state_dataset]\n",
    "  else:\n",
    "    location_df = location_df[location_df['geolocation_state']!=state_dataset]\n",
    "else:\n",
    "  if equal_or_not == 1:\n",
    "    location_df = location_df[location_df['geolocation_city']==city_dataset]\n",
    "  else:\n",
    "    location_df = location_df[location_df['geolocation_city']!=city_dataset]\n",
    "\n",
    "# consider only zip code prefix level info (not fine-grained till lat-lng level)\n",
    "# Finally, location_df contains locations of the state \"state_dataset\"\n",
    "location_df = location_df.drop_duplicates('geolocation_zip_code_prefix')\n",
    "\n",
    "# Processing \"order_df\" \n",
    "# Take only 'delivered' orders with known delivery dates \n",
    "order_df = order_df[order_df['order_status']=='delivered']\n",
    "order_df = order_df[order_df['order_delivered_customer_date'].notna()]\n",
    "order_df['order_delivered_customer_date'] = pd.to_datetime(order_df['order_delivered_customer_date'])\n",
    "# Take only delivery dates, not delivery time\n",
    "order_df['order_delivered_customer_date'] = order_df['order_delivered_customer_date'].dt.date\n",
    "# Clubbing multiple dates into one, because the data for a single day is too small\n",
    "club_num_dates = int(configParser.get('dataset-generation','club_num_dates')) # 16 (first 15 days will be mapped to 1st day and last 15-16 days will be mapped to 17th day)\n",
    "\n",
    "def club_date(dt,club_num_dates):\n",
    "  if(club_num_dates > 30): # monthwise clubbing is the intention\n",
    "    return pd.datetime(dt.year, dt.month, 1)\n",
    "  return pd.datetime(dt.year, dt.month, int(dt.day/club_num_dates)*club_num_dates+1) \n",
    "\n",
    "order_df['order_delivered_customer_date'] = order_df['order_delivered_customer_date'].map(lambda x: club_date(x,club_num_dates))\n",
    "\n",
    "order_df = order_df.sort_values(\"order_delivered_customer_date\")\n",
    "order_df = order_df.reset_index()\n",
    "\n",
    "\n",
    "# Merging DataFrames\n",
    "overall_customer_df = pd.merge(customer_df,order_df,on='customer_id')\n",
    "overall_customer_loc_df = pd.merge(overall_customer_df,location_df,left_on='customer_zip_code_prefix',right_on='geolocation_zip_code_prefix')\n",
    "\n",
    "print(\"customer_df.shape:\", customer_df.shape)\n",
    "print(\"order_df.shape:\", order_df.shape)\n",
    "print(\"overall_customer_df.shape:\", overall_customer_df.shape)\n",
    "print(\"location_df.shape:\", location_df.shape)\n",
    "print(\"overall_customer_loc_df.shape:\", overall_customer_loc_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training Data\"\"\"\n",
    "training_data_percent = int(configParser.get('dataset-generation','training_data_percent'))\n",
    "print(\"Training data percentage:\", training_data_percent)\n",
    "\n",
    "start_date_loc = int(0.1*len(order_df.index)) \n",
    "start_training_date = order_df[\"order_delivered_customer_date\"].iloc[start_date_loc]\n",
    "end_date_loc = int((10+training_data_percent)/100*len(order_df.index))\n",
    "end_training_date = order_df[\"order_delivered_customer_date\"].iloc[end_date_loc]\n",
    "\n",
    "training_order_df = order_df[order_df['order_delivered_customer_date']<=end_training_date]\n",
    "training_order_df = training_order_df[order_df['order_delivered_customer_date']>=start_training_date]\n",
    "num_training_days = training_order_df['order_delivered_customer_date'].nunique()\n",
    "\n",
    "training_customer_df = pd.merge(customer_df,training_order_df,on='customer_id') \n",
    "training_customer_df = training_customer_df.drop_duplicates('customer_unique_id')\n",
    "\n",
    "training_order_item_df = pd.merge(order_item_df,training_order_df,on='order_id')\n",
    "training_seller_df = pd.merge(seller_df,training_order_item_df,on='seller_id')\n",
    "training_seller_df = training_seller_df.drop_duplicates('seller_id')\n",
    "\n",
    "training_customer_loc_df = pd.merge(training_customer_df, location_df,left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix')\n",
    "training_seller_loc_df = pd.merge(training_seller_df, location_df,left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix')\n",
    "\n",
    "training_customer_loc_df.rename(columns = {'customer_city':'city'}, inplace = True)\n",
    "training_seller_loc_df.rename(columns = {'seller_city':'city'}, inplace = True)\n",
    "\n",
    "training_customer_locations = training_customer_loc_df[['geolocation_lat','geolocation_lng','city']]\n",
    "training_seller_locations = training_seller_loc_df[['geolocation_lat','geolocation_lng','city']]\n",
    "\n",
    "training_combined_locations = pd.concat([training_customer_locations,training_seller_locations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Finding maximum and minimum capacity of a warehouse for training data\"\"\"\n",
    "lis = ['geolocation_lat','geolocation_lng']\n",
    "\n",
    "# Customers\n",
    "customer_locs = training_customer_locations[lis].values\n",
    "customer_locs = np.transpose(customer_locs)\n",
    "c_lts, c_lngs = customer_locs[0], customer_locs[1] \n",
    "plt.scatter(c_lngs,c_lts,color='Red',label='Customers')\n",
    "\n",
    "# Sellers\n",
    "seller_locs = training_seller_locations[lis].values\n",
    "seller_locs = np.transpose(seller_locs)\n",
    "s_lts, s_lgs = seller_locs[0], seller_locs[1]\n",
    "# plt.scatter(s_lgs,s_lts,color='Blue',label='Sellers')\n",
    "plt.scatter(s_lgs,s_lts,color='Blue',label='Drivers')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Customers and Sellers locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Data\"\"\" \n",
    "testing_order_df = order_df[order_df['order_delivered_customer_date'] > start_training_date]\n",
    "testing_customer_df = pd.merge(customer_df,testing_order_df,on='customer_id')\n",
    "testing_customer_df = testing_customer_df.drop_duplicates('customer_unique_id')\n",
    "testing_customer_loc_df = pd.merge(testing_customer_df, location_df,left_on = 'customer_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
    "num_testing_days = testing_customer_loc_df['order_delivered_customer_date'].nunique()\n",
    "\n",
    "testing_order_item_df = pd.merge(order_item_df,training_order_df,on='order_id')\n",
    "testing_seller_df = pd.merge(seller_df,testing_order_item_df,on='seller_id')\n",
    "testing_seller_df = testing_seller_df.drop_duplicates('seller_id')\n",
    "testing_seller_loc_df = pd.merge(testing_seller_df, location_df,left_on = 'seller_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
    "\n",
    "lat_mean = training_combined_locations['geolocation_lat'].mean()\n",
    "lat_std = training_combined_locations['geolocation_lat'].std()\n",
    "\n",
    "testing_customer_loc_df = testing_customer_loc_df[(testing_customer_loc_df['geolocation_lat']-lat_mean)< 3*lat_std]\n",
    "testing_customer_loc_df = testing_customer_loc_df[(testing_customer_loc_df['geolocation_lat']-lat_mean)> -3*lat_std]\n",
    "\n",
    "lng_mean = training_combined_locations['geolocation_lng'].mean()\n",
    "lng_std = training_combined_locations['geolocation_lng'].std()\n",
    "\n",
    "testing_customer_loc_df = testing_customer_loc_df[(testing_customer_loc_df['geolocation_lng']-lng_mean)< 3*lng_std]\n",
    "testing_customer_loc_df = testing_customer_loc_df[(testing_customer_loc_df['geolocation_lng']-lng_mean)> -3*lng_std]\n",
    "\n",
    "testing_customer_loc_df = testing_customer_loc_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Facility Location to get the 'k' warehouse locations\"\"\"\n",
    "from shapely import geometry\n",
    "\n",
    "def random_loc_generator(c_locs):\n",
    "  lats, longs = c_locs[0], c_locs[1]\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  min_lat, max_lat = min(lats), max(lats)\n",
    "  min_lng, max_lng = min(longs), max(longs)\n",
    "  new_lat, new_lng = random.uniform(min_lat, max_lat), random.uniform(min_lng, max_lng) \n",
    "  return [new_lat, new_lng]\n",
    "\n",
    "\n",
    "def loc_in_zone(loc, c_locs):\n",
    "  lats, longs = c_locs[0], c_locs[1]\n",
    "  coords = [(x,y) for x,y in zip(lats, longs)]\n",
    "  polygon = geometry.MultiPoint(coords).convex_hull\n",
    "  Point_X, Point_Y = loc[0], loc[1]\n",
    "  point = geometry.Point(Point_X, Point_Y)\n",
    "  return point.within(polygon)\n",
    "\n",
    "# code to generate 'm' locations that lie within a given zone:\n",
    "def generate_locs(m, c_locs):\n",
    "    new_locs = []\n",
    "    num_generated = 0\n",
    "    while num_generated < m:\n",
    "        new_loc = random_loc_generator(c_locs)\n",
    "        sanity_check = loc_in_zone(new_loc, c_locs)\n",
    "        if sanity_check:\n",
    "            num_generated += 1\n",
    "            new_locs.append(new_loc)\n",
    "    return new_locs\n",
    "\n",
    "num_potential_locs = 100\n",
    "potential_locs = np.array(generate_locs(num_potential_locs, customer_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Customers and Sellers \n",
    "lis = ['geolocation_lat','geolocation_lng']\n",
    "\n",
    "# Customers\n",
    "customer_locs = training_customer_locations[lis].values\n",
    "customer_locs = np.transpose(customer_locs)\n",
    "c_lts, c_lngs = customer_locs[0], customer_locs[1] \n",
    "plt.scatter(c_lngs,c_lts,color='Red',label='Customers', s=12)\n",
    "\n",
    "# Sellers\n",
    "seller_locs = training_seller_locations[lis].values\n",
    "seller_locs = np.transpose(seller_locs)\n",
    "s_lts, s_lgs = seller_locs[0], seller_locs[1]\n",
    "# plt.scatter(s_lgs,s_lts,color='Blue',label='Sellers')\n",
    "plt.scatter(s_lgs,s_lts,color='Blue',label='Drivers', s=12)\n",
    "\n",
    "# Potenital Warehouse locations\n",
    "p_lts, p_lgs = potential_locs[:, 0], potential_locs[:, 1]\n",
    "plt.scatter(p_lgs, p_lts, color='Black', label='Potential FFCs', s=50)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Customers and Sellers locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer locations\n",
    "cust_lats, cust_lngs = customer_locs[0], customer_locs[1]\n",
    "cust_locs = np.array([[lat, lng] for lat, lng in zip(cust_lats, cust_lngs)])\n",
    "# potential ffc/warehouse locations\n",
    "pffc_locs = np.array(potential_locs)\n",
    "# cost per mile\n",
    "cost_per_mile = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Find the locations of the ffcs\n",
    "# Model: k-median variant of the Uncapacitated Facility Location Problem (UCFL)\n",
    "from itertools import product\n",
    "\n",
    "import gurobipy as gb\n",
    "from gurobipy import GRB\n",
    "\n",
    "# fucntion to calculate euclidean distance b/w facilty and customer sites\n",
    "def compute_distance(loc1, loc2):\n",
    "    dx = loc1[0] - loc2[0]\n",
    "    dy = loc1[1] - loc2[1]\n",
    "    return np.sqrt(dx*dx + dy*dy)\n",
    "\n",
    "# Key parameters of MIP model formulation\n",
    "num_facilities = pffc_locs.shape[0]\n",
    "num_customers = cust_locs.shape[0]\n",
    "cartesian_prod = list(product(range(num_customers), range(num_facilities))) # [num_customers x num_facilities]\n",
    "\n",
    "# Delivering / Shipping costs:\n",
    "delivery_cost = {(c, f): cost_per_mile*compute_distance(cust_locs[c], pffc_locs[f]) for c, f in cartesian_prod}\n",
    "cost_matrix = np.array(list(delivery_cost.values())).reshape((num_customers, num_facilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIP model formulation \n",
    "# select k-ffc locations from the given set of potential locations\n",
    "def kmedian(m, n, c, k):\n",
    "    '''\n",
    "    m: number of potential locations\n",
    "    n: number of customers\n",
    "    c: cost matrix\n",
    "    k: required number of ffcs/warehouses\n",
    "    '''\n",
    "    model = gb.Model(\"k-median\")\n",
    "    dem, fac = list(range(n)), list(range(m)) # facility and demand\n",
    "    \n",
    "    # VARIABLES:\n",
    "    assign = model.addVars(dem, fac)\n",
    "    select = model.addVars(fac, vtype=GRB.BINARY)  \n",
    "    # OBJECTIVE: \n",
    "    model.setObjective(gb.quicksum(c[i, j]*assign[i, j] for j in fac for i in dem), GRB.MINIMIZE)\n",
    "    # CONSTRAINTS:\n",
    "    model.addConstrs(gb.quicksum(assign[i, j] for j in fac)==1 for i in dem)\n",
    "    model.addConstr(gb.quicksum(select)==k)\n",
    "    model.addConstrs(assign[i, j] <= select[j] for j in fac for i in dem)\n",
    "    model.update() \n",
    "    model.Params.Method = 3\n",
    "    model.optimize()\n",
    "    \n",
    "    return model, assign, select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_ffc_center = int(configParser.get('dataset-generation', 'num_ffc_center'))\n",
    "_, assignz, selected_ffcs = kmedian(num_facilities, num_customers, cost_matrix, num_ffc_center) \n",
    "\n",
    "mask_ffcs = [abs(int(val.x)) for val in selected_ffcs.values()]\n",
    "mask_ffc = np.array([True if val==1 else False for val in mask_ffcs])\n",
    "final_ffc_locs = potential_locs[mask_ffc]\n",
    "\n",
    "assert sum(mask_ffcs)==num_ffc_center, \"The number of selected ffcs < required_num_ffc\"\n",
    "\n",
    "# find out which customers are assigned to which ffc:\n",
    "cust_ffc_map = {key:abs(int(val.x)) for key, val in assignz.items()}\n",
    "cust_ffc_map = {cust: ffc for (cust, ffc) in assignz.keys() if cust_ffc_map[(cust, ffc)]==1}\n",
    "\n",
    "old_indices = np.unique(np.array(list(cust_ffc_map.values()))) # indices of the selected warehouses in 'potential_locs'\n",
    "new_indices = list(range(num_ffc_center)) # indices of the warehouses in 'final_ffc_locs'\n",
    "idx_map = {oidx:nidx for oidx, nidx in zip(old_indices, new_indices)}\n",
    "\n",
    "for i in cust_ffc_map.keys():\n",
    "    old_val = cust_ffc_map[i]\n",
    "    cust_ffc_map[i] = idx_map[old_val]\n",
    "\n",
    "\n",
    "# assigning ffc in testing data:\n",
    "num_facilities = pffc_locs.shape[0]\n",
    "num_cust = cust_locs.shape[0]\n",
    "cartesian_prod = list(product(range(num_cust), range(num_facilities))) # [num_customers x num_facilities]\n",
    "\n",
    "# Delivering / Shipping costs:\n",
    "delivery_cost = {(c, f): cost_per_mile*compute_distance(cust_locs[c], pffc_locs[f]) for c, f in cartesian_prod}\n",
    "cost_matrix = np.array(list(delivery_cost.values())).reshape((num_cust, num_facilities))\n",
    "\n",
    "# Assigning FFC to each customer \n",
    "## Assign a customer to the nearest ffc:\n",
    "ffc_indices = []\n",
    "for idx in range(testing_customer_loc_df.shape[0]):\n",
    "    # print(idx)\n",
    "    c_loc = testing_customer_loc_df.iloc[idx][lis]\n",
    "    \n",
    "    min_dist = 1e9\n",
    "    ffc_idx = -1\n",
    "    for fidx in range(final_ffc_locs.shape[0]):\n",
    "        f_loc = final_ffc_locs[fidx]\n",
    "        dist = compute_distance(c_loc, f_loc)\n",
    "        if(dist < min_dist):\n",
    "            ffc_idx = fidx \n",
    "            min_dist = dist \n",
    "    ffc_indices.append(ffc_idx)\n",
    "\n",
    "\n",
    "lis = ['geolocation_lat','geolocation_lng']\n",
    "\n",
    "test_prediction = np.array(ffc_indices)\n",
    "testing_customer_loc_df['ffc_index'] = test_prediction\n",
    "testing_customer_loc_df = testing_customer_loc_df.sort_values('order_delivered_customer_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Customers and Sellers \n",
    "lis = ['geolocation_lat','geolocation_lng']\n",
    "\n",
    "# Customers\n",
    "customer_locs = training_customer_locations[lis].values\n",
    "customer_locs = np.transpose(customer_locs)\n",
    "c_lts, c_lngs = customer_locs[0], customer_locs[1] \n",
    "plt.scatter(c_lngs,c_lts,color='Red',label='Customers')\n",
    "\n",
    "# Sellers\n",
    "seller_locs = training_seller_locations[lis].values\n",
    "seller_locs = np.transpose(seller_locs)\n",
    "s_lts, s_lgs = seller_locs[0], seller_locs[1]\n",
    "# plt.scatter(s_lgs,s_lts,color='Blue',label='Sellers')\n",
    "plt.scatter(s_lgs,s_lts,color='Blue',label='Drivers')\n",
    "\n",
    "# Potenital Warehouse locations\n",
    "p_lts, p_lgs = final_ffc_locs[:, 0], final_ffc_locs[:, 1]\n",
    "plt.scatter(p_lgs, p_lts, color='Black', label='Selected FFCs', marker=\"*\", s=500)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Customers and Sellers locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ = pd.DataFrame(columns={'label', 'x', 'y'})\n",
    "label_['label'] = list(range(num_ffc_center))\n",
    "label_['x'] = final_ffc_locs[:, 0]\n",
    "label_['y'] = final_ffc_locs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding maximum and minimum capacity of a warehouse for training data\n",
    "lis = ['geolocation_lat','geolocation_lng']\n",
    "prediction_array = np.array(list(cust_ffc_map.values()))\n",
    "training_customer_loc_df['ffc_index'] = prediction_array\n",
    "\n",
    "# Finding the number of orders of particular ffc center for each date\n",
    "training_customer_group_date = training_customer_loc_df.groupby(['ffc_index','order_delivered_customer_date'])['order_id'].count()\n",
    "training_customer_group_date = pd.DataFrame({'count':training_customer_group_date}).reset_index()\n",
    "\n",
    "# Finding 'quantile_capacity'-th quantile values for each ffc wrt the number of order over all dates\n",
    "quantile_capacity=float(configParser.get('dataset-generation','qauntile_capacity')) \n",
    "\n",
    "training_customer_group_date = training_customer_group_date.groupby(['ffc_index'])['count'].quantile(quantile_capacity)\n",
    "training_customer_group_date = pd.DataFrame({'cap':training_customer_group_date}).reset_index()\n",
    "\n",
    "# For each ffc:\n",
    "training_customer_group_date['min_cap'] = np.ceil(0.3 * training_customer_group_date['cap'])\n",
    "training_customer_group_date['max_cap'] = np.ceil(1.0 * training_customer_group_date['cap'])\n",
    "training_customer_group_date = training_customer_group_date.sort_values('ffc_index')\n",
    "\n",
    "max_cap = training_customer_group_date['max_cap'].values\n",
    "min_cap = training_customer_group_date['min_cap'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_generation(seed_val):\n",
    "  '''\n",
    "  Creating drivers through grid formation:\n",
    "  - Divide the city into an MxN grid\n",
    "  - Number of drivers in a grid cell = rand_num * number of customers in the grid cell\n",
    "      where rand_num is a random number in range [0.5, 1.5]\n",
    "  '''\n",
    "\n",
    "  lis = ['geolocation_lat', 'geolocation_lng']\n",
    "    \n",
    "  # the drivers' locations are predicted based on the training data\n",
    "  # for making the grid we use combined_locs\n",
    "  combined_locs = training_combined_locations[lis].values \n",
    "  combined_locs = np.transpose(combined_locs)\n",
    "    \n",
    "  # for getting the number of customers in each grid cell we use customer_locs\n",
    "  customer_locs = training_customer_loc_df[lis].values\n",
    "  customer_locs = np.transpose(customer_locs)\n",
    "  \n",
    "  # min- and max-longitude values\n",
    "  min_long, max_long = np.amin(combined_locs[0]), np.amax(combined_locs[0])\n",
    "  # min- and max-lattitude values\n",
    "  min_lat, max_lat = np.amin(combined_locs[1]), np.amax(combined_locs[1])\n",
    "\n",
    "  # configParser.read(configFilePath)\n",
    "  prop_constant_driver_customer = float(configParser.get('dataset-generation','prop_constant_driver_customer'))\n",
    "  grid_length = int(configParser.get('dataset-generation','grid_length'))\n",
    "  grid_width = int(configParser.get('dataset-generation','grid_width'))   \n",
    "  prop_constant_lower_range = float(configParser.get('dataset-generation','prop_constant_lower_range'))  # 0.5 \n",
    "  prop_constant_higher_range = float(configParser.get('dataset-generation','prop_constant_upper_range')) # 1.5\n",
    "\n",
    "  random.seed(seed_val) \n",
    "    \n",
    "  # Making the grid:  \n",
    "  grid_size=[grid_length,grid_width]\n",
    "  \n",
    "  drivers=[[],[]]\n",
    "  sum_customer = 0\n",
    "  sum_driver = 0\n",
    "\n",
    "  for i in range(grid_size[0]):\n",
    "    left_long = min_long + ((max_long-min_long)/grid_size[0])*i\n",
    "    right_long = min_long + ((max_long-min_long)/grid_size[0])*(i+1)\n",
    "    ind1 = customer_locs[0]>=left_long  \n",
    "    ind2 = customer_locs[0]<=right_long\n",
    "\n",
    "    for j in range(grid_size[1]):\n",
    "      up_lat = min_lat + ((max_lat-min_lat)/grid_size[1])*(j+1)\n",
    "      down_lat = min_lat + ((max_lat-min_lat)/grid_size[1])*j\n",
    "      ind3 = customer_locs[1]>=down_lat\n",
    "      ind4 = customer_locs[1]<=up_lat \n",
    "      ind = ind1 & ind2 & ind3 & ind4\n",
    "      num_customers = np.count_nonzero(ind) / num_training_days\n",
    "      num_drivers = num_customers / prop_constant_driver_customer\n",
    "\n",
    "      random_constant = random.uniform(prop_constant_lower_range,prop_constant_higher_range) \n",
    "      num_drivers = int(num_drivers * random_constant)\n",
    "      \n",
    "      if num_drivers==0:\n",
    "        continue\n",
    "      for k in range(num_drivers):\n",
    "        drivers[0].append(random.uniform(left_long,right_long))\n",
    "        drivers[1].append(random.uniform(down_lat,up_lat))\n",
    "    \n",
    "      sum_driver += num_drivers\n",
    "      sum_customer += num_customers   \n",
    "  drivers = np.array(drivers)\n",
    "\n",
    "  print(\"Number of Drivers:\",len(drivers[0]))\n",
    "  new_drivers = np.transpose(drivers)\n",
    "  \n",
    "  # Plot Customers, Sellers, FFCs and Drivers\n",
    "  fig, ax = plt.subplots()\n",
    "  \n",
    "  # Customers\n",
    "  customer_locs = testing_customer_loc_df[lis].values\n",
    "  customer_locs = np.transpose(customer_locs)\n",
    "  c_lts, c_lngs = customer_locs[0], customer_locs[1]\n",
    "  plt.scatter(c_lngs,c_lts,color='Red',label='Customer')\n",
    " \n",
    "  # Sellers\n",
    "  seller_locs = testing_seller_loc_df[lis].values\n",
    "  seller_locs = np.transpose(seller_locs)\n",
    "  s_lts, s_lngs = seller_locs[0], seller_locs[1]\n",
    "  plt.scatter(s_lngs,s_lts,color='Blue',label='Seller')\n",
    "  \n",
    "  # FFCs\n",
    "  # ffc_locs = np.transpose(centre)\n",
    "  ffc_locs = np.transpose(final_ffc_locs)\n",
    "  f_lts, f_lngs = ffc_locs[0], ffc_locs[1]\n",
    "  plt.scatter(f_lngs,f_lts,color='Black',label='FF centres')\n",
    "    \n",
    "  # Drivers\n",
    "  driver_locs = drivers\n",
    "  d_lts, d_lngs = driver_locs[0], driver_locs[1]\n",
    "  plt.scatter(d_lngs,d_lts,color='Cyan',label='Drivers')\n",
    "  \n",
    "  plt.legend(loc='lower left',prop={'size':20})\n",
    "  plt.xlabel('Longitude')\n",
    "  plt.ylabel('Latitude')\n",
    "  # plt.savefig(\"Plots/Coordinate Plots/\"+state_dataset+\".pdf\")\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  return new_drivers\n",
    "\n",
    "# Generating Drivers\n",
    "driver_locs = driver_generation(12)\n",
    "print(\"Drivers' Locations:\\n\", driver_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lats, s_lngs = seller_locs[0], seller_locs[1]\n",
    "driver_locs = np.array([[lat, lng] for lat, lng in zip(s_lats, s_lngs)])\n",
    "\n",
    "def euclidean_distance(d_loc, z_loc):\n",
    "    lat1, lng1 = d_loc[0], d_loc[1]\n",
    "    lat2, lng2 = z_loc[0], z_loc[1]\n",
    "    dist = np.sqrt(np.power(lat1-lat2, 2) + np.power(lng1-lng2, 2))\n",
    "    return dist\n",
    "    \n",
    "def L2Distance(data):\n",
    "  # \"data\": latitude-longitude level locations \n",
    "  transposed = np.expand_dims(data, axis = 1)\n",
    "  distance = np.power(data - transposed, 2)\n",
    "  distance = np.power(np.abs(distance).sum(axis = 2), 0.5) \n",
    "  return distance \n",
    "\n",
    "# assigning ratings to sellers:\n",
    "import random\n",
    "def generate_ratings(num_drivers):\n",
    "    r_set = list(np.arange(0, 6, 0.1)) #[1, 2, 3, 4, 5]\n",
    "    ratings = [0.0]*num_drivers\n",
    "    for i in range(num_drivers):\n",
    "        ratings[i] = np.round(random.sample(r_set, 1)[0],1)\n",
    "    return ratings\n",
    "  \n",
    "def abs_difference(ratings):\n",
    "    transposed = np.expand_dims(ratings, axis=1)\n",
    "    diff = abs(ratings-transposed) \n",
    "    return diff\n",
    "\n",
    "# new_drivers = driver_locs\n",
    "new_drivers = driver_generation(3249+1)\n",
    "ratings_list = generate_ratings(new_drivers.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using max flow for the Vanilla Distribution of Drivers to the warehouse\n",
    "import networkx as nx\n",
    "from networkx.algorithms.flow import maximum_flow\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "# Haversine formula\n",
    "# angular distance between two points on the surface of a sphere \n",
    "# Refer: https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 12742 * asin(sqrt(a))\n",
    "    \n",
    "prop_constant_driver_customer=float(configParser.get('dataset-generation','prop_constant_driver_customer'))\n",
    "\n",
    "# Since prop_constant_driver_customer number of orders (or customers) are being mapped to one driver\n",
    "# so upper_cap and lower_cap (wrt drivers) of each FFC will be (1/prop_constant_driver_customer) times max_cap and min_cap (wrt orders (or customers))\n",
    "upper_cap = max_cap / prop_constant_driver_customer\n",
    "lower_cap = min_cap / prop_constant_driver_customer\n",
    "lower_cap = lower_cap.astype(int)\n",
    "upper_cap = upper_cap.astype(int)\n",
    "upper_cap = np.array(upper_cap)\n",
    "lower_cap = np.array(lower_cap)\n",
    "lower_cap[lower_cap<=0] = 1   \n",
    "\n",
    "\n",
    "'''\n",
    "MCCA (Minimum Cost Capacitated Assignment) problem !\n",
    "- It can be cast as a \"minimum cost b-matching problem\" in bipartite graphs\n",
    "- The \"min cost b-matching problem\" reduces to thte classical \"min cost flow problem\"\n",
    "'''\n",
    "\"\"\"Code to generate MCCA distribution using networkX library\"\"\"\n",
    "# only upper_cap respected, lower_cap not included\n",
    "def flow_with_max_capacity(driver_loc,centre):\n",
    "  G = nx.DiGraph()\n",
    "  num_variables = len(centre) + len(driver_loc)\n",
    "  for i in range(len(driver_loc)):\n",
    "    G.add_edge(1,i+2,capacity=1,weight=0)\n",
    "\n",
    "  for i in range(len(centre)):\n",
    "    G.add_edge(len(driver_loc)+1+(i+1),num_variables+2,capacity=int(upper_cap[i]),weight=0)\n",
    "\n",
    "  for i in range(len(driver_loc)):\n",
    "    for j in range(len(centre)):\n",
    "      dis = (driver_loc[i][0]-centre[j][0])**2 + (driver_loc[i][1]-centre[j][1])**2 \n",
    "      G.add_edge(i+2,len(driver_loc)+1+(j+1),capacity=1,weight=int(dis*1000))\n",
    "    \n",
    "  mincostFlow = nx.max_flow_min_cost(G, 1, num_variables+2) \n",
    "  minCost = nx.cost_of_flow(G, mincostFlow) \n",
    "  maxFlow = maximum_flow(G, 1, num_variables+2)[1]\n",
    "  maxflowValue = nx.maximum_flow_value(G,1,num_variables+2)\n",
    "\n",
    "  driver_df = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df['ffc_index'] = -1 # unassigned \n",
    "  \n",
    "  # Driver to ffc assignment based on a mincostFlow values from corresponding driver_node to ffc_nodes\n",
    "  for i in range(len(driver_loc)):\n",
    "    for j in range(len(centre)):\n",
    "      if abs(mincostFlow[i+2][len(driver_loc)+(j+2)] - 1) < 0.1: \n",
    "        driver_df.at[i,'ffc_index'] = j\n",
    "  \n",
    "  return driver_df\n",
    "\n",
    "\n",
    "\n",
    "'''Code to generate MCCA distribution using ortools library'''\n",
    "# lower_cap and upper_cap both considered\n",
    "from ortools.graph import pywrapgraph\n",
    "\n",
    "def flow_with_maxmin_capacity(driver_loc,centre):\n",
    "  '''\n",
    "  - Instantiate a \"SimpleMinCostFlow\" solver.\n",
    "  - Using min-max flow for the Vanilla Distribution of Drivers to the warehouse\n",
    "  '''\n",
    "  num_variables = len(centre) + len(driver_loc)\n",
    "  \n",
    "  min_cost_flow = pywrapgraph.SimpleMinCostFlow()\n",
    "  for i in range(len(driver_loc)):\n",
    "    min_cost_flow.AddArcWithCapacityAndUnitCost(1,i+2,1,0)\n",
    "\n",
    "  for i in range(len(driver_loc)):\n",
    "    for j in range(len(centre)):\n",
    "      dis=(driver_loc[i][0]-centre[j][0])**2 + (driver_loc[i][1]-centre[j][1])**2\n",
    "      min_cost_flow.AddArcWithCapacityAndUnitCost(i+2,len(driver_loc)+1+(j+1),1,int(dis*1000))\n",
    "\n",
    "  for i in range(len(centre)):\n",
    "    min_cost_flow.AddArcWithCapacityAndUnitCost(len(driver_loc)+1+(i+1),num_variables+2,int(upper_cap[i])-int(lower_cap[i]),0)\n",
    "  for i in range(0,len(centre)): \n",
    "    min_cost_flow.SetNodeSupply(len(driver_loc)+1+(i+1), -int(lower_cap[i]))\n",
    "\n",
    "  source_supply = int(len(driver_loc))\n",
    "  sink_supply = int(len(driver_loc)-np.sum(lower_cap))\n",
    "\n",
    "  min_cost_flow.SetNodeSupply(1,source_supply)\n",
    "  min_cost_flow.SetNodeSupply(num_variables+2,-sink_supply)\n",
    "\n",
    "  for i in range(0,len(driver_loc)):\n",
    "    min_cost_flow.SetNodeSupply(i+2,0)\n",
    "\n",
    "  if min_cost_flow.Solve() == min_cost_flow.OPTIMAL:\n",
    "      print('Minimum cost:', min_cost_flow.OptimalCost())\n",
    "      print('')\n",
    "      print('  Arc    Flow / Capacity  Cost')\n",
    "      for i in range(min_cost_flow.NumArcs()):\n",
    "        if min_cost_flow.Flow(i)!=0:\n",
    "          cost = min_cost_flow.Flow(i) * min_cost_flow.UnitCost(i)\n",
    "          print('%1s -> %1s   %3s  / %3s       %3s' % (\n",
    "              min_cost_flow.Tail(i),\n",
    "              min_cost_flow.Head(i),\n",
    "              min_cost_flow.Flow(i),\n",
    "              min_cost_flow.Capacity(i),\n",
    "              cost))\n",
    "  else:\n",
    "    print('There was an issue with the min cost flow input.')\n",
    "\n",
    "  driver_df1 = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df1['ffc_index'] = -1 # unassigned\n",
    "  \n",
    "  for i in range(min_cost_flow.NumArcs()):\n",
    "    if min_cost_flow.Flow(i)!=0 and min_cost_flow.Tail(i)!=1 and min_cost_flow.Head(i)!=(num_variables+2):\n",
    "      tail = min_cost_flow.Tail(i)\n",
    "      head = min_cost_flow.Head(i)\n",
    "      driver_df1.at[tail-2,'ffc_index'] = head-len(driver_loc)-2  # (head-len(driver_loc)-2) is ffc_label ! -len(driver_loc)-2 ensures that it's in {0, ..., (num_ffc-1)} \n",
    "  return driver_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dist = L2Distance(driver_locs) \n",
    "num_drivers, num_zones = driver_locs.shape[0], final_ffc_locs.shape[0]\n",
    "dz_dist = np.zeros(shape=(num_drivers, num_zones))\n",
    "for d_idx, driver in enumerate(driver_locs):\n",
    "    d_dist = np.zeros(num_zones)\n",
    "    for z_idx, zone in enumerate(final_ffc_locs):\n",
    "        dist = euclidean_distance(driver, zone)\n",
    "        d_dist[z_idx] = dist \n",
    "    dz_dist[d_idx] = d_dist\n",
    "# print(dz_dist.shape)\n",
    "\n",
    "def get_pa(d_dist, k):\n",
    "    prohibited_assignments = np.zeros(shape=(num_drivers, num_zones))\n",
    "    \n",
    "    for d_idx, d_dist in enumerate(d_dist):\n",
    "        idx = np.argpartition(d_dist, k-1) \n",
    "        prohibited_assignments[d_idx][idx[k-1:]] = 1 # set the indices NOT corresponding to k-smallest elements \n",
    "    \n",
    "    return prohibited_assignments\n",
    "\n",
    "prhbtd_assigns = get_pa(dz_dist, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplex import Cplex\n",
    "model = Cplex()\n",
    "model.parameters.simplex.tolerances.feasibility.get(),\\\n",
    "model.parameters.simplex.tolerances.optimality.get(),\\\n",
    "model.parameters.simplex.tolerances.markowitz.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Clustering - LPP contstraints and Cplex\n",
    "from cplex import Cplex\n",
    "# from lp_tools import *\n",
    "from lp_tools_kn import *\n",
    "\n",
    "alpha_fair = float(configParser.get('fairness-constraint','alpha_fair'))\n",
    "\n",
    "# fair_distance = 1e9 # uncomment to consider \"global fairness constraints\" \n",
    "# alpha_fair = 2 #cwp\n",
    "\n",
    "def fair_clustering(dataset, centres, ratings, prohibited_assignments):\n",
    "\n",
    "  # Step 1: \t Create an instance of Cplex \n",
    "  problem = Cplex()\n",
    "  \n",
    "  ## changing tolerances to improve solution quality (prevent ill-conditioning and avoid numerical instability)\n",
    "  ## https://www.ibm.com/docs/en/icos/20.1.0?topic=cplex-list-parameters\n",
    "  problem.parameters.simplex.tolerances.feasibility.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.optimality.set(float(1e-9))\n",
    "  problem.parameters.simplex.tolerances.markowitz.set(float(0.9))\n",
    "  \n",
    "\n",
    "  # Step 2: \t Declare that this is a minimization problem\n",
    "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
    "    \n",
    "  \"\"\"\n",
    "   Step 3.   Declare and  add variables to the model. \n",
    "        The function prepare_to_add_variables (dataset, centres) prepares all the required information for this stage.\n",
    "  \n",
    "    objective: a list of coefficients (float) in the linear objective function\n",
    "    lower bound: a list of floats containing the lower bounds for each variable\n",
    "    upper bound: a list of floats containing the upper bounds for each variable\n",
    "    variable_names: a list of strings that contains the name of the variables\n",
    "  \"\"\"\n",
    "  # objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
    "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres, prohibited_assignments)\n",
    "  problem.variables.add(\n",
    "      obj = objective,\n",
    "      lb = lower_bound,\n",
    "      ub = upper_bound,\n",
    "      names = variable_names\n",
    "     \n",
    "    )\n",
    "    \n",
    "  \"\"\"\n",
    "  Step 4.   Declare and add constraints to the model.\n",
    "            There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
    "            Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
    "            The function prepare_to_add_constraints(dataset, centres) prepares the required data for this step.\n",
    "  \n",
    "   coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
    "   senses: a list of strings that identifies whether the corresponding constraint is\n",
    "           an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
    "   rhs: a list of floats corresponding to the rhs of the constraints.\n",
    "   constraint_names: a list of string corresponding to the name of the constraint\n",
    "  \"\"\"\n",
    "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair,fair_distance,ratings,flag)\n",
    "  print(\"num_constraints:\", len(senses))\n",
    "  problem.linear_constraints.add(\n",
    "      rhs = rhs,\n",
    "      senses = senses,\n",
    "      names = row_names\n",
    "    )\n",
    "  problem.linear_constraints.set_coefficients(coefficients)\n",
    "\n",
    "  # Step 5.\tSolve the problem\n",
    "  problem.solve()\n",
    "\n",
    "  result = {\n",
    "    \"status\": problem.solution.get_status(),\n",
    "    \"success\": problem.solution.get_status_string(),\n",
    "    \"objective\": problem.solution.get_objective_value(),\n",
    "    \"assignment\": problem.solution.get_values(),\n",
    "  }\n",
    "    \n",
    "  qm = problem.solution.quality_metric  \n",
    "  print(\"Solution Quality:\", problem.solution.get_float_quality([qm.max_x, qm.max_primal_infeasibility]))\n",
    "  # qm.max_primal_infeasibility is the \"maximum bound violation\" across all parameters\n",
    "  # print(\"Solution Kappa:\", )\n",
    "  \n",
    "  # print(\"Status:\", result['status'])\n",
    "  print(\"Status:\", problem.solution.get_status_string())\n",
    "\n",
    "  feasibility = 0\n",
    "  if problem.solution.get_status_string()=='optimal':\n",
    "        feasibility = 1\n",
    "  return result, feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caluclate average distance each driver has to travel each day from going to home to the location of FF Center\n",
    "def average_distance(driver_dist, label):\n",
    "    distance_warehouse_driver_df = pd.merge(driver_dist, label, left_on = 'ffc_index' ,right_on='label')\n",
    "    dist1=0.0\n",
    "    for i in range(len(distance_warehouse_driver_df)):\n",
    "        dist1 = dist1 + math.sqrt((distance_warehouse_driver_df.loc[i].at[\"geolocation_lat\"] - distance_warehouse_driver_df.loc[i].at[\"x\"])**2 + (distance_warehouse_driver_df.loc[i].at[\"geolocation_lng\"] - distance_warehouse_driver_df.loc[i].at[\"y\"])**2)\n",
    "    return round(dist1/len(distance_warehouse_driver_df),2)\n",
    "\n",
    "\n",
    "# Returns gini index on income list\n",
    "def gini_index(incomes):\n",
    "  driver_income = np.array(incomes)\n",
    "  driver_income = np.sort(driver_income)\n",
    "  integer_array = np.array([(i+1) for i in range(len(driver_income))])\n",
    "  driver_income = driver_income*750\n",
    "  gini_index = 2*(np.dot(driver_income,integer_array))/(len(driver_income)*(np.sum(driver_income))) - 1 -1/(len(driver_income))\n",
    "  return round(gini_index,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Assignment of drivers to the FFCs / warehouses\n",
    "import dependent_routing as dp\n",
    "\n",
    "def fair_assignment(prob_dis,driver_loc):\n",
    "  '''Assigning the driver using the probaility distribution using dependent rounding'''\n",
    "  # \"prob_dis\" is the result of the Fair-LP program \"fair_clustering\"  \n",
    "  prob_dist = copy.deepcopy(prob_dis)\n",
    "\n",
    "  rounding = dp.DependentRounding(prob_dist)\n",
    "  rounding._buildGraph(prob_dist)\n",
    "  final_assignment = rounding.round()\n",
    "  final_assignment = np.around(final_assignment,2)\n",
    "\n",
    "  driver_df2 = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df2['ffc_index'] = -1 # unassigned\n",
    " \n",
    "  for i in range(num_samples):\n",
    "    for j in range(num_centres):\n",
    "      # choose values which are close to 1\n",
    "      if abs(final_assignment[i][j]-1) < 0.01: \n",
    "        driver_df2.at[i,'ffc_index'] = j\n",
    "        \n",
    "  return driver_df2,final_assignment\n",
    "\n",
    "\n",
    "# Randomly assigning the drivers to ffc keeping the upper_cap constraint\n",
    "def random_dist(driver_loc):\n",
    "  driver_df4 = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df4['ffc_index'] = -1 # unassigned\n",
    "  temp_upper_cap = list(upper_cap)\n",
    "  \n",
    "  for i in range(num_samples):\n",
    "    ffc = random.randint(1,num_centres)-1\n",
    "    while(temp_upper_cap[ffc] <= 0):\n",
    "      ffc = random.randint(1,num_centres)-1\n",
    "    \n",
    "    driver_df4.at[i,'ffc_index'] = ffc\n",
    "    temp_upper_cap[ffc] -= 1  \n",
    "  return driver_df4\n",
    "\n",
    "\n",
    "# Assign the drivers in round robin manner to ffc keeping the upper_cap constraint\n",
    "def round_robin_dist(d,driver_loc):\n",
    "  # input 'd' is the current date in {1,..,30}\n",
    "  driver_df5 = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df5['ffc_index'] = -1 # unassigned\n",
    "    \n",
    "  temp_upper_cap = list(upper_cap)\n",
    "  \n",
    "  for i in range(num_samples):\n",
    "    ffc = (d+i) % num_centres\n",
    "    while(temp_upper_cap[ffc]<=0):\n",
    "      ffc = (ffc+1)%num_centres\n",
    "    \n",
    "    driver_df5.at[i,'ffc_index']=ffc\n",
    "    temp_upper_cap[ffc] -= 1\n",
    "  \n",
    "  return driver_df5\n",
    "\n",
    "\n",
    "# Assign the drivers with LIPA manner to ffc keeping the upper_cap constraint\n",
    "'''\n",
    "LIPA (Least Income Priority Assignment):\n",
    "- sort drivers in non-decreasing order wrt to their cumulative incomes until before this day\n",
    "- sort ffcs in non-increasing order wrt to the number of deliveries (#deliveries is proxy to ffc income) \n",
    "'''\n",
    "def low_income_dist(prev_income_drivers,prev_incomes_warehouse,driver_loc,lpp_prob_dis):\n",
    "  if len(prev_income_drivers)==0:\n",
    "    return fair_assignment(lpp_prob_dis,driver_loc)[0]\n",
    "\n",
    "  driver_df6 = pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
    "  driver_df6['ffc_index'] = -1 # unassigned\n",
    "  temp_upper_cap = list(upper_cap)\n",
    "  driver_index_inc = np.argsort(np.array(prev_income_drivers))\n",
    "  \n",
    "  warehouse_index_inc = np.argsort(np.array(prev_incomes_warehouse))\n",
    "  j = num_centres-1\n",
    "  for i in driver_index_inc:\n",
    "    ffc = warehouse_index_inc[j]\n",
    "    while(temp_upper_cap[ffc]<=0):\n",
    "      j = j-1\n",
    "      ffc = warehouse_index_inc[j]\n",
    "    \n",
    "    driver_df6.at[i,'ffc_index'] = ffc\n",
    "    temp_upper_cap[ffc] -= 1\n",
    "  return driver_df6\n",
    "\n",
    "\n",
    "# Returns the total cost of the drivers from their location to ffc\n",
    "def total_cost(df):\n",
    "  sum = 0\n",
    "  for index,row in df.iterrows():\n",
    "    sum += np.sqrt( (row['geolocation_lat']-centre[int(row['ffc_index'])][0])**2 + (row['geolocation_lng']-centre[int(row['ffc_index'])][1])**2 )  \n",
    "  return round(sum,2)\n",
    "\n",
    "\n",
    "def inequality(df,date_index):\n",
    "  df['driver_index'] = np.arange(len(df.index))  \n",
    "  driver_assign = df.groupby('ffc_index')['geolocation_lat'].count() # for every ffc_index, counts the number of drivers\n",
    "  driver_assign = pd.DataFrame({'num_drivers':driver_assign}).reset_index()\n",
    "  \n",
    "  for i in range(len(centre)):\n",
    "    if i not in driver_assign['ffc_index'].values:\n",
    "      driver_assign.loc[len(driver_assign.index)] = [i,0]\n",
    "\n",
    "  driver_assign = driver_assign.sort_values('ffc_index')\n",
    "  driver_assign['min_cap'] = lower_cap\n",
    "  driver_assign['max_cap'] = upper_cap\n",
    "\n",
    "  testing_customer_group_date = testing_customer_loc_df.groupby(['ffc_index','order_delivered_customer_date'])['order_id'].count() # for each ffc, counts the number of orders on each day\n",
    "  testing_customer_group_date = pd.DataFrame({'num_delivery_per_date':testing_customer_group_date}).reset_index()\n",
    "  \n",
    "  dates = testing_customer_group_date['order_delivered_customer_date'].unique()\n",
    "  testing_customer_group_date = testing_customer_group_date[testing_customer_group_date['order_delivered_customer_date']==dates[date_index]]\n",
    "\n",
    "  warehouse_income_df = pd.merge(testing_customer_group_date, driver_assign, on='ffc_index', how='right')\n",
    "  warehouse_income_df['income_warehouse_driver'] = warehouse_income_df['num_delivery_per_date'] / warehouse_income_df['num_drivers']\n",
    "  warehouse_income_df.loc[~np.isfinite(warehouse_income_df['income_warehouse_driver']), 'income_warehouse_driver'] = 0 # takes care of divide by 0 i.e., if the number of drivers is 0; handles infintiy and NaN\n",
    "  warehouse_income_df = warehouse_income_df.round(2)\n",
    "  warehouse_income_df.reset_index(inplace=True)\n",
    " \n",
    "  warehouse_incomes = warehouse_income_df['income_warehouse_driver'].values\n",
    "\n",
    "  df['date'] = dates[date_index]\n",
    "  df['income'] = -1.0\n",
    "    \n",
    "  # Assigning the income of ffc to which the driver was assigned\n",
    "  for i in range(len(df.index)):\n",
    "    df.at[i,'income'] = warehouse_income_df.at[df.iloc[i]['ffc_index'],'income_warehouse_driver']\n",
    "    \n",
    "  return df, warehouse_incomes\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_ffc_maps = [] \n",
    "\n",
    "# Finding the income of all drivers for {num_testing_days}, with given distribution type\n",
    "def inequality2(algo_type=\"random\",num_days=30,lpp_prob_dis=[[]]):\n",
    "  if club_num_dates > 30:\n",
    "    num_days = 15\n",
    "    \n",
    "  fair_assignment_totaldays = None\n",
    "  overall_cost = 0\n",
    "  prev_income_drivers = []\n",
    "  prev_incomes_warehouse = []\n",
    "  df = None\n",
    "  \n",
    "  algos = [\"random\", \"round_robin\", \"fair_algo\", \"vanilla_max\", \"vanilla_maxmin\", \"low_income_dist\"]\n",
    "  global_assignments = {alg:\\\n",
    "                        {day+1: [-1]*len(new_drivers) for day in range(num_days)}\n",
    "                      for alg in algos\n",
    "                      }\n",
    "  \n",
    "  if algo_type==\"vanilla_max\":      df_vanilla_max = flow_with_max_capacity(new_drivers,centre)\n",
    "  if algo_type==\"vanilla_maxmin\":   df_vanilla_maxmin = flow_with_maxmin_capacity(new_drivers,centre)\n",
    "\n",
    "  for i in range(num_days):\n",
    "    if algo_type==\"random\":          df = random_dist(new_drivers)\n",
    "    if algo_type==\"round_robin\":     df = round_robin_dist(i,new_drivers)\n",
    "    if algo_type==\"fair_algo\":       df = fair_assignment(lpp_prob_dis,new_drivers)[0]\n",
    "    if algo_type==\"vanilla_max\":     df = df_vanilla_max   \n",
    "    if algo_type==\"vanilla_maxmin\":  df = df_vanilla_maxmin  \n",
    "    if algo_type==\"low_income_dist\": df = low_income_dist(prev_income_drivers,prev_incomes_warehouse,new_drivers,lpp_prob_dis)\n",
    "    \n",
    "    if algo_type==\"fair_algo\":\n",
    "        driver_ffc_maps.append(df)\n",
    "\n",
    "    \n",
    "    df_result, prev_incomes_warehouse = inequality(df,i)\n",
    "    overall_cost += total_cost(df)\n",
    "\n",
    "    if fair_assignment_totaldays is None:\n",
    "      fair_assignment_totaldays = df_result\n",
    "    else:\n",
    "      fair_assignment_totaldays = pd.concat([fair_assignment_totaldays,df_result])\n",
    "    \n",
    "    prev_income_drivers = fair_assignment_totaldays.groupby('driver_index')['income'].sum().values # for low_income_dist or LIPA\n",
    "\n",
    "    global_assignments[algo_type][i+1] = df['ffc_index'].values\n",
    "    \n",
    "  lis1 = fair_assignment_totaldays.groupby('driver_index')['income'].sum().values\n",
    "  \n",
    "  fair_assignment_totaldays['total_income'] = fair_assignment_totaldays['income'].groupby(fair_assignment_totaldays['driver_index']).transform('sum')\n",
    "  driver_income_df = fair_assignment_totaldays.groupby('driver_index').first().reset_index()  \n",
    "  avg_distance = average_distance(fair_assignment_totaldays, label_)\n",
    "\n",
    "  \n",
    "  return gini_index(lis1),avg_distance,fair_assignment_totaldays,np.sum(np.array(lis1)),lis1, global_assignments\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the \"income gap per pair of those drivers which are separated by less than fair_distance\"\n",
    "\n",
    "def helper_func(table, driver_loc, ratings):\n",
    "    ''' \"table\" is \"fair_assignment_totaldays\" form inequality2(...) '''\n",
    "    income_gap = 0.0\n",
    "    \n",
    "    assignment_total = table.groupby(['driver_index','geolocation_lat','geolocation_lng'])['income'].sum() # <=> fair_assignment_totaldays.groupby('driver_index')['income'].sum().values\n",
    "    assignment_total = pd.DataFrame({'income_sum':assignment_total}).reset_index()\n",
    "    fair_distance = float(configParser.get('fairness-constraint','fair_distance'))\n",
    "\n",
    "    income_array = []\n",
    "    for i in range(len(driver_loc)):\n",
    "        income_array.append(assignment_total.loc[i].at['income_sum'])\n",
    "    income_array = np.array(income_array)\n",
    "\n",
    "    if flag==0:\n",
    "        transposed = np.expand_dims(driver_loc, axis = 1)\n",
    "        distance_drivers = np.power(driver_loc - transposed, 2)\n",
    "        distance_drivers = np.power(np.abs(distance_drivers).sum(axis = 2), 0.5) \n",
    "\n",
    "    transposed = np.expand_dims(income_array, axis = 1)\n",
    "    income_diff = np.abs(income_array - transposed)\n",
    "\n",
    "    if flag==1: \n",
    "        transposed = np.expand_dims(ratings, axis=1)\n",
    "        ratings_diff = abs(ratings-transposed)\n",
    "        distance_drivers = ratings_diff\n",
    "\n",
    "    if flag==2:\n",
    "        transposed1 = np.expand_dims(driver_loc, axis = 1)\n",
    "        dists_diff = np.power(driver_loc - transposed1, 2)\n",
    "        dists_diff = np.power(np.abs(dists_diff).sum(axis = 2), 0.5)\n",
    "\n",
    "        transposed2 = np.expand_dims(ratings, axis=1)\n",
    "        ratings_diff = abs(ratings-transposed2)\n",
    "\n",
    "        combination = w1*dists_diff + w2*ratings_diff\n",
    "        distance_drivers = combination\n",
    "\n",
    "    # @set:\n",
    "    distance_drivers *= 110 \n",
    "    fair_distance *= 110\n",
    "    \n",
    "    num_pair_fair_drivers=1e-7\n",
    "    for i in range(len(driver_loc)-1):\n",
    "        for j in range(i+1,len(driver_loc)):\n",
    "            if(distance_drivers[i][j] <= fair_distance and distance_drivers[i][j]>0): \n",
    "                income_gap += (float(income_diff[i][j])/distance_drivers[i][j])\n",
    "                num_pair_fair_drivers = num_pair_fair_drivers+1\n",
    "\n",
    "    # @reset\n",
    "    distance_drivers /= 110\n",
    "    \n",
    "    net_income_gap_within_fair_distance = round(income_gap/num_pair_fair_drivers,2)\n",
    "    \n",
    "    return net_income_gap_within_fair_distance,income_diff,distance_drivers,assignment_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns spatial inequality.\n",
    "def spatial_inequality(income_diff, distance_drivers, table, driver_loc, ratings):\n",
    "    sum_i = 0.0\n",
    "    total_sum = 0.0\n",
    "\n",
    "    fair_distance = float(configParser.get('fairness-constraint','fair_distance'))\n",
    "    distance_drivers = distance_drivers \n",
    "\n",
    "    if flag==1:\n",
    "        transposed = np.expand_dims(ratings, axis=1)\n",
    "        ratings_diff = abs(ratings-transposed)\n",
    "        distance_drivers = ratings_diff\n",
    "    \n",
    "    if flag==2:\n",
    "        transposed2 = np.expand_dims(ratings, axis=1)\n",
    "        ratings_diff = abs(ratings-transposed2)\n",
    "        combined = w1*distance_drivers + w2*ratings_diff\n",
    "        distance_drivers = combined\n",
    "\n",
    "\n",
    "    for i in range(len(driver_loc)):\n",
    "        sum_i = sum_i + table.loc[i].at[\"income_sum\"]\n",
    "        sum_i_j = 0.0\n",
    "        num_i_j = 1e-7\n",
    "        \n",
    "        for j in range(i+1,len(driver_loc)):\n",
    "            dist = distance_drivers[i][j] \n",
    "            if (dist < fair_distance): \n",
    "                sum_i_j = sum_i_j + income_diff[i][j]\n",
    "                num_i_j = num_i_j + 1           \n",
    "        total_sum = total_sum + float(sum_i_j)/num_i_j    \n",
    "    sp_index = float(total_sum)/sum_i\n",
    "    return round(sp_index,4)\n",
    "\n",
    "\n",
    "def fraction(base,fair):\n",
    "    ''' \n",
    "    improvement in income = ((fair_income - base_income)/base_income)\n",
    "    '''   \n",
    "    fair_income_array = fair[\"income_sum\"].values\n",
    "    base_income_array = base[\"income_sum\"].values\n",
    " \n",
    "    return np.around((np.divide(fair_income_array,(base_income_array+1e-5))-1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def ind_spatial_stability(assign : List):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        assign: 1 x num_days \n",
    "        list of assigned zones \n",
    "    Output: \n",
    "        ss: spatial stability value for 'assign'\n",
    "    \"\"\"\n",
    "    # compute frequency distribution 'freqs'\n",
    "    score = 0.0\n",
    "    \n",
    "    ## Frequency distribution entropy\n",
    "    H = 0 \n",
    "    values, freqs = np.unique(assign, return_counts=True) \n",
    "    freqs = freqs/freqs.sum() \n",
    "    entropies = -1.0 * freqs * np.log(freqs) # log_e\n",
    "    H = entropies.sum() \n",
    "    \n",
    "    ## num_zone_changes\n",
    "    R = 0 \n",
    "    num_days = len(assign)\n",
    "    for idx in range(1, num_days):\n",
    "        if assign[idx] != assign[idx-1]:\n",
    "            R += 1\n",
    "\n",
    "    score = H * R\n",
    "    return score \n",
    "\n",
    "\n",
    "def spatial_stability(algo : str, all_assignments : Dict[str, Dict[str, List]]):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        'algo' \\in {\"random\",\"low_income_dist\",\"round_robin\",\"vanilla_max\",\"vanilla_maxmin\",\"fair_algo\"}\n",
    "        all_assignments: Dict({algo (str) : Dict({day_num : List of size 1 x num_days})})\n",
    "    Output:\n",
    "        Spatial stability metric value for 'algo'\n",
    "    \"\"\"\n",
    "    ss = 0.0 \n",
    "    L = []\n",
    "\n",
    "    assignments = all_assignments[algo] # {day_num : list of assignments}\n",
    "    num_days = len(assignments)\n",
    "    NUM_DRIVERS = len(assignments[1])\n",
    "\n",
    "    driver_assigns = np.zeros((num_days, NUM_DRIVERS))\n",
    "    for day_num in range(1, num_days+1):\n",
    "        day_assigns = assignments[day_num]\n",
    "        driver_assigns[day_num-1] = day_assigns \n",
    "    \n",
    "    driver_assigns = driver_assigns.T # shape: NUM_DRIVERS x num_days\n",
    "    print(driver_assigns)\n",
    "    for d_idx in range(NUM_DRIVERS):\n",
    "        curr_driver = driver_assigns[d_idx]\n",
    "        curr_ss = ind_spatial_stability(curr_driver)\n",
    "        L.append(curr_ss)\n",
    "\n",
    "    ss = np.mean(L)\n",
    "    print(L)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Gini_Index_List = []\n",
    "Avg_Distance_List = []\n",
    "Income_Gap_List = []\n",
    "spatial_inequality_list = []\n",
    "Name_list = []\n",
    "Income_sum_lis = []\n",
    "num_drivers = []\n",
    "fraction_list = []\n",
    "spatial_stability_list = []\n",
    "\n",
    "import copy\n",
    "\n",
    "def sanityCheck(weights):\n",
    "    \"\"\"\n",
    "    To cope with bound violations which can occur upto the feasibility parameter range \n",
    "    So the lower bound of 0.0 on the probabilities can get violated and the values can go down to (0-feasibility_parameter_value)\n",
    "    \"\"\"\n",
    "    for i in range(len(weights)):\n",
    "        last_pos_index = -1\n",
    "        neg_value = 0\n",
    "        \n",
    "        for j in range(len(weights[0])):\n",
    "            assert weights[i][j] > -1e-5 \n",
    "            if weights[i][j] < 0:\n",
    "                neg_value += weights[i][j]\n",
    "                weights[i][j] = 0\n",
    "            elif weights[i][j] > 0:\n",
    "                last_pos_index = j\n",
    "        \n",
    "        weights[i][last_pos_index] += neg_value\n",
    "        \n",
    "    return weights\n",
    "\n",
    "number_of_runs = int(configParser.get('dataset-generation','number_of_runs'))\n",
    "random_seed = int(configParser.get('dataset-generation','random_seed'))\n",
    "\n",
    "dis_type = [\"random\",\"low_income_dist\",\"round_robin\",\"vanilla_max\",\"vanilla_maxmin\",\"fair_algo\"]\n",
    "Name_dis_types = [\"Random\",\"LIPA\",\"RoundRobin\",\"MCCA\",\"MCCA-L\",\"FairAssign\"]\n",
    "colors = ['Indigo','Blue','Cyan','Green','Orange','Red']\n",
    "\n",
    "indexs_allowed = [0,1,2,3,4,5] \n",
    "\n",
    "# data to be stored for scatter income plot and lorenz curve plot:\n",
    "driver_income_dfs = {dist:{} for dist in dis_type} \n",
    "income_arrs = {dist:{} for dist in dis_type}\n",
    "centre = final_ffc_locs\n",
    "\n",
    "# Generates result for number_of_runs times, and store the value in differnt list\n",
    "for i in range(number_of_runs):\n",
    "    print(f\"----------- Run {i+1} ------------------\")\n",
    "    new_drivers = driver_generation(i+random.randint(0, 100))\n",
    "    # new_drivers = driver_locs\n",
    "    \n",
    "    if len(new_drivers) < sum(lower_cap):\n",
    "        print(\"Lower number of drivers generated\")\n",
    "        print(f\"Generated: {len(new_drivers)}, Required: {sum(lower_cap)}\")\n",
    "        i -= 1\n",
    "        continue\n",
    "    \n",
    "    ratings = generate_ratings(new_drivers.shape[0])\n",
    "    # result, feasible = fair_clustering(new_drivers,centre,ratings)\n",
    "    result, feasible = fair_clustering(new_drivers,centre,ratings,prhbtd_assigns)\n",
    "    num_samples = len(new_drivers)\n",
    "    num_centres = len(centre)\n",
    "    \n",
    "    # Since the assignment can be infeasible if sum(upper_cap)<num_samples, therefore perform driver generation until feasibility is attained !\n",
    "    while not feasible:\n",
    "        new_drivers = driver_generation(i+random.randint(0, 100))\n",
    "        num_samples = len(new_drivers)\n",
    "        ratings = generate_ratings(new_drivers.shape[0])\n",
    "        # result, feasible = fair_clustering(new_drivers,centre,ratings)\n",
    "        result, feasible = fair_clustering(new_drivers,centre,ratings,prhbtd_assigns)\n",
    "        \n",
    "    # Probability Distribution output from cplex\n",
    "    lpp_prob_dis = np.reshape(result['assignment'][:num_samples*num_centres],(-1,num_centres))\n",
    "    lpp_prob_dis = sanityCheck(copy.deepcopy(lpp_prob_dis))\n",
    "        \n",
    "    \n",
    "    print(\"Everything solved, graphs start\")\n",
    "    \n",
    "    for j in indexs_allowed:\n",
    "        tup = inequality2(algo_type=dis_type[j],lpp_prob_dis=lpp_prob_dis)\n",
    "        tup2 = helper_func(tup[2],new_drivers,ratings)\n",
    "        spat = spatial_inequality(tup2[1], tup2[2], tup2[3], new_drivers,ratings) \n",
    "\n",
    "        Gini_Index_List.append(tup[0])\n",
    "        Avg_Distance_List.append(tup[1])\n",
    "        Income_Gap_List.append(tup2[0])\n",
    "        spatial_inequality_list.append(spat) # defined globally over all drivers\n",
    "        Name_list.append(Name_dis_types[j])\n",
    "        Income_sum_lis.append(tup[3])\n",
    "        num_drivers.append(num_samples)\n",
    "\n",
    "        global_assignments = tup[-1]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        sp_st = spatial_stability(dis_type[j], global_assignments)\n",
    "        spatial_stability_list.append(sp_st) # defined individually over drivers\n",
    "        \n",
    "        # for scatter income plot for FairAssign:\n",
    "        driver_income_dfs[dis_type[j]] = tup[2]\n",
    "        # for lorenz curve:\n",
    "        income_arrs[dis_type[j]] = tup[4]\n",
    "        \n",
    "        if j==3: base=tup2[3] # MCCA\n",
    "        if j==5: fair=tup2[3] # FairAssign\n",
    "\n",
    "         \n",
    "\n",
    "    fraction_worse_off = fraction(base,fair)\n",
    "    fraction_list.append(fraction_worse_off)\n",
    "\n",
    "    # Below All_Income contains all info about all runs\n",
    "    All_Income = pd.DataFrame({'Dist Type':Name_list,'Gini': Gini_Index_List,'Avg Dist':Avg_Distance_List,'Income_Gap':Income_Gap_List,'spatial_index':spatial_inequality_list, 'spatial_stability':spatial_stability_list})\n",
    "    print(\"[ Raw Info all runs for each dist_type ] :\\n\", All_Income)\n",
    "    print(\"[ Mean for each Dist Type ] :\\n\", All_Income.groupby('Dist Type').mean())\n",
    "    print()\n",
    "    print(\"[ Max for each Dist Type ] :\\n\", All_Income.groupby('Dist Type').max())\n",
    "    print()\n",
    "    print(\"[ Min for each Dist Type ] :\\n\", All_Income.groupby('Dist Type').min())\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the result\n",
    "All_Income = pd.DataFrame({'Dist Type':Name_list,'Gini': Gini_Index_List, 'Avg Dist':Avg_Distance_List,'Income_Gap':Income_Gap_List,'spatial_index':spatial_inequality_list,'Income_sum':Income_sum_lis, 'spatial_stability':spatial_stability_list})\n",
    "All_Income = All_Income.replace(\"Low Income Distribution\",\"Robinhood\") # just as cool line! no meaning\n",
    "\n",
    "# configParser.read(configFilePath)\n",
    "print(\"State:\", state_dataset)\n",
    "# print(\"State: \",configParser.get('dataset-generation','state'))\n",
    "# print(\"Equal_to_or_not: \",configParser.get('dataset-generation','equal_to'))\n",
    "print(\"num_ffc_center: \",configParser.get('dataset-generation','num_ffc_center'))\n",
    "print(\"Grid Size: \",configParser.get('dataset-generation','grid_length'),configParser.get('dataset-generation','grid_width'))\n",
    "print(\"Fair Distance: \",configParser.get('fairness-constraint','fair_distance'))\n",
    "print(\"Alpha_fair: \",configParser.get('fairness-constraint','alpha_fair'))\n",
    "print(\"number_of_runs: \",configParser.get('dataset-generation','number_of_runs'))\n",
    "print()\n",
    "print(\"Avg Num of Drivers: \",np.mean(np.array(num_drivers)))\n",
    "print() \n",
    "\n",
    "print(\"#################################### FINAL RESULTS #######################################################\")\n",
    "print(\"[ Mean for each Dist Type ] :\")\n",
    "print(All_Income.groupby('Dist Type').mean().round(4))\n",
    "print()\n",
    "print(\"[ Max for each Dist Type ] :\\n\")\n",
    "print(All_Income.groupby('Dist Type').max())\n",
    "print()\n",
    "print(\"[ Min for each Dist Type ] :\\n\")\n",
    "print(All_Income.groupby('Dist Type').min())\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
